{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b05a88b-70b7-4dce-8f10-640b3ddf185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataframe (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>note_type</th>\n",
       "      <th>note_seq</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032-DS-21</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>DS</td>\n",
       "      <td>21</td>\n",
       "      <td>2180-05-07 00:00:00</td>\n",
       "      <td>2180-05-09 15:26:00</td>\n",
       "      <td>\\nName:  ___                     Unit No:   _...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032-DS-22</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357</td>\n",
       "      <td>DS</td>\n",
       "      <td>22</td>\n",
       "      <td>2180-06-27 00:00:00</td>\n",
       "      <td>2180-07-01 10:15:00</td>\n",
       "      <td>\\nName:  ___                     Unit No:   _...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032-DS-23</td>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034</td>\n",
       "      <td>DS</td>\n",
       "      <td>23</td>\n",
       "      <td>2180-07-25 00:00:00</td>\n",
       "      <td>2180-07-25 21:42:00</td>\n",
       "      <td>\\nName:  ___                     Unit No:   _...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032-DS-24</td>\n",
       "      <td>10000032</td>\n",
       "      <td>25742920</td>\n",
       "      <td>DS</td>\n",
       "      <td>24</td>\n",
       "      <td>2180-08-07 00:00:00</td>\n",
       "      <td>2180-08-10 05:43:00</td>\n",
       "      <td>\\nName:  ___                     Unit No:   _...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000084-DS-17</td>\n",
       "      <td>10000084</td>\n",
       "      <td>23052089</td>\n",
       "      <td>DS</td>\n",
       "      <td>17</td>\n",
       "      <td>2160-11-25 00:00:00</td>\n",
       "      <td>2160-11-25 15:09:00</td>\n",
       "      <td>\\nName:  ___                    Unit No:   __...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          note_id  subject_id   hadm_id note_type  note_seq  \\\n",
       "0  10000032-DS-21    10000032  22595853        DS        21   \n",
       "1  10000032-DS-22    10000032  22841357        DS        22   \n",
       "2  10000032-DS-23    10000032  29079034        DS        23   \n",
       "3  10000032-DS-24    10000032  25742920        DS        24   \n",
       "4  10000084-DS-17    10000084  23052089        DS        17   \n",
       "\n",
       "             charttime            storetime  \\\n",
       "0  2180-05-07 00:00:00  2180-05-09 15:26:00   \n",
       "1  2180-06-27 00:00:00  2180-07-01 10:15:00   \n",
       "2  2180-07-25 00:00:00  2180-07-25 21:42:00   \n",
       "3  2180-08-07 00:00:00  2180-08-10 05:43:00   \n",
       "4  2160-11-25 00:00:00  2160-11-25 15:09:00   \n",
       "\n",
       "                                                text  \n",
       "0   \\nName:  ___                     Unit No:   _...  \n",
       "1   \\nName:  ___                     Unit No:   _...  \n",
       "2   \\nName:  ___                     Unit No:   _...  \n",
       "3   \\nName:  ___                     Unit No:   _...  \n",
       "4   \\nName:  ___                    Unit No:   __...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a sample of the CSV file first to get an overview\n",
    "file_path = '/scratch/baj321/MIMIC-Note/physionet.org/files/mimic-iv-note/2.2/note/discharge.csv'\n",
    "# Display the first few rows of the dataframe to get an overview\n",
    "df_sample = pd.read_csv(file_path)\n",
    "\n",
    "print(\"First few rows of the dataframe (sample):\")\n",
    "display(df_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715df1a5-5c3e-4cff-ad40-f26a038aa25d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arguments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7fb2b977347a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margs_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'arguments'"
     ]
    }
   ],
   "source": [
    "from arguments import args_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a92395-6039-4a6b-a65a-3272a7688165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Discretizer:\n",
    "    def __init__(self, timestep=0.8, store_masks=True, impute_strategy='zero', start_time='zero',\n",
    "                 config_path= 'ehr_utils/resources/discretizer_config.json'):\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "            self._id_to_channel = config['id_to_channel']\n",
    "            self._channel_to_id = dict(zip(self._id_to_channel, range(len(self._id_to_channel))))\n",
    "            self._is_categorical_channel = config['is_categorical_channel']\n",
    "            self._possible_values = config['possible_values']\n",
    "            self._normal_values = config['normal_values']\n",
    "\n",
    "        self._header = [\"Hours\"] + self._id_to_channel\n",
    "        self._timestep = timestep\n",
    "        self._store_masks = store_masks\n",
    "        self._start_time = start_time\n",
    "        self._impute_strategy = impute_strategy\n",
    "\n",
    "        # for statistics\n",
    "        self._done_count = 0\n",
    "        self._empty_bins_sum = 0\n",
    "        self._unused_data_sum = 0\n",
    "\n",
    "    def transform(self, X, header=None, end=None):\n",
    "        if header is None:\n",
    "            header = self._header\n",
    "        assert header[0] == \"Hours\"\n",
    "        eps = 1e-6\n",
    "\n",
    "        N_channels = len(self._id_to_channel)\n",
    "        ts = [float(row[0]) for row in X]\n",
    "        for i in range(len(ts) - 1):\n",
    "            assert ts[i] < ts[i+1] + eps\n",
    "\n",
    "        if self._start_time == 'relative':\n",
    "            first_time = ts[0]\n",
    "        elif self._start_time == 'zero':\n",
    "            first_time = 0\n",
    "        else:\n",
    "            raise ValueError(\"start_time is invalid\")\n",
    "\n",
    "        if end is None:\n",
    "            max_hours = max(ts) - first_time\n",
    "        else:\n",
    "            max_hours = end - first_time\n",
    "\n",
    "        N_bins = int(max_hours / self._timestep + 1.0 - eps)\n",
    "\n",
    "        cur_len = 0\n",
    "        begin_pos = [0 for i in range(N_channels)]\n",
    "        end_pos = [0 for i in range(N_channels)]\n",
    "        for i in range(N_channels):\n",
    "            channel = self._id_to_channel[i]\n",
    "            begin_pos[i] = cur_len\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                end_pos[i] = begin_pos[i] + len(self._possible_values[channel])\n",
    "            else:\n",
    "                end_pos[i] = begin_pos[i] + 1\n",
    "            cur_len = end_pos[i]\n",
    "\n",
    "        data = np.zeros(shape=(N_bins, cur_len), dtype=float)\n",
    "        mask = np.zeros(shape=(N_bins, N_channels), dtype=int)\n",
    "        original_value = [[\"\" for j in range(N_channels)] for i in range(N_bins)]\n",
    "        total_data = 0\n",
    "        unused_data = 0\n",
    "\n",
    "        def write(data, bin_id, channel, value, begin_pos):\n",
    "            channel_id = self._channel_to_id[channel]\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                category_id = self._possible_values[channel].index(value)\n",
    "                N_values = len(self._possible_values[channel])\n",
    "                one_hot = np.zeros((N_values,))\n",
    "                one_hot[category_id] = 1\n",
    "                for pos in range(N_values):\n",
    "                    data[bin_id, begin_pos[channel_id] + pos] = one_hot[pos]\n",
    "            else:\n",
    "                data[bin_id, begin_pos[channel_id]] = float(value)\n",
    "\n",
    "        for row in X:\n",
    "            t = float(row[0]) - first_time\n",
    "            if t > max_hours + eps:\n",
    "                continue\n",
    "            bin_id = int(t / self._timestep - eps)\n",
    "            assert 0 <= bin_id < N_bins\n",
    "\n",
    "            for j in range(1, len(row)):\n",
    "                if row[j] == \"\":\n",
    "                    continue\n",
    "                channel = header[j]\n",
    "                channel_id = self._channel_to_id[channel]\n",
    "\n",
    "                total_data += 1\n",
    "                if mask[bin_id][channel_id] == 1:\n",
    "                    unused_data += 1\n",
    "                mask[bin_id][channel_id] = 1\n",
    "\n",
    "                write(data, bin_id, channel, row[j], begin_pos)\n",
    "                original_value[bin_id][channel_id] = row[j]\n",
    "\n",
    "        # impute missing values\n",
    "\n",
    "        if self._impute_strategy not in ['zero', 'normal_value', 'previous', 'next']:\n",
    "            raise ValueError(\"impute strategy is invalid\")\n",
    "\n",
    "        if self._impute_strategy in ['normal_value', 'previous']:\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if self._impute_strategy == 'normal_value':\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    if self._impute_strategy == 'previous':\n",
    "                        if len(prev_values[channel_id]) == 0:\n",
    "                            imputed_value = self._normal_values[channel]\n",
    "                        else:\n",
    "                            imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        if self._impute_strategy == 'next':\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins-1, -1, -1):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if len(prev_values[channel_id]) == 0:\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    else:\n",
    "                        imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        empty_bins = np.sum([1 - min(1, np.sum(mask[i, :])) for i in range(N_bins)])\n",
    "        self._done_count += 1\n",
    "        self._empty_bins_sum += empty_bins / (N_bins + eps)\n",
    "        self._unused_data_sum += unused_data / (total_data + eps)\n",
    "\n",
    "        if self._store_masks:\n",
    "            data = np.hstack([data, mask.astype(np.float32)])\n",
    "\n",
    "        # create new header\n",
    "        new_header = []\n",
    "        for channel in self._id_to_channel:\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                values = self._possible_values[channel]\n",
    "                for value in values:\n",
    "                    new_header.append(channel + \"->\" + value)\n",
    "            else:\n",
    "                new_header.append(channel)\n",
    "\n",
    "        if self._store_masks:\n",
    "            for i in range(len(self._id_to_channel)):\n",
    "                channel = self._id_to_channel[i]\n",
    "                new_header.append(\"mask->\" + channel)\n",
    "\n",
    "        new_header = \",\".join(new_header)\n",
    "\n",
    "        return (data, new_header)\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(\"statistics of discretizer:\")\n",
    "        print(\"\\tconverted {} examples\".format(self._done_count))\n",
    "        print(\"\\taverage unused data = {:.2f} percent\".format(100.0 * self._unused_data_sum / self._done_count))\n",
    "        print(\"\\taverage empty  bins = {:.2f} percent\".format(100.0 * self._empty_bins_sum / self._done_count))\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, fields=None):\n",
    "        self._means = None\n",
    "        self._stds = None\n",
    "        self._fields = None\n",
    "        if fields is not None:\n",
    "            self._fields = [col for col in fields]\n",
    "\n",
    "        self._sum_x = None\n",
    "        self._sum_sq_x = None\n",
    "        self._count = 0\n",
    "\n",
    "    def _feed_data(self, x):\n",
    "        x = np.array(x)\n",
    "        self._count += x.shape[0]\n",
    "        if self._sum_x is None:\n",
    "            self._sum_x = np.sum(x, axis=0)\n",
    "            self._sum_sq_x = np.sum(x**2, axis=0)\n",
    "        else:\n",
    "            self._sum_x += np.sum(x, axis=0)\n",
    "            self._sum_sq_x += np.sum(x**2, axis=0)\n",
    "\n",
    "    def _save_params(self, save_file_path):\n",
    "        eps = 1e-7\n",
    "        with open(save_file_path, \"wb\") as save_file:\n",
    "            N = self._count\n",
    "            self._means = 1.0 / N * self._sum_x\n",
    "            self._stds = np.sqrt(1.0/(N - 1) * (self._sum_sq_x - 2.0 * self._sum_x * self._means + N * self._means**2))\n",
    "            self._stds[self._stds < eps] = eps\n",
    "            pickle.dump(obj={'means': self._means,\n",
    "                             'stds': self._stds},\n",
    "                        file=save_file,\n",
    "                        protocol=2)\n",
    "\n",
    "    def load_params(self, load_file_path):\n",
    "        with open(load_file_path, \"rb\") as load_file:\n",
    "            if platform.python_version()[0] == '2':\n",
    "                dct = pickle.load(load_file)\n",
    "            else:\n",
    "                dct = pickle.load(load_file, encoding='latin1')\n",
    "            self._means = dct['means']\n",
    "            self._stds = dct['stds']\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._fields is None:\n",
    "            fields = range(X.shape[1])\n",
    "        else:\n",
    "            fields = self._fields\n",
    "        ret = 1.0 * X\n",
    "        for col in fields:\n",
    "            ret[:, col] = (X[:, col] - self._means[col]) / self._stds[col]\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e85c1e-4131-49e0-ac6b-dad783a4893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# import \n",
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "R_CLASSES  = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "       'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "       'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
    "       'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "\n",
    "CLASSES = [\n",
    "       'Acute and unspecified renal failure', 'Acute cerebrovascular disease',\n",
    "       'Acute myocardial infarction', 'Cardiac dysrhythmias',\n",
    "       'Chronic kidney disease',\n",
    "       'Chronic obstructive pulmonary disease and bronchiectasis',\n",
    "       'Complications of surgical procedures or medical care',\n",
    "       'Conduction disorders', 'Congestive heart failure; nonhypertensive',\n",
    "       'Coronary atherosclerosis and other heart disease',\n",
    "       'Diabetes mellitus with complications',\n",
    "       'Diabetes mellitus without complication',\n",
    "       'Disorders of lipid metabolism', 'Essential hypertension',\n",
    "       'Fluid and electrolyte disorders', 'Gastrointestinal hemorrhage',\n",
    "       'Hypertension with complications and secondary hypertension',\n",
    "       'Other liver diseases', 'Other lower respiratory disease',\n",
    "       'Other upper respiratory disease',\n",
    "       'Pleurisy; pneumothorax; pulmonary collapse',\n",
    "       'Pneumonia (except that caused by tuberculosis or sexually transmitted disease)',\n",
    "       'Respiratory failure; insufficiency; arrest (adult)',\n",
    "       'Septicemia (except in labor)', 'Shock'\n",
    "    ]\n",
    "                    \n",
    "class MIMIC_CXR_EHR(Dataset):\n",
    "    def __init__(self, args, metadata_with_labels, ehr_ds, cxr_ds, split='train'):\n",
    "        \n",
    "        self.CLASSES = CLASSES\n",
    "        if 'radiology' in args.labels_set:\n",
    "            self.CLASSES = R_CLASSES\n",
    "        \n",
    "        self.metadata_with_labels = metadata_with_labels\n",
    "        self.cxr_files_paired = self.metadata_with_labels.dicom_id.values\n",
    "        self.ehr_files_paired = (self.metadata_with_labels['stay'].values)\n",
    "        self.cxr_files_all = cxr_ds.filenames_loaded\n",
    "        self.ehr_files_all = ehr_ds.names\n",
    "        self.ehr_files_unpaired = list(set(self.ehr_files_all) - set(self.ehr_files_paired))\n",
    "        self.ehr_ds = ehr_ds\n",
    "        self.cxr_ds = cxr_ds\n",
    "        self.args = args\n",
    "        self.split = split\n",
    "        self.data_ratio = self.args.data_ratio \n",
    "        self.filtered_ehr_files_all = []\n",
    "        self.filtered_cxr_files_all = []\n",
    "\n",
    "        if args.data_pairs == 'paired_ehr_cxr':\n",
    "            if args.task == 'decompensation' or args.task == 'length-of-stay':\n",
    "                self.paired_times= (self.metadata_with_labels['period_length'].values)\n",
    "                self.ehr_paired_list = list(zip(self.ehr_files_paired, self.paired_times))\n",
    "                \n",
    "        if split=='test':\n",
    "            self.data_ratio =  1.0\n",
    "        elif split == 'val':\n",
    "            self.data_ratio =  0.0\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.args.data_pairs == 'paired_ehr_cxr':\n",
    "            if self.args.task == 'decompensation' or self.args.task == 'length-of-stay':\n",
    "                ehr_data, labels_ehr = self.ehr_ds[self.ehr_paired_list[index]]\n",
    "            else:\n",
    "                ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_paired[index]]\n",
    "            cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_paired[index]]\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        elif self.args.data_pairs == 'paired_ehr':\n",
    "            ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_paired[index]]\n",
    "            cxr_data, labels_cxr = None, None\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        elif self.args.data_pairs == 'radiology':\n",
    "            ehr_data, labels_ehr = np.zeros((1, 10)), np.zeros(self.args.num_classes)\n",
    "            cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_all[index]]\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        elif self.args.data_pairs == 'partial_ehr':\n",
    "            ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_all[index]]\n",
    "            cxr_data, labels_cxr = None, None\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        \n",
    "        elif self.args.data_pairs == 'partial_ehr_cxr':\n",
    "            if index < len(self.ehr_files_paired):\n",
    "                ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_paired[index]]\n",
    "                cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_paired[index]]\n",
    "            else:\n",
    "                index = random.randint(0, len(self.ehr_files_unpaired)-1) \n",
    "                if self.args.task == 'decompensation' or self.args.task == 'length-of-stay':\n",
    "                    ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_paired[index]]\n",
    "                else:\n",
    "                    ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_paired[index]]\n",
    "                cxr_data, labels_cxr = None, None\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        if 'paired' in self.args.data_pairs:\n",
    "            return len(self.ehr_files_paired)\n",
    "        elif self.args.data_pairs == 'partial_ehr':\n",
    "            return len(self.ehr_files_all)\n",
    "        elif self.args.data_pairs == 'radiology':\n",
    "            return len(self.cxr_files_all)\n",
    "        elif self.args.data_pairs == 'partial_ehr_cxr':\n",
    "            return len(self.ehr_files_paired) + int(self.data_ratio * len(self.ehr_files_unpaired)) \n",
    "        \n",
    "\n",
    "\n",
    "def loadmetadata(args):\n",
    "\n",
    "    data_dir = args.cxr_data_dir\n",
    "    cxr_metadata = pd.read_csv(f'{data_dir}/mimic-cxr-2.0.0-metadata.csv')\n",
    "    icu_stay_metadata = pd.read_csv(f'{args.ehr_data_dir}/root/all_stays.csv')\n",
    "    columns = ['subject_id', 'stay_id', 'intime', 'outtime']\n",
    "\n",
    "    # only common subjects with both icu stay and an xray\n",
    "    cxr_merged_icustays = cxr_metadata.merge(icu_stay_metadata[columns ], how='inner', on='subject_id')\n",
    "    # combine study date time\n",
    "    cxr_merged_icustays['StudyTime'] = cxr_merged_icustays['StudyTime'].apply(lambda x: f'{int(float(x)):06}' )\n",
    "    cxr_merged_icustays['StudyDateTime'] = pd.to_datetime(cxr_merged_icustays['StudyDate'].astype(str) + ' ' + cxr_merged_icustays['StudyTime'].astype(str) ,format=\"%Y%m%d %H%M%S\")\n",
    "    \n",
    "    cxr_merged_icustays.intime=pd.to_datetime(cxr_merged_icustays.intime)\n",
    "    cxr_merged_icustays.outtime=pd.to_datetime(cxr_merged_icustays.outtime)\n",
    "    \n",
    "    \n",
    "    if args.task == 'decompensation' or args.task == 'length-of-stay':\n",
    "        train_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/train_listfile.csv')\n",
    "        train_listfile.columns = ['stay' , 'period_length' , 'stay_id' ,'y_true', 'intime' , 'endtime']\n",
    "        test_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/test_listfile.csv')\n",
    "        test_listfile.columns = ['stay' , 'period_length' , 'stay_id' ,'y_true', 'intime' , 'endtime']\n",
    "        val_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/val_listfile.csv')\n",
    "        val_listfile.columns = ['stay' , 'period_length' , 'stay_id' ,'y_true', 'intime' , 'endtime']\n",
    "        listfile = train_listfile.append(test_listfile)\n",
    "        listfile = listfile.append(val_listfile)\n",
    "        listfile['subject_id'] = listfile['stay'].apply(lambda x: x.split(\"_\")[0])\n",
    "        #print(listfile.head)\n",
    "\n",
    "        columns2 = ['subject_id', 'endtime']\n",
    "        listfile['subject_id'] = listfile['subject_id'].astype('int64')\n",
    "        cxr_merged_icustays = cxr_merged_icustays.merge(listfile[columns2], how='inner', on='subject_id')\n",
    "        cxr_merged_icustays.endtime=pd.to_datetime(cxr_merged_icustays.endtime)\n",
    "        cxr_merged_icustays_during = cxr_merged_icustays.loc[((cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&(cxr_merged_icustays.StudyDateTime<=cxr_merged_icustays.endtime))]\n",
    "        \n",
    "    \n",
    "    if args.task == 'in-hospital-mortality':\n",
    "        end_time = cxr_merged_icustays.intime + pd.DateOffset(hours=48)\n",
    "        cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=end_time))]\n",
    "\n",
    "    if args.task == 'phenotyping' or args.task == 'readmission' :\n",
    "        end_time = cxr_merged_icustays.outtime\n",
    "        cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=end_time))]\n",
    "\n",
    "    # cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=cxr_merged_icustays.outtime))]\n",
    "    # select cxrs with the ViewPosition == 'AP\n",
    "    cxr_merged_icustays_AP = cxr_merged_icustays_during[cxr_merged_icustays_during['ViewPosition'] == 'AP']\n",
    "\n",
    "    groups = cxr_merged_icustays_AP.groupby('stay_id')\n",
    "\n",
    "    groups_selected = []\n",
    "    for group in groups:\n",
    "        # select the latest cxr for the icu stay\n",
    "        selected = group[1].sort_values('StudyDateTime').tail(1).reset_index()\n",
    "        groups_selected.append(selected)\n",
    "    groups = pd.concat(groups_selected, ignore_index=True)\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    # groups['cxr_length'] = (groups['StudyDateTime'] - groups['intime']).astype('timedelta64[h]')\n",
    "    return groups\n",
    "\n",
    "# def \n",
    "def load_cxr_ehr(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds):\n",
    "\n",
    "    cxr_merged_icustays = loadmetadata(args) \n",
    "\n",
    "    # if args.task == 'decompensation' or args.task == 'length-of-stay':\n",
    "    #     splits_labels_train = pd.read_csv(f'/scratch/tmp/{args.task}/train_listfile.csv')\n",
    "    #     splits_labels_val = pd.read_csv(f'/scratch/tmp/{args.task}/val_listfile.csv')\n",
    "    #     splits_labels_test = pd.read_csv(f'/scratch/tmp/{args.task}/test_listfile.csv')\n",
    "    # else:\n",
    "    splits_labels_train = pd.read_csv(f'{args.ehr_data_dir}/{args.task}/train_listfile.csv')\n",
    "    splits_labels_val = pd.read_csv(f'{args.ehr_data_dir}/{args.task}/val_listfile.csv')\n",
    "    splits_labels_test = pd.read_csv(f'{args.ehr_data_dir}/{args.task}/test_listfile.csv')\n",
    "        \n",
    "    train_meta_with_labels = cxr_merged_icustays.merge(splits_labels_train, how='inner', on='stay_id')\n",
    "    val_meta_with_labels = cxr_merged_icustays.merge(splits_labels_val, how='inner', on='stay_id')\n",
    "    test_meta_with_labels = cxr_merged_icustays.merge(splits_labels_test, how='inner', on='stay_id')\n",
    "    print(\"does this contain time?:\",train_meta_with_labels.head())\n",
    "    \n",
    "    train_ds = MIMIC_CXR_EHR(args, train_meta_with_labels, ehr_train_ds, cxr_train_ds)\n",
    "    val_ds = MIMIC_CXR_EHR(args, val_meta_with_labels, ehr_val_ds, cxr_val_ds, split='val')\n",
    "    test_ds = MIMIC_CXR_EHR(args, test_meta_with_labels, ehr_test_ds, cxr_test_ds, split='test')\n",
    "    \n",
    "    if args.task == 'decompensation' or args.task == 'length-of-stay':\n",
    "        print(\"big one\")\n",
    "        train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=True)\n",
    "        val_dl = DataLoader(val_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "        test_dl = DataLoader(test_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "    else:\n",
    "        train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=True)\n",
    "        val_dl = DataLoader(val_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "        test_dl = DataLoader(test_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "\n",
    "    return train_dl, val_dl, test_dl\n",
    "\n",
    "def printPrevalence(merged_file, args):\n",
    "    if args.labels_set == 'pheno':\n",
    "        total_rows = len(merged_file)\n",
    "        print(merged_file[CLASSES].sum()/total_rows)\n",
    "    else:\n",
    "        total_rows = len(merged_file)\n",
    "        print(merged_file['y_true'].value_counts())\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "def my_collate(batch):\n",
    "    x = [item[0] for item in batch]\n",
    "    pairs = [False if item[1] is None else True for item in batch]\n",
    "    img = torch.stack([torch.zeros(3, 224, 224) if item[1] is None else item[1] for item in batch])\n",
    "    x, seq_length = pad_zeros(x)\n",
    "    targets_ehr = np.array([item[2] for item in batch])\n",
    "    targets_cxr = torch.stack([torch.zeros(14) if item[3] is None else item[3] for item in batch])\n",
    "    return [x, img, targets_ehr, targets_cxr, seq_length, pairs]\n",
    "\n",
    "def pad_zeros(arr, min_length=None):\n",
    "    dtype = arr[0].dtype\n",
    "    seq_length = [x.shape[0] for x in arr]\n",
    "    max_len = max(seq_length)\n",
    "    ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "           for x in arr]\n",
    "    if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "        ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "               for x in ret]\n",
    "    return np.array(ret), seq_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409e5bab-97e7-4541-b437-a4fe98f7eb0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trainers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0926f3255d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion_trainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFusionTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmtm_trainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMMTMTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaft_trainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDAFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trainers'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "from trainers.fusion_trainer import FusionTrainer\n",
    "from trainers.mmtm_trainer import MMTMTrainer\n",
    "from trainers.daft_trainer import DAFTTrainer\n",
    "\n",
    "# from ehr_utils.preprocessing import Discretizer, Normalizer\n",
    "from datasets.ehr_dataset import get_datasets\n",
    "from datasets.cxr_dataset import get_cxr_datasets\n",
    "# from datasets.fusion import load_cxr_ehr\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from arguments import args_parser\n",
    "\n",
    "parser = args_parser()\n",
    "args = parser.parse_args([ \n",
    "'--vision-backbone', 'resnet34' ,\n",
    "'--resize', '256' , \n",
    "'--task' , 'in-hospital-mortality' ,\n",
    "'--epochs' , '2' , \n",
    "'--batch_size' , '2' , '--lr' , '0.8' ,\n",
    "'--mode' , 'train' ,\n",
    "'--fusion_type' , 'None' ,\n",
    "'--save_dir' , '/scratch/se1525/mml-ssl/checkpoints/phenotyping/models' ,\n",
    " '--ehr_data_dir', '/scratch/fs999/shamoutlab/data/mimic-iv-extracted',\n",
    "'--data_pairs', 'paired_ehr_cxr', \n",
    "'--fusion_type' , 'uni_ehr', \n",
    "'--num_classes' , '1'])\n",
    "\n",
    "\n",
    "# add more arguments here ...\n",
    "# args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "if args.missing_token is not None:\n",
    "    from trainers.fusion_tokens_trainer import FusionTokensTrainer as FusionTrainer\n",
    "    \n",
    "path = Path(args.save_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "seed = 1002\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def read_timeseries(args):\n",
    "    path = f'{args.ehr_data_dir}/{args.task}/train/14991576_episode3_timeseries.csv'\n",
    "    ret = []\n",
    "    with open(path, \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        assert header[0] == \"Hours\"\n",
    "        for line in tsfile:\n",
    "            mas = line.strip().split(',')\n",
    "            ret.append(np.array(mas))\n",
    "    return np.stack(ret)\n",
    "    \n",
    "\n",
    "discretizer = Discretizer(timestep=float(args.timestep),\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero')\n",
    "\n",
    "\n",
    "discretizer_header = discretizer.transform(read_timeseries(args))[1].split(',')\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "normalizer_state = args.normalizer_state\n",
    "if normalizer_state is None:\n",
    "    normalizer_state = 'normalizers/ph_ts{}.input_str:previous.start_time:zero.normalizer'.format(args.timestep)\n",
    "    normalizer_state = os.path.join(os.path.dirname('/scratch/se1525/mml-ssl/medfuse_baseline/'), normalizer_state)\n",
    "normalizer.load_params(normalizer_state)\n",
    "\n",
    "ehr_train_ds, ehr_val_ds, ehr_test_ds = get_datasets(discretizer, normalizer, args)\n",
    "\n",
    "cxr_train_ds, cxr_val_ds, cxr_test_ds = get_cxr_datasets(args)\n",
    "\n",
    "#print(\" ehr_train_ds\" , ehr_train_ds[('16918793_episode1_timeseries.csv', 27.0)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425b5d1-358b-4568-935b-b7736326cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = load_cxr_ehr(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Medfuse",
   "language": "python",
   "name": "medfuse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
