{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b05a88b-70b7-4dce-8f10-640b3ddf185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataframe (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>note_type</th>\n",
       "      <th>note_seq</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032-RR-14</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>RR</td>\n",
       "      <td>14</td>\n",
       "      <td>2180-05-06 21:19:00</td>\n",
       "      <td>2180-05-06 23:32:00</td>\n",
       "      <td>EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032-RR-15</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>RR</td>\n",
       "      <td>15</td>\n",
       "      <td>2180-05-06 23:00:00</td>\n",
       "      <td>2180-05-06 23:26:00</td>\n",
       "      <td>EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032-RR-16</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>RR</td>\n",
       "      <td>16</td>\n",
       "      <td>2180-05-07 09:55:00</td>\n",
       "      <td>2180-05-07 11:15:00</td>\n",
       "      <td>INDICATION:  ___ HCV cirrhosis c/b ascites, hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032-RR-18</td>\n",
       "      <td>10000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RR</td>\n",
       "      <td>18</td>\n",
       "      <td>2180-06-03 12:46:00</td>\n",
       "      <td>2180-06-03 14:01:00</td>\n",
       "      <td>EXAMINATION:  Ultrasound-guided paracentesis.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032-RR-20</td>\n",
       "      <td>10000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RR</td>\n",
       "      <td>20</td>\n",
       "      <td>2180-07-08 13:18:00</td>\n",
       "      <td>2180-07-08 14:15:00</td>\n",
       "      <td>EXAMINATION:  Paracentesis\\n\\nINDICATION:  ___...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          note_id  subject_id     hadm_id note_type  note_seq  \\\n",
       "0  10000032-RR-14    10000032  22595853.0        RR        14   \n",
       "1  10000032-RR-15    10000032  22595853.0        RR        15   \n",
       "2  10000032-RR-16    10000032  22595853.0        RR        16   \n",
       "3  10000032-RR-18    10000032         NaN        RR        18   \n",
       "4  10000032-RR-20    10000032         NaN        RR        20   \n",
       "\n",
       "            charttime            storetime  \\\n",
       "0 2180-05-06 21:19:00  2180-05-06 23:32:00   \n",
       "1 2180-05-06 23:00:00  2180-05-06 23:26:00   \n",
       "2 2180-05-07 09:55:00  2180-05-07 11:15:00   \n",
       "3 2180-06-03 12:46:00  2180-06-03 14:01:00   \n",
       "4 2180-07-08 13:18:00  2180-07-08 14:15:00   \n",
       "\n",
       "                                                text  \n",
       "0  EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...  \n",
       "1  EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ...  \n",
       "2  INDICATION:  ___ HCV cirrhosis c/b ascites, hi...  \n",
       "3  EXAMINATION:  Ultrasound-guided paracentesis.\\...  \n",
       "4  EXAMINATION:  Paracentesis\\n\\nINDICATION:  ___...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column types:\n",
      "note_id               object\n",
      "subject_id             int64\n",
      "hadm_id              float64\n",
      "note_type             object\n",
      "note_seq               int64\n",
      "charttime     datetime64[ns]\n",
      "storetime             object\n",
      "text                  object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/scratch/baj321/MIMIC-Note/physionet.org/files/mimic-iv-note/2.2/note/radiology.csv'\n",
    "df_sample = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'charttime' to datetime\n",
    "df_sample['charttime'] = pd.to_datetime(df_sample['charttime'])\n",
    "\n",
    "# Display the first few rows of the dataframe to get an overview\n",
    "print(\"First few rows of the dataframe (sample):\")\n",
    "display(df_sample.head())\n",
    "\n",
    "# Print the column types\n",
    "print(\"Column types:\")\n",
    "print(df_sample.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c480f805-d2d2-4d24-b722-a5f905b80eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataframe (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>note_type</th>\n",
       "      <th>note_seq</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032-RR-14</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>RR</td>\n",
       "      <td>14</td>\n",
       "      <td>2180-05-06 21:19:00</td>\n",
       "      <td>2180-05-06 23:32:00</td>\n",
       "      <td>EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032-RR-15</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>RR</td>\n",
       "      <td>15</td>\n",
       "      <td>2180-05-06 23:00:00</td>\n",
       "      <td>2180-05-06 23:26:00</td>\n",
       "      <td>EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032-RR-16</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>RR</td>\n",
       "      <td>16</td>\n",
       "      <td>2180-05-07 09:55:00</td>\n",
       "      <td>2180-05-07 11:15:00</td>\n",
       "      <td>INDICATION:  ___ HCV cirrhosis c/b ascites, hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032-RR-18</td>\n",
       "      <td>10000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RR</td>\n",
       "      <td>18</td>\n",
       "      <td>2180-06-03 12:46:00</td>\n",
       "      <td>2180-06-03 14:01:00</td>\n",
       "      <td>EXAMINATION:  Ultrasound-guided paracentesis.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032-RR-20</td>\n",
       "      <td>10000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RR</td>\n",
       "      <td>20</td>\n",
       "      <td>2180-07-08 13:18:00</td>\n",
       "      <td>2180-07-08 14:15:00</td>\n",
       "      <td>EXAMINATION:  Paracentesis\\n\\nINDICATION:  ___...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          note_id  subject_id     hadm_id note_type  note_seq  \\\n",
       "0  10000032-RR-14    10000032  22595853.0        RR        14   \n",
       "1  10000032-RR-15    10000032  22595853.0        RR        15   \n",
       "2  10000032-RR-16    10000032  22595853.0        RR        16   \n",
       "3  10000032-RR-18    10000032         NaN        RR        18   \n",
       "4  10000032-RR-20    10000032         NaN        RR        20   \n",
       "\n",
       "             charttime            storetime  \\\n",
       "0  2180-05-06 21:19:00  2180-05-06 23:32:00   \n",
       "1  2180-05-06 23:00:00  2180-05-06 23:26:00   \n",
       "2  2180-05-07 09:55:00  2180-05-07 11:15:00   \n",
       "3  2180-06-03 12:46:00  2180-06-03 14:01:00   \n",
       "4  2180-07-08 13:18:00  2180-07-08 14:15:00   \n",
       "\n",
       "                                                text  \n",
       "0  EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...  \n",
       "1  EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ...  \n",
       "2  INDICATION:  ___ HCV cirrhosis c/b ascites, hi...  \n",
       "3  EXAMINATION:  Ultrasound-guided paracentesis.\\...  \n",
       "4  EXAMINATION:  Paracentesis\\n\\nINDICATION:  ___...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a sample of the CSV file first to get an overview\n",
    "file_path = '/scratch/baj321/MIMIC-Note/physionet.org/files/mimic-iv-note/2.2/note/radiology.csv'\n",
    "# Display the first few rows of the dataframe to get an overview\n",
    "df_sample = pd.read_csv(file_path)\n",
    "\n",
    "print(\"First few rows of the dataframe (sample):\")\n",
    "display(df_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0b1d721-e34a-4443-b3ee-3f3548a5d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ORGAN)\n",
      "\n",
      "INDICATION:  ___ year-old female with cirrhosis, jaundice.\n",
      "\n",
      "TECHNIQUE:  Grey scale and color Doppler ultrasound images of the abdomen were\n",
      "obtained.\n",
      "\n",
      "COMPARISON:  None.\n",
      "\n",
      "FINDINGS: \n",
      "\n",
      "LIVER: The liver is coarsened and nodular in echotexture. There is no focal\n",
      "liver mass. Main portal vein and its major branches are patent with normal\n",
      "hepatopetal flow.  The main hepatic artery shows normal arterial waveform. \n",
      "There is a small amount of ascites.\n",
      "\n",
      "BILE DUCTS: There is no intrahepatic biliary dilation. The CBD measures 4 mm.\n",
      "\n",
      "GALLBLADDER: The gallbladder is contracted with a shadowing gallstone.\n",
      "\n",
      "PANCREAS: Imaged portion of the pancreas appears within normal limits, without\n",
      "masses or pancreatic ductal dilation, with portions of the pancreatic tail\n",
      "obscured by overlying bowel gas.\n",
      "\n",
      "SPLEEN: Normal echogenicity, measuring 13.5 cm.\n",
      "\n",
      "KIDNEYS: The right kidney measures 12.1 cm. The left kidney measures 13.4 cm.\n",
      "Normal cortical echogenicity and corticomedullary differentiation is seen\n",
      "bilaterally.\n",
      "RETROPERITONEUM: Visualized portions of aorta and IVC are within normal\n",
      "limits.\n",
      "\n",
      "IMPRESSION: \n",
      "\n",
      "1. Nodular appearance of the liver compatible with cirrhosis. Signs of portal\n",
      "hypertension including small amount of ascites and splenomegaly.\n",
      "2.  Cholelithiasis.\n",
      "3.  Patent portal veins with normal hepatopetal flow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_sample.iloc[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f16a28d-4594-42f2-80a6-020915ccd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/baj321/.local/lib/python3.12/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /home/baj321/.local/lib/python3.12/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/baj321/.local/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/baj321/.local/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/baj321/.local/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/baj321/.local/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/baj321/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715df1a5-5c3e-4cff-ad40-f26a038aa25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arguments import args_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a92395-6039-4a6b-a65a-3272a7688165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Discretizer:\n",
    "    def __init__(self, timestep=0.8, store_masks=True, impute_strategy='zero', start_time='zero',\n",
    "                 config_path= 'ehr_utils/resources/discretizer_config.json'):\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "            self._id_to_channel = config['id_to_channel']\n",
    "            self._channel_to_id = dict(zip(self._id_to_channel, range(len(self._id_to_channel))))\n",
    "            self._is_categorical_channel = config['is_categorical_channel']\n",
    "            self._possible_values = config['possible_values']\n",
    "            self._normal_values = config['normal_values']\n",
    "\n",
    "        self._header = [\"Hours\"] + self._id_to_channel\n",
    "        self._timestep = timestep\n",
    "        self._store_masks = store_masks\n",
    "        self._start_time = start_time\n",
    "        self._impute_strategy = impute_strategy\n",
    "\n",
    "        # for statistics\n",
    "        self._done_count = 0\n",
    "        self._empty_bins_sum = 0\n",
    "        self._unused_data_sum = 0\n",
    "\n",
    "    def transform(self, X, header=None, end=None):\n",
    "        if header is None:\n",
    "            header = self._header\n",
    "        assert header[0] == \"Hours\"\n",
    "        eps = 1e-6\n",
    "\n",
    "        N_channels = len(self._id_to_channel)\n",
    "        ts = [float(row[0]) for row in X]\n",
    "        for i in range(len(ts) - 1):\n",
    "            assert ts[i] < ts[i+1] + eps\n",
    "\n",
    "        if self._start_time == 'relative':\n",
    "            first_time = ts[0]\n",
    "        elif self._start_time == 'zero':\n",
    "            first_time = 0\n",
    "        else:\n",
    "            raise ValueError(\"start_time is invalid\")\n",
    "\n",
    "        if end is None:\n",
    "            max_hours = max(ts) - first_time\n",
    "        else:\n",
    "            max_hours = end - first_time\n",
    "\n",
    "        N_bins = int(max_hours / self._timestep + 1.0 - eps)\n",
    "\n",
    "        cur_len = 0\n",
    "        begin_pos = [0 for i in range(N_channels)]\n",
    "        end_pos = [0 for i in range(N_channels)]\n",
    "        for i in range(N_channels):\n",
    "            channel = self._id_to_channel[i]\n",
    "            begin_pos[i] = cur_len\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                end_pos[i] = begin_pos[i] + len(self._possible_values[channel])\n",
    "            else:\n",
    "                end_pos[i] = begin_pos[i] + 1\n",
    "            cur_len = end_pos[i]\n",
    "\n",
    "        data = np.zeros(shape=(N_bins, cur_len), dtype=float)\n",
    "        mask = np.zeros(shape=(N_bins, N_channels), dtype=int)\n",
    "        original_value = [[\"\" for j in range(N_channels)] for i in range(N_bins)]\n",
    "        total_data = 0\n",
    "        unused_data = 0\n",
    "\n",
    "        def write(data, bin_id, channel, value, begin_pos):\n",
    "            channel_id = self._channel_to_id[channel]\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                category_id = self._possible_values[channel].index(value)\n",
    "                N_values = len(self._possible_values[channel])\n",
    "                one_hot = np.zeros((N_values,))\n",
    "                one_hot[category_id] = 1\n",
    "                for pos in range(N_values):\n",
    "                    data[bin_id, begin_pos[channel_id] + pos] = one_hot[pos]\n",
    "            else:\n",
    "                data[bin_id, begin_pos[channel_id]] = float(value)\n",
    "\n",
    "        for row in X:\n",
    "            t = float(row[0]) - first_time\n",
    "            if t > max_hours + eps:\n",
    "                continue\n",
    "            bin_id = int(t / self._timestep - eps)\n",
    "            assert 0 <= bin_id < N_bins\n",
    "\n",
    "            for j in range(1, len(row)):\n",
    "                if row[j] == \"\":\n",
    "                    continue\n",
    "                channel = header[j]\n",
    "                channel_id = self._channel_to_id[channel]\n",
    "\n",
    "                total_data += 1\n",
    "                if mask[bin_id][channel_id] == 1:\n",
    "                    unused_data += 1\n",
    "                mask[bin_id][channel_id] = 1\n",
    "\n",
    "                write(data, bin_id, channel, row[j], begin_pos)\n",
    "                original_value[bin_id][channel_id] = row[j]\n",
    "\n",
    "        # impute missing values\n",
    "\n",
    "        if self._impute_strategy not in ['zero', 'normal_value', 'previous', 'next']:\n",
    "            raise ValueError(\"impute strategy is invalid\")\n",
    "\n",
    "        if self._impute_strategy in ['normal_value', 'previous']:\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if self._impute_strategy == 'normal_value':\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    if self._impute_strategy == 'previous':\n",
    "                        if len(prev_values[channel_id]) == 0:\n",
    "                            imputed_value = self._normal_values[channel]\n",
    "                        else:\n",
    "                            imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        if self._impute_strategy == 'next':\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins-1, -1, -1):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if len(prev_values[channel_id]) == 0:\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    else:\n",
    "                        imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        empty_bins = np.sum([1 - min(1, np.sum(mask[i, :])) for i in range(N_bins)])\n",
    "        self._done_count += 1\n",
    "        self._empty_bins_sum += empty_bins / (N_bins + eps)\n",
    "        self._unused_data_sum += unused_data / (total_data + eps)\n",
    "\n",
    "        if self._store_masks:\n",
    "            data = np.hstack([data, mask.astype(np.float32)])\n",
    "\n",
    "        # create new header\n",
    "        new_header = []\n",
    "        for channel in self._id_to_channel:\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                values = self._possible_values[channel]\n",
    "                for value in values:\n",
    "                    new_header.append(channel + \"->\" + value)\n",
    "            else:\n",
    "                new_header.append(channel)\n",
    "\n",
    "        if self._store_masks:\n",
    "            for i in range(len(self._id_to_channel)):\n",
    "                channel = self._id_to_channel[i]\n",
    "                new_header.append(\"mask->\" + channel)\n",
    "\n",
    "        new_header = \",\".join(new_header)\n",
    "\n",
    "        return (data, new_header)\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(\"statistics of discretizer:\")\n",
    "        print(\"\\tconverted {} examples\".format(self._done_count))\n",
    "        print(\"\\taverage unused data = {:.2f} percent\".format(100.0 * self._unused_data_sum / self._done_count))\n",
    "        print(\"\\taverage empty  bins = {:.2f} percent\".format(100.0 * self._empty_bins_sum / self._done_count))\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, fields=None):\n",
    "        self._means = None\n",
    "        self._stds = None\n",
    "        self._fields = None\n",
    "        if fields is not None:\n",
    "            self._fields = [col for col in fields]\n",
    "\n",
    "        self._sum_x = None\n",
    "        self._sum_sq_x = None\n",
    "        self._count = 0\n",
    "\n",
    "    def _feed_data(self, x):\n",
    "        x = np.array(x)\n",
    "        self._count += x.shape[0]\n",
    "        if self._sum_x is None:\n",
    "            self._sum_x = np.sum(x, axis=0)\n",
    "            self._sum_sq_x = np.sum(x**2, axis=0)\n",
    "        else:\n",
    "            self._sum_x += np.sum(x, axis=0)\n",
    "            self._sum_sq_x += np.sum(x**2, axis=0)\n",
    "\n",
    "    def _save_params(self, save_file_path):\n",
    "        eps = 1e-7\n",
    "        with open(save_file_path, \"wb\") as save_file:\n",
    "            N = self._count\n",
    "            self._means = 1.0 / N * self._sum_x\n",
    "            self._stds = np.sqrt(1.0/(N - 1) * (self._sum_sq_x - 2.0 * self._sum_x * self._means + N * self._means**2))\n",
    "            self._stds[self._stds < eps] = eps\n",
    "            pickle.dump(obj={'means': self._means,\n",
    "                             'stds': self._stds},\n",
    "                        file=save_file,\n",
    "                        protocol=2)\n",
    "\n",
    "    def load_params(self, load_file_path):\n",
    "        with open(load_file_path, \"rb\") as load_file:\n",
    "            if platform.python_version()[0] == '2':\n",
    "                dct = pickle.load(load_file)\n",
    "            else:\n",
    "                dct = pickle.load(load_file, encoding='latin1')\n",
    "            self._means = dct['means']\n",
    "            self._stds = dct['stds']\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._fields is None:\n",
    "            fields = range(X.shape[1])\n",
    "        else:\n",
    "            fields = self._fields\n",
    "        ret = 1.0 * X\n",
    "        for col in fields:\n",
    "            ret[:, col] = (X[:, col] - self._means[col]) / self._stds[col]\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71556c10-bb36-4a07-a784-ae5e05998cfc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import pandas as pd \n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# # import \n",
    "# import glob\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "# import random\n",
    "\n",
    "\n",
    "# R_CLASSES  = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "#        'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "#        'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
    "#        'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "\n",
    "# CLASSES = [\n",
    "#        'Acute and unspecified renal failure', 'Acute cerebrovascular disease',\n",
    "#        'Acute myocardial infarction', 'Cardiac dysrhythmias',\n",
    "#        'Chronic kidney disease',\n",
    "#        'Chronic obstructive pulmonary disease and bronchiectasis',\n",
    "#        'Complications of surgical procedures or medical care',\n",
    "#        'Conduction disorders', 'Congestive heart failure; nonhypertensive',\n",
    "#        'Coronary atherosclerosis and other heart disease',\n",
    "#        'Diabetes mellitus with complications',\n",
    "#        'Diabetes mellitus without complication',\n",
    "#        'Disorders of lipid metabolism', 'Essential hypertension',\n",
    "#        'Fluid and electrolyte disorders', 'Gastrointestinal hemorrhage',\n",
    "#        'Hypertension with complications and secondary hypertension',\n",
    "#        'Other liver diseases', 'Other lower respiratory disease',\n",
    "#        'Other upper respiratory disease',\n",
    "#        'Pleurisy; pneumothorax; pulmonary collapse',\n",
    "#        'Pneumonia (except that caused by tuberculosis or sexually transmitted disease)',\n",
    "#        'Respiratory failure; insufficiency; arrest (adult)',\n",
    "#        'Septicemia (except in labor)', 'Shock'\n",
    "#     ]\n",
    "                    \n",
    "# class MIMIC_CXR_EHR_RR_DN(Dataset):\n",
    "#     def __init__(self, args, metadata_with_labels, ehr_ds, cxr_ds, split='train'):\n",
    "        \n",
    "#         self.CLASSES = CLASSES\n",
    "#         if 'radiology' in args.labels_set:\n",
    "#             self.CLASSES = R_CLASSES\n",
    "        \n",
    "#         self.metadata_with_labels = metadata_with_labels\n",
    "#         self.discharge_notes_paired = self.metadata_with_labels['discharge_text'].values\n",
    "#         self.radiology_notes_paired = self.metadata_with_labels['radiology_text'].values\n",
    "#         self.time_diff = self.metadata_with_labels.time_diff\n",
    "#         self.lower = self.metadata_with_labels.lower\n",
    "#         self.upper = self.metadata_with_labels.upper\n",
    "#         self.cxr_files_paired = self.metadata_with_labels.dicom_id.values\n",
    "#         self.ehr_files_paired = (self.metadata_with_labels['stay'].values)\n",
    "#         self.cxr_files_all = cxr_ds.filenames_loaded\n",
    "#         self.ehr_files_all = ehr_ds.names\n",
    "#         self.ehr_files_unpaired = list(set(self.ehr_files_all) - set(self.ehr_files_paired))\n",
    "#         self.ehr_ds = ehr_ds\n",
    "#         self.cxr_ds = cxr_ds\n",
    "#         self.args = args\n",
    "#         self.split = split\n",
    "#         self.data_ratio = self.args.data_ratio \n",
    "      \n",
    "#         self.paired_times= (self.metadata_with_labels['period_length'].values)\n",
    "#         self.ehr_paired_list = list(zip(self.ehr_files_paired, self.paired_times))\n",
    "\n",
    "                \n",
    "#         if split=='test':\n",
    "#             self.data_ratio =  1.0\n",
    "#         elif split == 'val':\n",
    "#             self.data_ratio =  0.0\n",
    "        \n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         lower = self.metadata_with_labels.iloc[index].lower\n",
    "#         upper = self.metadata_with_labels.iloc[index].upper\n",
    "#         discharge_note = self.discharge_notes_paired[index]\n",
    "#         radiology_note = self.radiology_notes_paired[index] \n",
    "    \n",
    "#         # Determine the appropriate EHR DataFrame based on the task\n",
    "#         if self.args.task == 'decompensation' or self.args.task == 'length-of-stay':\n",
    "#             ehr_df = self.ehr_paired_list\n",
    "#         else:\n",
    "#             ehr_df = self.ehr_files_paired\n",
    "    \n",
    "#         # Initialize labels and data\n",
    "#         ehr_data, labels_ehr = None, None\n",
    "#         cxr_data, labels_cxr = None, None\n",
    "    \n",
    "#         # Handle EHR data loading\n",
    "#         if 'EHR' in self.args.modalities:\n",
    "#             ehr_data, labels_ehr = self.ehr_ds.__getitem__(ehr_df[index], lower, upper)\n",
    "#         else:\n",
    "#             ehr_data, labels_ehr = np.zeros((1, 10)), np.zeros(self.args.num_classes)\n",
    "    \n",
    "#         # Handle CXR data loading\n",
    "#         if 'CXR' in self.args.modalities:\n",
    "#             cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_paired[index]]\n",
    "#         else:\n",
    "#             cxr_data, labels_cxr = np.zeros((1, 10)), np.zeros(self.args.num_classes)\n",
    "    \n",
    "#         return ehr_data, cxr_data, discharge_note, radiology_note, labels_ehr, labels_cxr\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "#     def __len__(self):\n",
    "#         if self.args.task == 'decompensation' or self.args.task == 'length-of-stay':\n",
    "#             ehr_df = self.ehr_paired_list\n",
    "#         else:\n",
    "#             ehr_df = self.ehr_files_paired\n",
    "#         if 'paired' in self.args.data_pairs:\n",
    "#             return len(self.ehr_files_paired)\n",
    "#         elif self.args.data_pairs == 'partial_ehr':\n",
    "#             return len(self.ehr_files_all)\n",
    "#         elif self.args.data_pairs == 'radiology':\n",
    "#             return len(self.cxr_files_all)\n",
    "#         elif self.args.data_pairs == 'partial_ehr_cxr':\n",
    "#             return len(self.ehr_files_paired) + int(self.data_ratio * len(self.ehr_files_unpaired)) \n",
    "        \n",
    "# def loadmetadata(args, discharge_notes, radiology_reports):\n",
    "#     data_dir = args.cxr_data_dir\n",
    "#     cxr_metadata = pd.read_csv(f'{data_dir}/mimic-cxr-2.0.0-metadata.csv')\n",
    "#     icu_stay_metadata = pd.read_csv(f'{args.ehr_data_dir}/root/all_stays.csv')\n",
    "#     columns = ['subject_id', 'stay_id', 'intime', 'outtime', 'hadm_id']\n",
    "\n",
    "#     cxr_merged_icustays = pd.DataFrame()\n",
    "\n",
    "#     if 'EHR' in args.modalities and 'CXR' in args.modalities:\n",
    "#         # Merge EHR and CXR data\n",
    "#         cxr_merged_icustays = cxr_metadata.merge(icu_stay_metadata[columns], how='inner', on='subject_id')\n",
    "#         cxr_merged_icustays.intime = pd.to_datetime(cxr_merged_icustays.intime)\n",
    "#         cxr_merged_icustays.outtime = pd.to_datetime(cxr_merged_icustays.outtime)\n",
    "#     elif 'EHR' in args.modalities:\n",
    "#         cxr_merged_icustays = icu_stay_metadata[columns]\n",
    "#         cxr_merged_icustays['StudyDateTime'] = None\n",
    "#     elif 'CXR' in args.modalities:\n",
    "#         cxr_merged_icustays = cxr_metadata\n",
    "#         cxr_merged_icustays['intime'] = None\n",
    "#         cxr_merged_icustays['outtime'] = None\n",
    "\n",
    "#     if 'CXR' in args.modalities:\n",
    "#         cxr_merged_icustays['StudyTime'] = cxr_merged_icustays['StudyTime'].apply(lambda x: f'{int(float(x)):06}')\n",
    "#         cxr_merged_icustays['StudyDateTime'] = pd.to_datetime(cxr_merged_icustays['StudyDate'].astype(str) + ' ' + cxr_merged_icustays['StudyTime'].astype(str), format=\"%Y%m%d %H%M%S\")\n",
    "\n",
    "#         if 'EHR' in args.modalities:\n",
    "#             cxr_merged_icustays.intime = pd.to_datetime(cxr_merged_icustays.intime)\n",
    "#             cxr_merged_icustays.outtime = pd.to_datetime(cxr_merged_icustays.outtime)\n",
    "    \n",
    "#             cxr_merged_icustays['time_diff'] = cxr_merged_icustays.StudyDateTime - cxr_merged_icustays.intime\n",
    "#             cxr_merged_icustays['time_diff'] = cxr_merged_icustays['time_diff'].apply(lambda x: np.round(x.total_seconds() / 60 / 60, 3))\n",
    "    \n",
    "#             cxr_merged_icustays['full_stay_time'] = cxr_merged_icustays.outtime - cxr_merged_icustays.intime\n",
    "#             cxr_merged_icustays['full_stay_time'] = cxr_merged_icustays['full_stay_time'].apply(lambda x: np.round(x.total_seconds() / 60 / 60, 3))\n",
    "\n",
    "#     if 'RR' in args.modalities or 'DN' in args.modalities:\n",
    "#         dsrr_merge_columns = ['subject_id', 'hadm_id']\n",
    "#         dsrr_columns = ['subject_id', 'hadm_id','charttime', 'text']\n",
    "\n",
    "#     if 'DN' in args.modalities:\n",
    "#         cxr_merged_icustays = cxr_merged_icustays.merge(discharge_notes[dsrr_columns], how='left', on=dsrr_merge_columns)\n",
    "#         cxr_merged_icustays.rename(columns={'text': 'discharge_text'}, inplace=True)\n",
    "#         cxr_merged_icustays.rename(columns={'charttime': 'discharge_charttime'}, inplace=True)\n",
    "#         cxr_merged_icustays['discharge_charttime'] = pd.to_datetime(cxr_merged_icustays['discharge_charttime'])\n",
    "#     else:\n",
    "#         cxr_merged_icustays['discharge_text'] = None\n",
    "\n",
    "#     if 'RR' in args.modalities:\n",
    "#         cxr_merged_icustays = cxr_merged_icustays.merge(radiology_reports[dsrr_columns], how='left', on=dsrr_merge_columns)\n",
    "#         cxr_merged_icustays.rename(columns={'text': 'radiology_text'}, inplace=True)\n",
    "#         cxr_merged_icustays.rename(columns={'charttime': 'radiology_charttime'}, inplace=True)\n",
    "#         cxr_merged_icustays['radiology_charttime'] = pd.to_datetime(cxr_merged_icustays['radiology_charttime'])\n",
    "#     else:\n",
    "#         cxr_merged_icustays['radiology_text'] = None\n",
    "    \n",
    "#     cxr_merged_icustays_during = cxr_merged_icustays\n",
    "\n",
    "#     if args.task == 'decompensation' or args.task == 'length-of-stay':\n",
    "#         train_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/train_listfile.csv')\n",
    "#         train_listfile.columns = ['stay', 'period_length', 'stay_id', 'y_true', 'intime', 'endtime']\n",
    "#         test_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/test_listfile.csv')\n",
    "#         test_listfile.columns = ['stay', 'period_length', 'stay_id', 'y_true', 'intime', 'endtime']\n",
    "#         val_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/val_listfile.csv')\n",
    "#         val_listfile.columns = ['stay', 'period_length', 'stay_id', 'y_true', 'intime', 'endtime']\n",
    "#         listfile = train_listfile.append(test_listfile)\n",
    "#         listfile = listfile.append(val_listfile)\n",
    "#         listfile['subject_id'] = listfile['stay'].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "#         columns2 = ['subject_id', 'endtime']\n",
    "#         listfile['subject_id'] = listfile['subject_id'].astype('int64')\n",
    "#         cxr_merged_icustays = cxr_merged_icustays.merge(listfile[columns2], how='inner', on='subject_id')\n",
    "#         cxr_merged_icustays.endtime = pd.to_datetime(cxr_merged_icustays.endtime)\n",
    "#         if 'CXR' in args.modalities:    \n",
    "#             cxr_merged_icustays_during = cxr_merged_icustays.loc[\n",
    "#                 ((cxr_merged_icustays.StudyDateTime >= cxr_merged_icustays.intime) & (cxr_merged_icustays.StudyDateTime <= cxr_merged_icustays.endtime))]\n",
    "#         if 'DN' in args.modalities:\n",
    "#             cxr_merged_icustays_during = cxr_merged_icustays_during.loc[\n",
    "#                 ((cxr_merged_icustays_during['discharge_charttime'] >= cxr_merged_icustays_during.intime)&(cxr_merged_icustays_during['discharge_charttime'] <= cxr_merged_icustays_during.endtime))]\n",
    "#         if 'RR' in args.modalities:\n",
    "#                 cxr_merged_icustays_during = cxr_merged_icustays_during.loc[\n",
    "#                 ((cxr_merged_icustays_during['radiology_charttime'] >= cxr_merged_icustays_during.intime)&(cxr_merged_icustays_during['radiology_charttime'] <= cxr_merged_icustays_during.endtime))]\n",
    "\n",
    "#     if args.task == 'in-hospital-mortality':\n",
    "#         end_time = cxr_merged_icustays.intime + pd.DateOffset(hours=48)\n",
    "#         if 'CXR' in args.modalities:\n",
    "#             cxr_merged_icustays_during = cxr_merged_icustays.loc[\n",
    "#                 ((cxr_merged_icustays.StudyDateTime >= cxr_merged_icustays.intime) & (cxr_merged_icustays.StudyDateTime <= end_time))]\n",
    "#         if 'DN' in args.modalities:\n",
    "#             print(cxr_merged_icustays_during.head())\n",
    "#             cxr_merged_icustays_during = cxr_merged_icustays_during.loc[\n",
    "#                 ((cxr_merged_icustays_during['discharge_charttime'] >= cxr_merged_icustays_during.intime)&(cxr_merged_icustays_during['discharge_charttime'] <= cxr_merged_icustays_during.intime + pd.DateOffset(hours=48)))]\n",
    "#         if 'RR' in args.modalities:\n",
    "#             cxr_merged_icustays_during = cxr_merged_icustays_during.loc[\n",
    "#                 ((cxr_merged_icustays_during['radiology_charttime'] >= cxr_merged_icustays_during.intime)&(cxr_merged_icustays_during['radiology_charttime'] <= cxr_merged_icustays_during.intime + pd.DateOffset(hours=48)))]\n",
    "\n",
    "#     if args.task == 'phenotyping' or args.task == 'readmission':\n",
    "#         end_time = cxr_merged_icustays.outtime\n",
    "#         if 'CXR' in args.modalities:\n",
    "#             cxr_merged_icustays_during = cxr_merged_icustays.loc[\n",
    "#             ((cxr_merged_icustays.StudyDateTime >= cxr_merged_icustays.intime) & (cxr_merged_icustays.StudyDateTime <= end_time))]\n",
    "#         if 'DN' in args.modalities:\n",
    "#             print(cxr_merged_icustays_during.head())\n",
    "#             cxr_merged_icustays_during = cxr_merged_icustays_during.loc[\n",
    "#                 ((cxr_merged_icustays_during['discharge_charttime'] >= cxr_merged_icustays_during.intime)&(cxr_merged_icustays_during['discharge_charttime'] <= cxr_merged_icustays_during.outtime))]\n",
    "#         if 'RR' in args.modalities:\n",
    "#             cxr_merged_icustays_during = cxr_merged_icustays_during.loc[\n",
    "#                 ((cxr_merged_icustays_during['radiology_charttime'] >= cxr_merged_icustays_during.intime)&(cxr_merged_icustays_during['radiology_charttime'] <= cxr_merged_icustays_during.outtime))]\n",
    "\n",
    "#     cxr_merged_icustays_AP = cxr_merged_icustays_during[cxr_merged_icustays_during['ViewPosition'] == 'AP']\n",
    "\n",
    "#     if args.retrieve_cxr == 'recent':\n",
    "#         groups = cxr_merged_icustays_AP.groupby('stay_id')\n",
    "#         groups_selected = []\n",
    "#         for group in groups:\n",
    "#             # Select the latest CXR for the ICU stay\n",
    "#             selected = group[1].sort_values('StudyDateTime').tail(1).reset_index()\n",
    "#             groups_selected.append(selected)\n",
    "#         groups = pd.concat(groups_selected, ignore_index=True)\n",
    "#         groups['lower'] = 0\n",
    "#         groups['upper'] = groups.full_stay_time\n",
    "#     elif args.retrieve_cxr == 'all':\n",
    "#         print(\"All CXR\")\n",
    "#         groups = cxr_merged_icustays_AP.groupby('study_id').first()\n",
    "#         groups = groups.reset_index()\n",
    "#         groups = groups.groupby('study_id').first().sort_values(by=['stay_id', 'StudyDateTime'])\n",
    "#         groups = groups.reset_index()\n",
    "#         groups['lower'] = 0\n",
    "#         groups['upper'] = groups.time_diff\n",
    "\n",
    "#     return groups\n",
    "\n",
    "\n",
    "# def load_cxr_ehr_rr_dn(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds):\n",
    "#     discharge_notes=pd.read_csv('/scratch/baj321/MIMIC-Note/physionet.org/files/mimic-iv-note/2.2/note/discharge.csv')\n",
    "#     radiology_reports=pd.read_csv('/scratch/baj321/MIMIC-Note/physionet.org/files/mimic-iv-note/2.2/note/radiology.csv')\n",
    "#     cxr_merged_icustays = loadmetadata(args, discharge_notes, radiology_reports) \n",
    "\n",
    "#     splits_labels_train = pd.read_csv(f'{args.ehr_data_dir}/{args.task}/train_listfile.csv')\n",
    "#     splits_labels_val = pd.read_csv(f'{args.ehr_data_dir}/{args.task}/val_listfile.csv')\n",
    "#     splits_labels_test = pd.read_csv(f'{args.ehr_data_dir}/{args.task}/test_listfile.csv')\n",
    "#     train_meta_with_labels = cxr_merged_icustays.merge(splits_labels_train, how='inner', on='stay_id')\n",
    "#     val_meta_with_labels = cxr_merged_icustays.merge(splits_labels_val, how='inner', on='stay_id')\n",
    "#     test_meta_with_labels = cxr_merged_icustays.merge(splits_labels_test, how='inner', on='stay_id')\n",
    "    \n",
    "#     train_ds = MIMIC_CXR_EHR_RR_DN(args, train_meta_with_labels, ehr_train_ds, cxr_train_ds)\n",
    "#     val_ds = MIMIC_CXR_EHR_RR_DN(args, val_meta_with_labels, ehr_val_ds, cxr_val_ds, split='val')\n",
    "#     test_ds = MIMIC_CXR_EHR_RR_DN(args, test_meta_with_labels, ehr_test_ds, cxr_test_ds, split='test')\n",
    "    \n",
    "#     if args.task == 'decompensation' or args.task == 'length-of-stay':\n",
    "#         print(\"big one\")\n",
    "#         train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=True)\n",
    "#         val_dl = DataLoader(val_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "#         test_dl = DataLoader(test_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "#     else:\n",
    "#         train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=True)\n",
    "#         val_dl = DataLoader(val_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "#         test_dl = DataLoader(test_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "\n",
    "#     return train_dl, val_dl, test_dl\n",
    "\n",
    "\n",
    "# def printPrevalence(merged_file, args):\n",
    "#     if args.labels_set == 'pheno':\n",
    "#         total_rows = len(merged_file)\n",
    "#         print(merged_file[CLASSES].sum()/total_rows)\n",
    "#     else:\n",
    "#         total_rows = len(merged_file)\n",
    "#         print(merged_file['y_true'].value_counts())\n",
    "#     # import pdb; pdb.set_trace()\n",
    "\n",
    "# def my_collate(batch):\n",
    "#     x = [item[0] for item in batch]\n",
    "#     pairs = [False if item[1] is None else True for item in batch]\n",
    "#     img = torch.stack([torch.zeros(3, 224, 224) if item[1] is None else item[1] for item in batch])\n",
    "#     x, seq_length = pad_zeros(x)\n",
    "#     discharge_note = [item[2] for item in batch]\n",
    "#     radiology_note = [item[3] for item in batch]\n",
    "#     targets_ehr = np.array([item[4] for item in batch])\n",
    "#     targets_cxr = torch.stack([torch.zeros(14) if item[5] is None else item[5] for item in batch])\n",
    "#     return [x, img, discharge_note, radiology_note, targets_ehr, targets_cxr, seq_length, pairs]\n",
    "\n",
    "# def pad_zeros(arr, min_length=None):\n",
    "#     dtype = arr[0].dtype\n",
    "#     seq_length = [x.shape[0] for x in arr]\n",
    "#     max_len = max(seq_length)\n",
    "#     ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "#            for x in arr]\n",
    "#     if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "#         ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "#                for x in ret]\n",
    "#     return np.array(ret), seq_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409e5bab-97e7-4541-b437-a4fe98f7eb0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(H_mode='predefined-hierarchical', align=0.0, batch_size=2, beta_1=0.9, crop=224, cxr_data_dir='/scratch/fs999/shamoutlab/data/physionet.org/files/mimic-cxr-jpg/2.0.0', daft_activation='linear', data_pairs='paired_ehr_cxr', data_ratio=1.0, depth=1, dim=256, dropout=0.0, ehr_data_dir='/scratch/fs999/shamoutlab/data/mimic-iv-extracted', epochs=2, eval=False, fusion='joint', fusion_type='uni_ehr', imputation='previous', labels_set='pheno', layer_after=4, layers=1, load_state=None, load_state_cxr=None, load_state_ehr=None, lr=0.8, missing_token=None, mmtm_ratio=4, modalities='EHR-CXR-RR', mode='train', network=None, normalizer_state=None, num_classes=1, order='EHR-CXR-RR', patience=15, pretrained=False, rec_dropout=0.0, resize=256, resume=False, retrieve_cxr='recent', run_method='fine_tune', save_dir='/scratch/se1525/mml-ssl/checkpoints/phenotyping/models', task='in-hospital-mortality', timestep=1.0, vision_backbone='resnet34', vision_num_classes=14)\n",
      "/scratch/fs999/shamoutlab/data/mimic-iv-extracted/in-hospital-mortality/train_listfile.csv\n",
      "/scratch/fs999/shamoutlab/data/mimic-iv-extracted/in-hospital-mortality/train\n",
      "/scratch/fs999/shamoutlab/data/mimic-iv-extracted/in-hospital-mortality/train\n",
      "/scratch/fs999/shamoutlab/data/mimic-iv-extracted/in-hospital-mortality/test\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "from trainers.fusion_trainer import FusionTrainer\n",
    "from trainers.mmtm_trainer import MMTMTrainer\n",
    "from trainers.daft_trainer import DAFTTrainer\n",
    "\n",
    "# from ehr_utils.preprocessing import Discretizer, Normalizer\n",
    "from datasets.ehr_dataset import get_datasets\n",
    "from datasets.cxr_dataset import get_cxr_datasets\n",
    "from datasets.Quatrimodal_Fusion import load_cxr_ehr_rr_dn\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from arguments import args_parser\n",
    "\n",
    "parser = args_parser()\n",
    "args = parser.parse_args([ \n",
    "'--vision-backbone', 'resnet34' ,\n",
    "'--modalities', 'EHR-CXR-RR' ,\n",
    "'--resize', '256' , \n",
    "'--task' , 'in-hospital-mortality' ,\n",
    "'--epochs' , '2' , \n",
    "'--batch_size' , '4' , '--lr' , '0.8' ,\n",
    "'--mode' , 'train' ,\n",
    "'--H_mode' , 'predefined-hierarchical' ,\n",
    "'--order' , 'EHR-CXR-RR' ,\n",
    "'--fusion_type' , 'None' ,\n",
    "'--save_dir' , '/scratch/se1525/mml-ssl/checkpoints/phenotyping/models' ,\n",
    " '--ehr_data_dir', '/scratch/fs999/shamoutlab/data/mimic-iv-extracted',\n",
    "'--data_pairs', 'paired_ehr_cxr', \n",
    "'--fusion_type' , 'uni_ehr', \n",
    "'--num_classes' , '1'])\n",
    "\n",
    "\n",
    "# add more arguments here ...\n",
    "# args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "if args.missing_token is not None:\n",
    "    from trainers.fusion_tokens_trainer import FusionTokensTrainer as FusionTrainer\n",
    "    \n",
    "path = Path(args.save_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "seed = 1002\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def read_timeseries(args):\n",
    "    path = f'{args.ehr_data_dir}/{args.task}/train/14991576_episode3_timeseries.csv'\n",
    "    ret = []\n",
    "    with open(path, \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        assert header[0] == \"Hours\"\n",
    "        for line in tsfile:\n",
    "            mas = line.strip().split(',')\n",
    "            ret.append(np.array(mas))\n",
    "    return np.stack(ret)\n",
    "    \n",
    "\n",
    "discretizer = Discretizer(timestep=float(args.timestep),\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero')\n",
    "\n",
    "\n",
    "discretizer_header = discretizer.transform(read_timeseries(args))[1].split(',')\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "normalizer_state = args.normalizer_state\n",
    "if normalizer_state is None:\n",
    "    normalizer_state = 'normalizers/ph_ts{}.input_str:previous.start_time:zero.normalizer'.format(args.timestep)\n",
    "    normalizer_state = os.path.join(os.path.dirname('/scratch/se1525/mml-ssl/medfuse_baseline/'), normalizer_state)\n",
    "normalizer.load_params(normalizer_state)\n",
    "\n",
    "ehr_train_ds, ehr_val_ds, ehr_test_ds = get_datasets(discretizer, normalizer, args)\n",
    "\n",
    "cxr_train_ds, cxr_val_ds, cxr_test_ds = get_cxr_datasets(args)\n",
    "\n",
    "#print(\" ehr_train_ds\" , ehr_train_ds[('16918793_episode1_timeseries.csv', 27.0)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3425b5d1-358b-4568-935b-b7736326cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = load_cxr_ehr_rr_dn(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3df149c-6fb0-450b-b74e-a5b484addf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.DHF_trainer import DHFTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dcffa20-4cb3-425b-915b-591789e0e4fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbaj321\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/baj321/MedFuse/wandb/run-20240715_130437-ydw3ayfz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/baj321/DHF_uni_ehr/runs/ydw3ayfz' target=\"_blank\">wandering-donkey-27</a></strong> to <a href='https://wandb.ai/baj321/DHF_uni_ehr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/baj321/DHF_uni_ehr' target=\"_blank\">https://wandb.ai/baj321/DHF_uni_ehr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/baj321/DHF_uni_ehr/runs/ydw3ayfz' target=\"_blank\">https://wandb.ai/baj321/DHF_uni_ehr/runs/ydw3ayfz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trainer = DHFTrainer(\n",
    "        train_dl, \n",
    "        val_dl, \n",
    "        args,\n",
    "        test_dl\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248fb356-4850-42cc-a7eb-b51a1167f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for fusion_type uni_ehr\n",
      "starting val epoch 1\n",
      "seq_lengths: [62, 69]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (152x69 and 48x384)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/baj321/MedFuse/trainers/DHF_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_eval_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0;31m#self.save_checkpoint(prefix='last')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/baj321/MedFuse/trainers/DHF_trainer.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, dl)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'EHR'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodalities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     \u001b[0mv_ehr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mehr_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m                     \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EHR'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_ehr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                     \u001b[0mr_ehr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mehr_r_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_ehr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/medfuse/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/baj321/MedFuse/models/ehr_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ehr)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mehr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mehr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_ehr_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mehr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mehr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mehr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_ehr_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mehr\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Embed raw EHR data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mehr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mcls_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mehr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Repeat CLS token for batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/medfuse/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/medfuse/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/medfuse/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (152x69 and 48x384)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf81d6b-19a4-4d2a-8fff-48b47a7ce058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "EHR Data: [[[ 1.          0.         -0.03486972 ...  1.          0.\n",
      "    0.        ]\n",
      "  [ 1.          0.         -0.04838953 ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 1.          0.         -0.00332347 ...  1.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 1.          0.          0.00568974 ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 1.          0.          0.00568974 ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 1.          0.          0.00568974 ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 1.          0.          0.00118313 ...  1.          1.\n",
      "    0.        ]\n",
      "  [ 1.          0.         -0.0213499  ...  1.          0.\n",
      "    0.        ]\n",
      "  [ 1.          0.          0.01920956 ...  0.          0.\n",
      "    1.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n",
      "CXR Data: tensor([[[[-2.1179, -2.1179, -2.1179,  ...,  0.7419,  0.7077,  0.7762],\n",
      "          [-2.1179, -2.1179, -2.1179,  ...,  0.8104,  0.7419,  0.7933],\n",
      "          [-2.1179, -2.1179, -2.1179,  ...,  0.8104,  0.7933,  0.8447],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ...,  0.8104,  0.7933,  0.7248],\n",
      "          [-2.1179, -2.1179, -2.1179,  ...,  0.7933,  0.7933,  0.7591],\n",
      "          [-2.1179, -2.1179, -2.1179,  ...,  0.8104,  0.7591,  0.7248]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ...,  0.8880,  0.8529,  0.9230],\n",
      "          [-2.0357, -2.0357, -2.0357,  ...,  0.9580,  0.8880,  0.9405],\n",
      "          [-2.0357, -2.0357, -2.0357,  ...,  0.9580,  0.9405,  0.9930],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ...,  0.9580,  0.9405,  0.8704],\n",
      "          [-2.0357, -2.0357, -2.0357,  ...,  0.9405,  0.9405,  0.9055],\n",
      "          [-2.0357, -2.0357, -2.0357,  ...,  0.9580,  0.9055,  0.8704]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ...,  1.1062,  1.0714,  1.1411],\n",
      "          [-1.8044, -1.8044, -1.8044,  ...,  1.1759,  1.1062,  1.1585],\n",
      "          [-1.8044, -1.8044, -1.8044,  ...,  1.1759,  1.1585,  1.2108],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ...,  1.1759,  1.1585,  1.0888],\n",
      "          [-1.8044, -1.8044, -1.8044,  ...,  1.1585,  1.1585,  1.1237],\n",
      "          [-1.8044, -1.8044, -1.8044,  ...,  1.1759,  1.1237,  1.0888]]],\n",
      "\n",
      "\n",
      "        [[[-1.0733, -1.0219, -0.9534,  ..., -2.0665, -2.0665, -2.0665],\n",
      "          [-1.0219, -0.9877, -1.0562,  ..., -2.0665, -2.0665, -2.1179],\n",
      "          [-0.9363, -0.9363, -0.9877,  ..., -2.0665, -2.0665, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-0.9678, -0.9153, -0.8452,  ..., -1.9832, -1.9832, -1.9832],\n",
      "          [-0.9153, -0.8803, -0.9503,  ..., -1.9832, -1.9832, -2.0357],\n",
      "          [-0.8277, -0.8277, -0.8803,  ..., -1.9832, -1.9832, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-0.7413, -0.6890, -0.6193,  ..., -1.7522, -1.7522, -1.7522],\n",
      "          [-0.6890, -0.6541, -0.7238,  ..., -1.7522, -1.7522, -1.8044],\n",
      "          [-0.6018, -0.6018, -0.6541,  ..., -1.7522, -1.7522, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]])\n",
      "Discharge Note: [None, None]\n",
      "Radiology Note: [None, None]\n",
      "Labels EHR: [0 0]\n",
      "Labels CXR: tensor([[0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader and print the contents of each batch\n",
    "num = 0\n",
    "for i, (ehr_data, cxr_data, discharge_note, radiology_note, labels_ehr, labels_cxr,seq_length, pairs) in enumerate(train_dl):\n",
    "    if i == num:\n",
    "        print(f\"Batch {i+1}:\")\n",
    "        print(f\"EHR Data: {ehr_data}\")\n",
    "        print(f\"CXR Data: {cxr_data}\")\n",
    "        print(f\"Discharge Note: {discharge_note}\")\n",
    "        print(f\"Radiology Note: {radiology_note}\")\n",
    "        print(f\"Labels EHR: {labels_ehr}\")\n",
    "        print(f\"Labels CXR: {labels_cxr}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Optionally, break after the first batch for brevity\n",
    "    if i == num:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c46af-eba0-4e97-8fb2-e4d4b8564549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_dl):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Batch content: {batch}\")\n",
    "\n",
    "    # Optionally, break after the first batch for brevity\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a83d6e-9d8c-457c-b4ce-5912200de6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Medfuse",
   "language": "python",
   "name": "medfuse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
