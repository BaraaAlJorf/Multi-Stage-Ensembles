Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ehr loaded
cxr loaded
==> training
running for fusion_type joint
0
starting val epoch 0
val [0000 / 0050] validation loss: 	0.77787
checkpoint
starting train epoch 0
 epoch [0000 / 0050] [0009/280] eta: 0 Days 14:55:1        lr: 	1.0000E-06 loss: 	0.78703
 epoch [0000 / 0050] [0109/280] eta: 0 Days 2:8:22         lr: 	1.0000E-06 loss: 	0.47740
 epoch [0000 / 0050] [0209/280] eta: 0 Days 1:31:57        lr: 	1.0000E-06 loss: 	0.43982
1
starting val epoch 1
val [0001 / 0050] validation loss: 	0.40688
checkpoint
starting train epoch 1
 epoch [0001 / 0050] [0009/280] eta: 0 Days 1:33:17        lr: 	1.0000E-06 loss: 	0.42870
 epoch [0001 / 0050] [0109/280] eta: 0 Days 1:23:29        lr: 	1.0000E-06 loss: 	0.39829
 epoch [0001 / 0050] [0209/280] eta: 0 Days 1:16:38        lr: 	1.0000E-06 loss: 	0.39079
2
starting val epoch 2
val [0002 / 0050] validation loss: 	0.39224
checkpoint
starting train epoch 2
 epoch [0002 / 0050] [0009/280] eta: 0 Days 1:18:52        lr: 	1.0000E-06 loss: 	0.45075
 epoch [0002 / 0050] [0109/280] eta: 0 Days 1:14:4         lr: 	1.0000E-06 loss: 	0.39356
 epoch [0002 / 0050] [0209/280] eta: 0 Days 1:10:47        lr: 	1.0000E-06 loss: 	0.37271
3
starting val epoch 3
val [0003 / 0050] validation loss: 	0.38689
checkpoint
starting train epoch 3
 epoch [0003 / 0050] [0009/280] eta: 0 Days 1:12:48        lr: 	1.0000E-06 loss: 	0.42901
 epoch [0003 / 0050] [0109/280] eta: 0 Days 1:9:57         lr: 	1.0000E-06 loss: 	0.37747
 epoch [0003 / 0050] [0209/280] eta: 0 Days 1:7:35         lr: 	1.0000E-06 loss: 	0.37444
4
starting val epoch 4
val [0004 / 0050] validation loss: 	0.38573
checkpoint
starting train epoch 4
 epoch [0004 / 0050] [0009/280] eta: 0 Days 1:9:3          lr: 	1.0000E-06 loss: 	0.42797
 epoch [0004 / 0050] [0109/280] eta: 0 Days 1:7:10         lr: 	1.0000E-06 loss: 	0.37277
 epoch [0004 / 0050] [0209/280] eta: 0 Days 1:5:20         lr: 	1.0000E-06 loss: 	0.36730
5
starting val epoch 5
val [0005 / 0050] validation loss: 	0.38731
checkpoint
starting train epoch 5
 epoch [0005 / 0050] [0009/280] eta: 0 Days 1:6:30         lr: 	1.0000E-06 loss: 	0.40572
 epoch [0005 / 0050] [0109/280] eta: 0 Days 1:4:51         lr: 	1.0000E-06 loss: 	0.37180
 epoch [0005 / 0050] [0209/280] eta: 0 Days 1:3:23         lr: 	1.0000E-06 loss: 	0.35618
6
starting val epoch 6
val [0006 / 0050] validation loss: 	0.37023
checkpoint
starting train epoch 6
 epoch [0006 / 0050] [0009/280] eta: 0 Days 1:4:22         lr: 	1.0000E-06 loss: 	0.43433
 epoch [0006 / 0050] [0109/280] eta: 0 Days 1:2:53         lr: 	1.0000E-06 loss: 	0.35801
 epoch [0006 / 0050] [0209/280] eta: 0 Days 1:1:39         lr: 	1.0000E-06 loss: 	0.34260
7
starting val epoch 7
val [0007 / 0050] validation loss: 	0.35905
checkpoint
starting train epoch 7
 epoch [0007 / 0050] [0009/280] eta: 0 Days 1:2:16         lr: 	1.0000E-06 loss: 	0.26417
 epoch [0007 / 0050] [0109/280] eta: 0 Days 1:1:10         lr: 	1.0000E-06 loss: 	0.31117
 epoch [0007 / 0050] [0209/280] eta: 0 Days 1:0:3          lr: 	1.0000E-06 loss: 	0.31234
8
starting val epoch 8
val [0008 / 0050] validation loss: 	0.35053
checkpoint
starting train epoch 8
 epoch [0008 / 0050] [0009/280] eta: 0 Days 1:0:34         lr: 	1.0000E-06 loss: 	0.36071
 epoch [0008 / 0050] [0109/280] eta: 0 Days 0:59:23        lr: 	1.0000E-06 loss: 	0.31876
 epoch [0008 / 0050] [0209/280] eta: 0 Days 0:58:16        lr: 	1.0000E-06 loss: 	0.31466
9
starting val epoch 9
val [0009 / 0050] validation loss: 	0.34243
checkpoint
starting train epoch 9
 epoch [0009 / 0050] [0009/280] eta: 0 Days 0:58:38        lr: 	1.0000E-06 loss: 	0.31460
 epoch [0009 / 0050] [0109/280] eta: 0 Days 0:57:37        lr: 	1.0000E-06 loss: 	0.30743
 epoch [0009 / 0050] [0209/280] eta: 0 Days 0:56:36        lr: 	1.0000E-06 loss: 	0.30223
10
starting val epoch 10
val [0010 / 0050] validation loss: 	0.33740
checkpoint
starting train epoch 10
 epoch [0010 / 0050] [0009/280] eta: 0 Days 0:56:55        lr: 	1.0000E-06 loss: 	0.27643
 epoch [0010 / 0050] [0109/280] eta: 0 Days 0:55:59        lr: 	1.0000E-06 loss: 	0.28447
 epoch [0010 / 0050] [0209/280] eta: 0 Days 0:55:5         lr: 	1.0000E-06 loss: 	0.28719
11
starting val epoch 11
val [0011 / 0050] validation loss: 	0.33166
checkpoint
starting train epoch 11
 epoch [0011 / 0050] [0009/280] eta: 0 Days 0:55:18        lr: 	1.0000E-06 loss: 	0.28800
 epoch [0011 / 0050] [0109/280] eta: 0 Days 0:54:25        lr: 	1.0000E-06 loss: 	0.27869
 epoch [0011 / 0050] [0209/280] eta: 0 Days 0:53:34        lr: 	1.0000E-06 loss: 	0.28131
12
starting val epoch 12
val [0012 / 0050] validation loss: 	0.32897
checkpoint
starting train epoch 12
 epoch [0012 / 0050] [0009/280] eta: 0 Days 0:53:42        lr: 	1.0000E-06 loss: 	0.25360
 epoch [0012 / 0050] [0109/280] eta: 0 Days 0:52:54        lr: 	1.0000E-06 loss: 	0.25485
 epoch [0012 / 0050] [0209/280] eta: 0 Days 0:52:5         lr: 	1.0000E-06 loss: 	0.27422
13
starting val epoch 13
val [0013 / 0050] validation loss: 	0.32823
checkpoint
starting train epoch 13
 epoch [0013 / 0050] [0009/280] eta: 0 Days 0:52:15        lr: 	1.0000E-06 loss: 	0.35401
 epoch [0013 / 0050] [0109/280] eta: 0 Days 0:51:25        lr: 	1.0000E-06 loss: 	0.30134
 epoch [0013 / 0050] [0209/280] eta: 0 Days 0:50:38        lr: 	1.0000E-06 loss: 	0.27339
14
starting val epoch 14
val [0014 / 0050] validation loss: 	0.32892
checkpoint
starting train epoch 14
 epoch [0014 / 0050] [0009/280] eta: 0 Days 0:50:42        lr: 	1.0000E-06 loss: 	0.34579
 epoch [0014 / 0050] [0109/280] eta: 0 Days 0:50:0         lr: 	1.0000E-06 loss: 	0.27511
 epoch [0014 / 0050] [0209/280] eta: 0 Days 0:49:14        lr: 	1.0000E-06 loss: 	0.26852
15
starting val epoch 15
val [0015 / 0050] validation loss: 	0.32688
checkpoint
starting train epoch 15
 epoch [0015 / 0050] [0009/280] eta: 0 Days 0:49:16        lr: 	1.0000E-06 loss: 	0.25284
 epoch [0015 / 0050] [0109/280] eta: 0 Days 0:48:30        lr: 	1.0000E-06 loss: 	0.26038
 epoch [0015 / 0050] [0209/280] eta: 0 Days 0:47:45        lr: 	1.0000E-06 loss: 	0.26431
16
starting val epoch 16
val [0016 / 0050] validation loss: 	0.33126
starting train epoch 16
 epoch [0016 / 0050] [0009/280] eta: 0 Days 0:47:39        lr: 	1.0000E-06 loss: 	0.31112
 epoch [0016 / 0050] [0109/280] eta: 0 Days 0:46:55        lr: 	1.0000E-06 loss: 	0.24422
 epoch [0016 / 0050] [0209/280] eta: 0 Days 0:46:11        lr: 	1.0000E-06 loss: 	0.25917
17
starting val epoch 17
val [0017 / 0050] validation loss: 	0.33096
starting train epoch 17
 epoch [0017 / 0050] [0009/280] eta: 0 Days 0:46:3         lr: 	1.0000E-06 loss: 	0.24917
 epoch [0017 / 0050] [0109/280] eta: 0 Days 0:45:22        lr: 	1.0000E-06 loss: 	0.25619
 epoch [0017 / 0050] [0209/280] eta: 0 Days 0:44:41        lr: 	1.0000E-06 loss: 	0.25319
18
starting val epoch 18
val [0018 / 0050] validation loss: 	0.32802
starting train epoch 18
 epoch [0018 / 0050] [0009/280] eta: 0 Days 0:44:31        lr: 	1.0000E-06 loss: 	0.26820
 epoch [0018 / 0050] [0109/280] eta: 0 Days 0:43:50        lr: 	1.0000E-06 loss: 	0.26150
 epoch [0018 / 0050] [0209/280] eta: 0 Days 0:43:9         lr: 	1.0000E-06 loss: 	0.24931
19
starting val epoch 19
val [0019 / 0050] validation loss: 	0.33040
starting train epoch 19
 epoch [0019 / 0050] [0009/280] eta: 0 Days 0:42:57        lr: 	1.0000E-06 loss: 	0.25947
 epoch [0019 / 0050] [0109/280] eta: 0 Days 0:42:18        lr: 	1.0000E-06 loss: 	0.21675
 epoch [0019 / 0050] [0209/280] eta: 0 Days 0:41:38        lr: 	1.0000E-06 loss: 	0.23336
20
starting val epoch 20
val [0020 / 0050] validation loss: 	0.33217
starting train epoch 20
 epoch [0020 / 0050] [0009/280] eta: 0 Days 0:41:28        lr: 	1.0000E-06 loss: 	0.29204
 epoch [0020 / 0050] [0109/280] eta: 0 Days 0:40:51        lr: 	1.0000E-06 loss: 	0.23832
 epoch [0020 / 0050] [0209/280] eta: 0 Days 0:40:12        lr: 	1.0000E-06 loss: 	0.23864
21
starting val epoch 21
val [0021 / 0050] validation loss: 	0.33125
checkpoint
starting train epoch 21
 epoch [0021 / 0050] [0009/280] eta: 0 Days 0:40:4         lr: 	1.0000E-06 loss: 	0.26464
 epoch [0021 / 0050] [0109/280] eta: 0 Days 0:39:26        lr: 	1.0000E-06 loss: 	0.24550
 epoch [0021 / 0050] [0209/280] eta: 0 Days 0:38:48        lr: 	1.0000E-06 loss: 	0.23776
22
starting val epoch 22
val [0022 / 0050] validation loss: 	0.33561
starting train epoch 22
 epoch [0022 / 0050] [0009/280] eta: 0 Days 0:38:36        lr: 	1.0000E-06 loss: 	0.20018
 epoch [0022 / 0050] [0109/280] eta: 0 Days 0:37:59        lr: 	1.0000E-06 loss: 	0.23422
 epoch [0022 / 0050] [0209/280] eta: 0 Days 0:37:23        lr: 	1.0000E-06 loss: 	0.24592
23
starting val epoch 23
val [0023 / 0050] validation loss: 	0.34783
starting train epoch 23
 epoch [0023 / 0050] [0009/280] eta: 0 Days 0:37:9         lr: 	1.0000E-06 loss: 	0.25020
 epoch [0023 / 0050] [0109/280] eta: 0 Days 0:36:32        lr: 	1.0000E-06 loss: 	0.23443
 epoch [0023 / 0050] [0209/280] eta: 0 Days 0:35:56        lr: 	1.0000E-06 loss: 	0.23694
24
starting val epoch 24
val [0024 / 0050] validation loss: 	0.33700
starting train epoch 24
 epoch [0024 / 0050] [0009/280] eta: 0 Days 0:35:42        lr: 	1.0000E-06 loss: 	0.27675
 epoch [0024 / 0050] [0109/280] eta: 0 Days 0:35:7         lr: 	1.0000E-06 loss: 	0.22615
 epoch [0024 / 0050] [0209/280] eta: 0 Days 0:34:31        lr: 	1.0000E-06 loss: 	0.23209
25
starting val epoch 25
val [0025 / 0050] validation loss: 	0.34119
starting train epoch 25
 epoch [0025 / 0050] [0009/280] eta: 0 Days 0:34:16        lr: 	1.0000E-06 loss: 	0.23883
 epoch [0025 / 0050] [0109/280] eta: 0 Days 0:33:40        lr: 	1.0000E-06 loss: 	0.21144
 epoch [0025 / 0050] [0209/280] eta: 0 Days 0:33:5         lr: 	1.0000E-06 loss: 	0.22611
26
starting val epoch 26
val [0026 / 0050] validation loss: 	0.35806
starting train epoch 26
 epoch [0026 / 0050] [0009/280] eta: 0 Days 0:32:50        lr: 	1.0000E-06 loss: 	0.23593
 epoch [0026 / 0050] [0109/280] eta: 0 Days 0:32:15        lr: 	1.0000E-06 loss: 	0.21550
 epoch [0026 / 0050] [0209/280] eta: 0 Days 0:31:41        lr: 	1.0000E-06 loss: 	0.22485
27
starting val epoch 27
val [0027 / 0050] validation loss: 	0.35527
starting train epoch 27
 epoch [0027 / 0050] [0009/280] eta: 0 Days 0:31:24        lr: 	1.0000E-06 loss: 	0.16687
 epoch [0027 / 0050] [0109/280] eta: 0 Days 0:30:50        lr: 	1.0000E-06 loss: 	0.21443
 epoch [0027 / 0050] [0209/280] eta: 0 Days 0:30:16        lr: 	1.0000E-06 loss: 	0.21720
28
starting val epoch 28
val [0028 / 0050] validation loss: 	0.35520
starting train epoch 28
 epoch [0028 / 0050] [0009/280] eta: 0 Days 0:30:0         lr: 	1.0000E-06 loss: 	0.24284
 epoch [0028 / 0050] [0109/280] eta: 0 Days 0:29:26        lr: 	1.0000E-06 loss: 	0.21746
 epoch [0028 / 0050] [0209/280] eta: 0 Days 0:28:52        lr: 	1.0000E-06 loss: 	0.21026
29
starting val epoch 29
val [0029 / 0050] validation loss: 	0.35692
starting train epoch 29
 epoch [0029 / 0050] [0009/280] eta: 0 Days 0:28:35        lr: 	1.0000E-06 loss: 	0.30092
 epoch [0029 / 0050] [0109/280] eta: 0 Days 0:28:2         lr: 	1.0000E-06 loss: 	0.20150
 epoch [0029 / 0050] [0209/280] eta: 0 Days 0:27:29        lr: 	1.0000E-06 loss: 	0.20493
30
starting val epoch 30
val [0030 / 0050] validation loss: 	0.35542
starting train epoch 30
 epoch [0030 / 0050] [0009/280] eta: 0 Days 0:27:12        lr: 	1.0000E-06 loss: 	0.18053
 epoch [0030 / 0050] [0109/280] eta: 0 Days 0:26:38        lr: 	1.0000E-06 loss: 	0.21975
 epoch [0030 / 0050] [0209/280] eta: 0 Days 0:26:6         lr: 	1.0000E-06 loss: 	0.20935
31
starting val epoch 31
val [0031 / 0050] validation loss: 	0.36839
starting train epoch 31
 epoch [0031 / 0050] [0009/280] eta: 0 Days 0:25:48        lr: 	1.0000E-06 loss: 	0.25861
 epoch [0031 / 0050] [0109/280] eta: 0 Days 0:25:15        lr: 	1.0000E-06 loss: 	0.20735
 epoch [0031 / 0050] [0209/280] eta: 0 Days 0:24:43        lr: 	1.0000E-06 loss: 	0.20171
32
starting val epoch 32
val [0032 / 0050] validation loss: 	0.36618
starting train epoch 32
 epoch [0032 / 0050] [0009/280] eta: 0 Days 0:24:25        lr: 	1.0000E-06 loss: 	0.20064
 epoch [0032 / 0050] [0109/280] eta: 0 Days 0:23:52        lr: 	1.0000E-06 loss: 	0.20685
 epoch [0032 / 0050] [0209/280] eta: 0 Days 0:23:20        lr: 	1.0000E-06 loss: 	0.20845
33
starting val epoch 33
val [0033 / 0050] validation loss: 	0.36209
starting train epoch 33
 epoch [0033 / 0050] [0009/280] eta: 0 Days 0:23:2         lr: 	1.0000E-06 loss: 	0.20154
 epoch [0033 / 0050] [0109/280] eta: 0 Days 0:22:30        lr: 	1.0000E-06 loss: 	0.21391
 epoch [0033 / 0050] [0209/280] eta: 0 Days 0:21:59        lr: 	1.0000E-06 loss: 	0.19510
34
starting val epoch 34
val [0034 / 0050] validation loss: 	0.36780
starting train epoch 34
 epoch [0034 / 0050] [0009/280] eta: 0 Days 0:21:40        lr: 	1.0000E-06 loss: 	0.17829
 epoch [0034 / 0050] [0109/280] eta: 0 Days 0:21:8         lr: 	1.0000E-06 loss: 	0.18059
 epoch [0034 / 0050] [0209/280] eta: 0 Days 0:20:36        lr: 	1.0000E-06 loss: 	0.18996
35
starting val epoch 35
val [0035 / 0050] validation loss: 	0.38309
starting train epoch 35
 epoch [0035 / 0050] [0009/280] eta: 0 Days 0:20:17        lr: 	1.0000E-06 loss: 	0.22249
 epoch [0035 / 0050] [0109/280] eta: 0 Days 0:19:46        lr: 	1.0000E-06 loss: 	0.20659
 epoch [0035 / 0050] [0209/280] eta: 0 Days 0:19:15        lr: 	1.0000E-06 loss: 	0.19599
36
starting val epoch 36
val [0036 / 0050] validation loss: 	0.37174
starting val epoch 0
val [0000 / 0050] validation loss: 	0.33881
Acute and unspecified renal failure                                                        & 0.832(0.861, 0.800) & 0.511 (0.590, 0.436)
fused_ehr test  0   best mean auc :0.832 mean auprc 0.511
                    CI AUROC (0.800, 0.861) CI AUPRC (0.436, 0.590)
                     AUROC accute 0.832 mixed 0.832 chronic 0.832
                     AUROC accute CI (0.800, 0.861) mixed (0.800 , 0.861) chronic (0.800, 0.861)
                     AUPRC accute  0.511 mixed 0.511 chronic 0.511
                     AUPRC accute CI  (0.436, 0.590) mixed (0.436,  0.590) chronic (0.436, 0.590)