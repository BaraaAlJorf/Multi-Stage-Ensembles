Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ehr loaded
cxr loaded
==> training
running for fusion_type joint
0
starting val epoch 0
val [0000 / 0050] validation loss: 	0.77787
checkpoint
starting train epoch 0
 epoch [0000 / 0050] [0009/280] eta: 0 Days 14:33:44       lr: 	1.0000E-03 loss: 	1.10140
 epoch [0000 / 0050] [0109/280] eta: 0 Days 2:6:17         lr: 	1.0000E-03 loss: 	0.48912
 epoch [0000 / 0050] [0209/280] eta: 0 Days 1:30:27        lr: 	1.0000E-03 loss: 	0.45561
1
starting val epoch 1
val [0001 / 0050] validation loss: 	0.42435
checkpoint
starting train epoch 1
 epoch [0001 / 0050] [0009/280] eta: 0 Days 1:33:47        lr: 	1.0000E-03 loss: 	0.45890
 epoch [0001 / 0050] [0109/280] eta: 0 Days 1:22:28        lr: 	1.0000E-03 loss: 	0.42724
 epoch [0001 / 0050] [0209/280] eta: 0 Days 1:16:10        lr: 	1.0000E-03 loss: 	0.42184
2
starting val epoch 2
val [0002 / 0050] validation loss: 	0.42447
starting train epoch 2
 epoch [0002 / 0050] [0009/280] eta: 0 Days 1:17:46        lr: 	1.0000E-03 loss: 	0.49982
 epoch [0002 / 0050] [0109/280] eta: 0 Days 1:13:26        lr: 	1.0000E-03 loss: 	0.43870
 epoch [0002 / 0050] [0209/280] eta: 0 Days 1:9:39         lr: 	1.0000E-03 loss: 	0.41651
3
starting val epoch 3
val [0003 / 0050] validation loss: 	0.42482
checkpoint
starting train epoch 3
 epoch [0003 / 0050] [0009/280] eta: 0 Days 1:11:40        lr: 	1.0000E-03 loss: 	0.44708
 epoch [0003 / 0050] [0109/280] eta: 0 Days 1:8:43         lr: 	1.0000E-03 loss: 	0.42927
 epoch [0003 / 0050] [0209/280] eta: 0 Days 1:6:52         lr: 	1.0000E-03 loss: 	0.42546
4
starting val epoch 4
val [0004 / 0050] validation loss: 	0.42453
starting train epoch 4
 epoch [0004 / 0050] [0009/280] eta: 0 Days 1:7:49         lr: 	1.0000E-03 loss: 	0.47475
 epoch [0004 / 0050] [0109/280] eta: 0 Days 1:5:57         lr: 	1.0000E-03 loss: 	0.43272
 epoch [0004 / 0050] [0209/280] eta: 0 Days 1:3:56         lr: 	1.0000E-03 loss: 	0.42679
5
starting val epoch 5
val [0005 / 0050] validation loss: 	0.43197
checkpoint
starting train epoch 5
 epoch [0005 / 0050] [0009/280] eta: 0 Days 1:5:16         lr: 	1.0000E-03 loss: 	0.48390
 epoch [0005 / 0050] [0109/280] eta: 0 Days 1:3:47         lr: 	1.0000E-03 loss: 	0.43765
 epoch [0005 / 0050] [0209/280] eta: 0 Days 1:2:41         lr: 	1.0000E-03 loss: 	0.42313
6
starting val epoch 6
val [0006 / 0050] validation loss: 	0.42796
starting train epoch 6
 epoch [0006 / 0050] [0009/280] eta: 0 Days 1:3:27         lr: 	1.0000E-03 loss: 	0.52232
 epoch [0006 / 0050] [0109/280] eta: 0 Days 1:2:12         lr: 	1.0000E-03 loss: 	0.44466
 epoch [0006 / 0050] [0209/280] eta: 0 Days 1:1:0          lr: 	1.0000E-03 loss: 	0.42779
7
starting val epoch 7
val [0007 / 0050] validation loss: 	0.42866
checkpoint
starting train epoch 7
 epoch [0007 / 0050] [0009/280] eta: 0 Days 1:1:46         lr: 	1.0000E-03 loss: 	0.33631
 epoch [0007 / 0050] [0109/280] eta: 0 Days 1:0:26         lr: 	1.0000E-03 loss: 	0.40775
 epoch [0007 / 0050] [0209/280] eta: 0 Days 0:59:19        lr: 	1.0000E-03 loss: 	0.41077
8
starting val epoch 8
val [0008 / 0050] validation loss: 	0.43494
checkpoint
starting train epoch 8
 epoch [0008 / 0050] [0009/280] eta: 0 Days 0:59:46        lr: 	1.0000E-03 loss: 	0.47297
 epoch [0008 / 0050] [0109/280] eta: 0 Days 0:58:35        lr: 	1.0000E-03 loss: 	0.43586
 epoch [0008 / 0050] [0209/280] eta: 0 Days 0:57:28        lr: 	1.0000E-03 loss: 	0.42537
9
starting val epoch 9
val [0009 / 0050] validation loss: 	0.42452
starting train epoch 9
 epoch [0009 / 0050] [0009/280] eta: 0 Days 0:57:39        lr: 	1.0000E-03 loss: 	0.47021
 epoch [0009 / 0050] [0109/280] eta: 0 Days 0:56:42        lr: 	1.0000E-03 loss: 	0.42665
 epoch [0009 / 0050] [0209/280] eta: 0 Days 0:55:48        lr: 	1.0000E-03 loss: 	0.42071
10
starting val epoch 10
val [0010 / 0050] validation loss: 	0.42479
starting train epoch 10
 epoch [0010 / 0050] [0009/280] eta: 0 Days 0:56:11        lr: 	1.0000E-03 loss: 	0.41111
 epoch [0010 / 0050] [0109/280] eta: 0 Days 0:55:19        lr: 	1.0000E-03 loss: 	0.41224
 epoch [0010 / 0050] [0209/280] eta: 0 Days 0:54:26        lr: 	1.0000E-03 loss: 	0.41708
11
starting val epoch 11
val [0011 / 0050] validation loss: 	0.42607
starting train epoch 11
 epoch [0011 / 0050] [0009/280] eta: 0 Days 0:54:34        lr: 	1.0000E-03 loss: 	0.39151
 epoch [0011 / 0050] [0109/280] eta: 0 Days 0:53:45        lr: 	1.0000E-03 loss: 	0.41679
 epoch [0011 / 0050] [0209/280] eta: 0 Days 0:52:56        lr: 	1.0000E-03 loss: 	0.42234
12
starting val epoch 12
val [0012 / 0050] validation loss: 	0.42462
starting train epoch 12
 epoch [0012 / 0050] [0009/280] eta: 0 Days 0:52:58        lr: 	1.0000E-03 loss: 	0.42792
 epoch [0012 / 0050] [0109/280] eta: 0 Days 0:52:3         lr: 	1.0000E-03 loss: 	0.41695
 epoch [0012 / 0050] [0209/280] eta: 0 Days 0:51:15        lr: 	1.0000E-03 loss: 	0.42466
13
starting val epoch 13
val [0013 / 0050] validation loss: 	0.42443
checkpoint
starting train epoch 13
 epoch [0013 / 0050] [0009/280] eta: 0 Days 0:51:29        lr: 	1.0000E-03 loss: 	0.55305
 epoch [0013 / 0050] [0109/280] eta: 0 Days 0:50:42        lr: 	1.0000E-03 loss: 	0.45413
 epoch [0013 / 0050] [0209/280] eta: 0 Days 0:49:55        lr: 	1.0000E-03 loss: 	0.43165
14
starting val epoch 14
val [0014 / 0050] validation loss: 	0.42457
starting train epoch 14
 epoch [0014 / 0050] [0009/280] eta: 0 Days 0:49:53        lr: 	1.0000E-03 loss: 	0.54283
 epoch [0014 / 0050] [0109/280] eta: 0 Days 0:49:6         lr: 	1.0000E-03 loss: 	0.43476
 epoch [0014 / 0050] [0209/280] eta: 0 Days 0:48:25        lr: 	1.0000E-03 loss: 	0.42276
15
starting val epoch 15
val [0015 / 0050] validation loss: 	0.42434
starting train epoch 15
 epoch [0015 / 0050] [0009/280] eta: 0 Days 0:48:29        lr: 	1.0000E-03 loss: 	0.45865
 epoch [0015 / 0050] [0109/280] eta: 0 Days 0:47:46        lr: 	1.0000E-03 loss: 	0.42743
 epoch [0015 / 0050] [0209/280] eta: 0 Days 0:47:4         lr: 	1.0000E-03 loss: 	0.43291
16
starting val epoch 16
val [0016 / 0050] validation loss: 	0.42433
starting train epoch 16
 epoch [0016 / 0050] [0009/280] eta: 0 Days 0:47:1         lr: 	1.0000E-03 loss: 	0.45089
 epoch [0016 / 0050] [0109/280] eta: 0 Days 0:46:19        lr: 	1.0000E-03 loss: 	0.41556
 epoch [0016 / 0050] [0209/280] eta: 0 Days 0:45:42        lr: 	1.0000E-03 loss: 	0.42934
17
starting val epoch 17
val [0017 / 0050] validation loss: 	0.42445
starting train epoch 17
 epoch [0017 / 0050] [0009/280] eta: 0 Days 0:45:37        lr: 	1.0000E-03 loss: 	0.45725
 epoch [0017 / 0050] [0109/280] eta: 0 Days 0:45:0         lr: 	1.0000E-03 loss: 	0.43445
 epoch [0017 / 0050] [0209/280] eta: 0 Days 0:44:19        lr: 	1.0000E-03 loss: 	0.42417
18
starting val epoch 18
val [0018 / 0050] validation loss: 	0.42440
starting train epoch 18
 epoch [0018 / 0050] [0009/280] eta: 0 Days 0:44:14        lr: 	1.0000E-03 loss: 	0.43324
 epoch [0018 / 0050] [0109/280] eta: 0 Days 0:43:36        lr: 	1.0000E-03 loss: 	0.44482
 epoch [0018 / 0050] [0209/280] eta: 0 Days 0:42:57        lr: 	1.0000E-03 loss: 	0.42424
19
starting val epoch 19
val [0019 / 0050] validation loss: 	0.42593
starting train epoch 19
 epoch [0019 / 0050] [0009/280] eta: 0 Days 0:42:47        lr: 	1.0000E-03 loss: 	0.39169
 epoch [0019 / 0050] [0109/280] eta: 0 Days 0:42:11        lr: 	1.0000E-03 loss: 	0.42452
 epoch [0019 / 0050] [0209/280] eta: 0 Days 0:41:36        lr: 	1.0000E-03 loss: 	0.42754
20
starting val epoch 20
val [0020 / 0050] validation loss: 	0.42489
starting train epoch 20
 epoch [0020 / 0050] [0009/280] eta: 0 Days 0:41:29        lr: 	1.0000E-03 loss: 	0.46614
 epoch [0020 / 0050] [0109/280] eta: 0 Days 0:40:50        lr: 	1.0000E-03 loss: 	0.41986
 epoch [0020 / 0050] [0209/280] eta: 0 Days 0:40:11        lr: 	1.0000E-03 loss: 	0.41937
21
starting val epoch 21
val [0021 / 0050] validation loss: 	0.42363
starting train epoch 21
 epoch [0021 / 0050] [0009/280] eta: 0 Days 0:40:2         lr: 	1.0000E-03 loss: 	0.51713
 epoch [0021 / 0050] [0109/280] eta: 0 Days 0:39:25        lr: 	1.0000E-03 loss: 	0.42787
 epoch [0021 / 0050] [0209/280] eta: 0 Days 0:38:50        lr: 	1.0000E-03 loss: 	0.42083
22
starting val epoch 22
val [0022 / 0050] validation loss: 	0.41954
starting train epoch 22
 epoch [0022 / 0050] [0009/280] eta: 0 Days 0:38:37        lr: 	1.0000E-03 loss: 	0.37335
 epoch [0022 / 0050] [0109/280] eta: 0 Days 0:38:1         lr: 	1.0000E-03 loss: 	0.40030
 epoch [0022 / 0050] [0209/280] eta: 0 Days 0:37:25        lr: 	1.0000E-03 loss: 	0.42832
23
starting val epoch 23
val [0023 / 0050] validation loss: 	0.42591
checkpoint
starting train epoch 23
 epoch [0023 / 0050] [0009/280] eta: 0 Days 0:37:15        lr: 	1.0000E-03 loss: 	0.48162
 epoch [0023 / 0050] [0109/280] eta: 0 Days 0:36:40        lr: 	1.0000E-03 loss: 	0.42993
 epoch [0023 / 0050] [0209/280] eta: 0 Days 0:36:5         lr: 	1.0000E-03 loss: 	0.42120
24
starting val epoch 24
val [0024 / 0050] validation loss: 	0.41926
starting train epoch 24
 epoch [0024 / 0050] [0009/280] eta: 0 Days 0:35:50        lr: 	1.0000E-03 loss: 	0.52266
 epoch [0024 / 0050] [0109/280] eta: 0 Days 0:35:15        lr: 	1.0000E-03 loss: 	0.39270
 epoch [0024 / 0050] [0209/280] eta: 0 Days 0:34:40        lr: 	1.0000E-03 loss: 	0.41217
25
starting val epoch 25
val [0025 / 0050] validation loss: 	0.42502
starting train epoch 25
 epoch [0025 / 0050] [0009/280] eta: 0 Days 0:34:25        lr: 	1.0000E-03 loss: 	0.44159
 epoch [0025 / 0050] [0109/280] eta: 0 Days 0:33:49        lr: 	1.0000E-03 loss: 	0.40002
 epoch [0025 / 0050] [0209/280] eta: 0 Days 0:33:15        lr: 	1.0000E-03 loss: 	0.41297
26
starting val epoch 26
val [0026 / 0050] validation loss: 	0.42393
starting train epoch 26
 epoch [0026 / 0050] [0009/280] eta: 0 Days 0:33:1         lr: 	1.0000E-03 loss: 	0.44910
 epoch [0026 / 0050] [0109/280] eta: 0 Days 0:32:27        lr: 	1.0000E-03 loss: 	0.39953
 epoch [0026 / 0050] [0209/280] eta: 0 Days 0:31:52        lr: 	1.0000E-03 loss: 	0.42200
27
starting val epoch 27
val [0027 / 0050] validation loss: 	0.41528
starting train epoch 27
 epoch [0027 / 0050] [0009/280] eta: 0 Days 0:31:38        lr: 	1.0000E-03 loss: 	0.43579
 epoch [0027 / 0050] [0109/280] eta: 0 Days 0:31:4         lr: 	1.0000E-03 loss: 	0.40624
 epoch [0027 / 0050] [0209/280] eta: 0 Days 0:30:29        lr: 	1.0000E-03 loss: 	0.41098
28
starting val epoch 28
val [0028 / 0050] validation loss: 	0.42148
starting train epoch 28
 epoch [0028 / 0050] [0009/280] eta: 0 Days 0:30:15        lr: 	1.0000E-03 loss: 	0.41467
 epoch [0028 / 0050] [0109/280] eta: 0 Days 0:29:43        lr: 	1.0000E-03 loss: 	0.41054
 epoch [0028 / 0050] [0209/280] eta: 0 Days 0:29:8         lr: 	1.0000E-03 loss: 	0.41734
29
starting val epoch 29
val [0029 / 0050] validation loss: 	0.42390
starting train epoch 29
 epoch [0029 / 0050] [0009/280] eta: 0 Days 0:28:51        lr: 	1.0000E-03 loss: 	0.39906
 epoch [0029 / 0050] [0109/280] eta: 0 Days 0:28:19        lr: 	1.0000E-03 loss: 	0.40836
 epoch [0029 / 0050] [0209/280] eta: 0 Days 0:27:46        lr: 	1.0000E-03 loss: 	0.42273
30
starting val epoch 30
val [0030 / 0050] validation loss: 	0.42362
starting train epoch 30
 epoch [0030 / 0050] [0009/280] eta: 0 Days 0:27:28        lr: 	1.0000E-03 loss: 	0.43312
 epoch [0030 / 0050] [0109/280] eta: 0 Days 0:26:55        lr: 	1.0000E-03 loss: 	0.42661
 epoch [0030 / 0050] [0209/280] eta: 0 Days 0:26:24        lr: 	1.0000E-03 loss: 	0.41403
31
starting val epoch 31
val [0031 / 0050] validation loss: 	0.42393
starting train epoch 31
 epoch [0031 / 0050] [0009/280] eta: 0 Days 0:26:7         lr: 	1.0000E-03 loss: 	0.54096
 epoch [0031 / 0050] [0109/280] eta: 0 Days 0:25:34        lr: 	1.0000E-03 loss: 	0.42523
 epoch [0031 / 0050] [0209/280] eta: 0 Days 0:25:0         lr: 	1.0000E-03 loss: 	0.41290
32
starting val epoch 32
val [0032 / 0050] validation loss: 	0.41900
starting train epoch 32
 epoch [0032 / 0050] [0009/280] eta: 0 Days 0:24:41        lr: 	1.0000E-03 loss: 	0.49348
 epoch [0032 / 0050] [0109/280] eta: 0 Days 0:24:9         lr: 	1.0000E-03 loss: 	0.43532
 epoch [0032 / 0050] [0209/280] eta: 0 Days 0:23:37        lr: 	1.0000E-03 loss: 	0.42437
33
starting val epoch 33
val [0033 / 0050] validation loss: 	0.43297
starting train epoch 33
 epoch [0033 / 0050] [0009/280] eta: 0 Days 0:23:18        lr: 	1.0000E-03 loss: 	0.38383
 epoch [0033 / 0050] [0109/280] eta: 0 Days 0:22:45        lr: 	1.0000E-03 loss: 	0.41184
 epoch [0033 / 0050] [0209/280] eta: 0 Days 0:22:13        lr: 	1.0000E-03 loss: 	0.39926
34
starting val epoch 34
val [0034 / 0050] validation loss: 	0.41634
starting train epoch 34
 epoch [0034 / 0050] [0009/280] eta: 0 Days 0:21:53        lr: 	1.0000E-03 loss: 	0.45582
 epoch [0034 / 0050] [0109/280] eta: 0 Days 0:21:22        lr: 	1.0000E-03 loss: 	0.38400
 epoch [0034 / 0050] [0209/280] eta: 0 Days 0:20:50        lr: 	1.0000E-03 loss: 	0.40181
35
starting val epoch 35
val [0035 / 0050] validation loss: 	0.41236
starting train epoch 35
 epoch [0035 / 0050] [0009/280] eta: 0 Days 0:20:33        lr: 	1.0000E-03 loss: 	0.48602
 epoch [0035 / 0050] [0109/280] eta: 0 Days 0:20:1         lr: 	1.0000E-03 loss: 	0.41974
 epoch [0035 / 0050] [0209/280] eta: 0 Days 0:19:30        lr: 	1.0000E-03 loss: 	0.41195
36
starting val epoch 36
val [0036 / 0050] validation loss: 	0.42405
starting train epoch 36
 epoch [0036 / 0050] [0009/280] eta: 0 Days 0:19:10        lr: 	1.0000E-03 loss: 	0.51402
 epoch [0036 / 0050] [0109/280] eta: 0 Days 0:18:39        lr: 	1.0000E-03 loss: 	0.40711
 epoch [0036 / 0050] [0209/280] eta: 0 Days 0:18:8         lr: 	1.0000E-03 loss: 	0.41549
37
starting val epoch 37
val [0037 / 0050] validation loss: 	0.41895
starting train epoch 37
 epoch [0037 / 0050] [0009/280] eta: 0 Days 0:17:49        lr: 	1.0000E-03 loss: 	0.43905
 epoch [0037 / 0050] [0109/280] eta: 0 Days 0:17:18        lr: 	1.0000E-03 loss: 	0.39730
 epoch [0037 / 0050] [0209/280] eta: 0 Days 0:16:47        lr: 	1.0000E-03 loss: 	0.40761
38
starting val epoch 38
val [0038 / 0050] validation loss: 	0.42515
starting val epoch 0
val [0000 / 0050] validation loss: 	0.43005
Acute and unspecified renal failure                                                        & 0.626(0.670, 0.582) & 0.219 (0.272, 0.181)
fused_ehr test  0   best mean auc :0.626 mean auprc 0.219
                    CI AUROC (0.582, 0.670) CI AUPRC (0.181, 0.272)
                     AUROC accute 0.626 mixed 0.626 chronic 0.626
                     AUROC accute CI (0.582, 0.670) mixed (0.582 , 0.670) chronic (0.582, 0.670)
                     AUPRC accute  0.219 mixed 0.219 chronic 0.219
                     AUPRC accute CI  (0.181, 0.272) mixed (0.181,  0.272) chronic (0.181, 0.272)