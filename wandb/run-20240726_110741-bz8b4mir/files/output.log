Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ehr loaded
cxr loaded
rr loaded
==> training
running for fusion_type joint
0
starting val epoch 0
val [0000 / 0050] validation loss: 	0.76769
checkpoint
starting train epoch 0
 epoch [0000 / 0050] [0009/267] eta: 0 Days 17:41:9        lr: 	1.0000E-03 loss: 	0.88228
 epoch [0000 / 0050] [0109/267] eta: 0 Days 3:14:42        lr: 	1.0000E-03 loss: 	0.47607
 epoch [0000 / 0050] [0209/267] eta: 0 Days 2:33:6         lr: 	1.0000E-03 loss: 	0.44879
1
starting val epoch 1
val [0001 / 0050] validation loss: 	0.44135
checkpoint
starting train epoch 1
 epoch [0001 / 0050] [0009/267] eta: 0 Days 2:40:4         lr: 	1.0000E-03 loss: 	0.38063
 epoch [0001 / 0050] [0109/267] eta: 0 Days 2:25:58        lr: 	1.0000E-03 loss: 	0.40686
 epoch [0001 / 0050] [0209/267] eta: 0 Days 2:16:55        lr: 	1.0000E-03 loss: 	0.41853
2
starting val epoch 2
val [0002 / 0050] validation loss: 	0.45184
checkpoint
starting train epoch 2
 epoch [0002 / 0050] [0009/267] eta: 0 Days 2:21:51        lr: 	1.0000E-03 loss: 	0.46762
 epoch [0002 / 0050] [0109/267] eta: 0 Days 2:15:28        lr: 	1.0000E-03 loss: 	0.41866
 epoch [0002 / 0050] [0209/267] eta: 0 Days 2:10:30        lr: 	1.0000E-03 loss: 	0.42628
3
starting val epoch 3
val [0003 / 0050] validation loss: 	0.44096
starting train epoch 3
 epoch [0003 / 0050] [0009/267] eta: 0 Days 2:12:15        lr: 	1.0000E-03 loss: 	0.49543
 epoch [0003 / 0050] [0109/267] eta: 0 Days 2:8:22         lr: 	1.0000E-03 loss: 	0.43919
 epoch [0003 / 0050] [0209/267] eta: 0 Days 2:5:7          lr: 	1.0000E-03 loss: 	0.42262
4
starting val epoch 4
val [0004 / 0050] validation loss: 	0.44309
starting train epoch 4
 epoch [0004 / 0050] [0009/267] eta: 0 Days 2:6:20         lr: 	1.0000E-03 loss: 	0.43795
 epoch [0004 / 0050] [0109/267] eta: 0 Days 2:3:15         lr: 	1.0000E-03 loss: 	0.38903
 epoch [0004 / 0050] [0209/267] eta: 0 Days 2:0:31         lr: 	1.0000E-03 loss: 	0.40883
5
starting val epoch 5
val [0005 / 0050] validation loss: 	0.43639
starting train epoch 5
 epoch [0005 / 0050] [0009/267] eta: 0 Days 2:1:31         lr: 	1.0000E-03 loss: 	0.44653
 epoch [0005 / 0050] [0109/267] eta: 0 Days 1:59:6         lr: 	1.0000E-03 loss: 	0.42245
 epoch [0005 / 0050] [0209/267] eta: 0 Days 1:56:52        lr: 	1.0000E-03 loss: 	0.42551
6
starting val epoch 6
val [0006 / 0050] validation loss: 	0.43606
starting train epoch 6
 epoch [0006 / 0050] [0009/267] eta: 0 Days 1:57:34        lr: 	1.0000E-03 loss: 	0.44934
 epoch [0006 / 0050] [0109/267] eta: 0 Days 1:55:19        lr: 	1.0000E-03 loss: 	0.43261
 epoch [0006 / 0050] [0209/267] eta: 0 Days 1:53:18        lr: 	1.0000E-03 loss: 	0.42537
7
starting val epoch 7
val [0007 / 0050] validation loss: 	0.43573
starting train epoch 7
 epoch [0007 / 0050] [0009/267] eta: 0 Days 1:53:48        lr: 	1.0000E-03 loss: 	0.45976
 epoch [0007 / 0050] [0109/267] eta: 0 Days 1:52:0         lr: 	1.0000E-03 loss: 	0.41359
 epoch [0007 / 0050] [0209/267] eta: 0 Days 1:50:6         lr: 	1.0000E-03 loss: 	0.41490
8
starting val epoch 8
val [0008 / 0050] validation loss: 	0.43626
starting train epoch 8
 epoch [0008 / 0050] [0009/267] eta: 0 Days 1:50:30        lr: 	1.0000E-03 loss: 	0.56997
 epoch [0008 / 0050] [0109/267] eta: 0 Days 1:48:46        lr: 	1.0000E-03 loss: 	0.42564
 epoch [0008 / 0050] [0209/267] eta: 0 Days 1:47:8         lr: 	1.0000E-03 loss: 	0.41653
9
starting val epoch 9
val [0009 / 0050] validation loss: 	0.43839
starting train epoch 9
 epoch [0009 / 0050] [0009/267] eta: 0 Days 1:47:20        lr: 	1.0000E-03 loss: 	0.40662
 epoch [0009 / 0050] [0109/267] eta: 0 Days 1:45:43        lr: 	1.0000E-03 loss: 	0.43331
 epoch [0009 / 0050] [0209/267] eta: 0 Days 1:44:12        lr: 	1.0000E-03 loss: 	0.42481
10
starting val epoch 10
val [0010 / 0050] validation loss: 	0.43820
starting train epoch 10
 epoch [0010 / 0050] [0009/267] eta: 0 Days 1:44:26        lr: 	1.0000E-03 loss: 	0.48853
 epoch [0010 / 0050] [0109/267] eta: 0 Days 1:42:56        lr: 	1.0000E-03 loss: 	0.44398
 epoch [0010 / 0050] [0209/267] eta: 0 Days 1:41:28        lr: 	1.0000E-03 loss: 	0.43383
11
starting val epoch 11
val [0011 / 0050] validation loss: 	0.43705
starting train epoch 11
 epoch [0011 / 0050] [0009/267] eta: 0 Days 1:41:33        lr: 	1.0000E-03 loss: 	0.48168
 epoch [0011 / 0050] [0109/267] eta: 0 Days 1:40:5         lr: 	1.0000E-03 loss: 	0.42232
 epoch [0011 / 0050] [0209/267] eta: 0 Days 1:38:39        lr: 	1.0000E-03 loss: 	0.42425
12
starting val epoch 12
val [0012 / 0050] validation loss: 	0.43617
starting train epoch 12
 epoch [0012 / 0050] [0009/267] eta: 0 Days 1:38:40        lr: 	1.0000E-03 loss: 	0.51556
 epoch [0012 / 0050] [0109/267] eta: 0 Days 1:37:19        lr: 	1.0000E-03 loss: 	0.44103
 epoch [0012 / 0050] [0209/267] eta: 0 Days 1:35:55        lr: 	1.0000E-03 loss: 	0.42880
13
starting val epoch 13
val [0013 / 0050] validation loss: 	0.44249
starting train epoch 13
 epoch [0013 / 0050] [0009/267] eta: 0 Days 1:35:51        lr: 	1.0000E-03 loss: 	0.49949
 epoch [0013 / 0050] [0109/267] eta: 0 Days 1:34:30        lr: 	1.0000E-03 loss: 	0.42496
 epoch [0013 / 0050] [0209/267] eta: 0 Days 1:33:11        lr: 	1.0000E-03 loss: 	0.42757
14
starting val epoch 14
val [0014 / 0050] validation loss: 	0.43583
starting train epoch 14
 epoch [0014 / 0050] [0009/267] eta: 0 Days 1:33:3         lr: 	1.0000E-03 loss: 	0.44687
 epoch [0014 / 0050] [0109/267] eta: 0 Days 1:31:44        lr: 	1.0000E-03 loss: 	0.44316
 epoch [0014 / 0050] [0209/267] eta: 0 Days 1:30:27        lr: 	1.0000E-03 loss: 	0.42757
15
starting val epoch 15
val [0015 / 0050] validation loss: 	0.44668
starting train epoch 15
 epoch [0015 / 0050] [0009/267] eta: 0 Days 1:30:15        lr: 	1.0000E-03 loss: 	0.44593
 epoch [0015 / 0050] [0109/267] eta: 0 Days 1:28:58        lr: 	1.0000E-03 loss: 	0.42298
 epoch [0015 / 0050] [0209/267] eta: 0 Days 1:27:42        lr: 	1.0000E-03 loss: 	0.41558
16
starting val epoch 16
val [0016 / 0050] validation loss: 	0.43548
starting train epoch 16
 epoch [0016 / 0050] [0009/267] eta: 0 Days 1:27:29        lr: 	1.0000E-03 loss: 	0.41516
 epoch [0016 / 0050] [0109/267] eta: 0 Days 1:26:13        lr: 	1.0000E-03 loss: 	0.42082
 epoch [0016 / 0050] [0209/267] eta: 0 Days 1:25:0         lr: 	1.0000E-03 loss: 	0.42684
17
starting val epoch 17
val [0017 / 0050] validation loss: 	0.43754
starting val epoch 0
val [0000 / 0050] validation loss: 	0.44438
Acute and unspecified renal failure                                                        & 0.674(0.722, 0.622) & 0.282 (0.352, 0.228)
fused_ehr test  0   best mean auc :0.674 mean auprc 0.282
                    CI AUROC (0.622, 0.722) CI AUPRC (0.228, 0.352)
                     AUROC accute 0.674 mixed 0.674 chronic 0.674
                     AUROC accute CI (0.622, 0.722) mixed (0.622 , 0.722) chronic (0.622, 0.722)
                     AUPRC accute  0.282 mixed 0.282 chronic 0.282
                     AUPRC accute CI  (0.228, 0.352) mixed (0.228,  0.352) chronic (0.228, 0.352)