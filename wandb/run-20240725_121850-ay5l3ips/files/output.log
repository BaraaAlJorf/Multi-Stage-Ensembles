Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ehr loaded
cxr loaded
==> training
running for fusion_type early
0
starting val epoch 0
val [0000 / 0050] validation loss: 	0.77787
checkpoint
starting train epoch 0
 epoch [0000 / 0050] [0009/280] eta: 0 Days 13:27:14       lr: 	1.0000E-05 loss: 	0.70697
 epoch [0000 / 0050] [0109/280] eta: 0 Days 1:30:8         lr: 	1.0000E-05 loss: 	0.42248
 epoch [0000 / 0050] [0209/280] eta: 0 Days 0:55:24        lr: 	1.0000E-05 loss: 	0.40018
1
starting val epoch 1
val [0001 / 0050] validation loss: 	0.38946
checkpoint
starting train epoch 1
 epoch [0001 / 0050] [0009/280] eta: 0 Days 0:56:23        lr: 	1.0000E-05 loss: 	0.38855
 epoch [0001 / 0050] [0109/280] eta: 0 Days 0:46:18        lr: 	1.0000E-05 loss: 	0.36686
 epoch [0001 / 0050] [0209/280] eta: 0 Days 0:40:1         lr: 	1.0000E-05 loss: 	0.36483
2
starting val epoch 2
val [0002 / 0050] validation loss: 	0.39018
checkpoint
starting train epoch 2
 epoch [0002 / 0050] [0009/280] eta: 0 Days 0:42:24        lr: 	1.0000E-05 loss: 	0.42325
 epoch [0002 / 0050] [0109/280] eta: 0 Days 0:38:29        lr: 	1.0000E-05 loss: 	0.37360
 epoch [0002 / 0050] [0209/280] eta: 0 Days 0:35:25        lr: 	1.0000E-05 loss: 	0.35445
3
starting val epoch 3
val [0003 / 0050] validation loss: 	0.39098
checkpoint
starting train epoch 3
 epoch [0003 / 0050] [0009/280] eta: 0 Days 0:37:19        lr: 	1.0000E-05 loss: 	0.42669
 epoch [0003 / 0050] [0109/280] eta: 0 Days 0:35:0         lr: 	1.0000E-05 loss: 	0.37020
 epoch [0003 / 0050] [0209/280] eta: 0 Days 0:33:5         lr: 	1.0000E-05 loss: 	0.36428
4
starting val epoch 4
val [0004 / 0050] validation loss: 	0.39109
checkpoint
starting train epoch 4
 epoch [0004 / 0050] [0009/280] eta: 0 Days 0:34:37        lr: 	1.0000E-05 loss: 	0.40925
 epoch [0004 / 0050] [0109/280] eta: 0 Days 0:32:58        lr: 	1.0000E-05 loss: 	0.35310
 epoch [0004 / 0050] [0209/280] eta: 0 Days 0:31:31        lr: 	1.0000E-05 loss: 	0.34562
5
starting val epoch 5
val [0005 / 0050] validation loss: 	0.39084
checkpoint
starting train epoch 5
 epoch [0005 / 0050] [0009/280] eta: 0 Days 0:32:40        lr: 	1.0000E-05 loss: 	0.40593
 epoch [0005 / 0050] [0109/280] eta: 0 Days 0:31:22        lr: 	1.0000E-05 loss: 	0.35394
 epoch [0005 / 0050] [0209/280] eta: 0 Days 0:30:10        lr: 	1.0000E-05 loss: 	0.33556
6
starting val epoch 6
val [0006 / 0050] validation loss: 	0.39295
checkpoint
starting train epoch 6
 epoch [0006 / 0050] [0009/280] eta: 0 Days 0:31:8         lr: 	1.0000E-05 loss: 	0.43110
 epoch [0006 / 0050] [0109/280] eta: 0 Days 0:30:9         lr: 	1.0000E-05 loss: 	0.35526
 epoch [0006 / 0050] [0209/280] eta: 0 Days 0:29:12        lr: 	1.0000E-05 loss: 	0.34316
7
starting val epoch 7
val [0007 / 0050] validation loss: 	0.39031
starting train epoch 7
 epoch [0007 / 0050] [0009/280] eta: 0 Days 0:29:51        lr: 	1.0000E-05 loss: 	0.25701
 epoch [0007 / 0050] [0109/280] eta: 0 Days 0:29:4         lr: 	1.0000E-05 loss: 	0.30784
 epoch [0007 / 0050] [0209/280] eta: 0 Days 0:28:13        lr: 	1.0000E-05 loss: 	0.32017
8
starting val epoch 8
val [0008 / 0050] validation loss: 	0.38487
starting train epoch 8
 epoch [0008 / 0050] [0009/280] eta: 0 Days 0:28:43        lr: 	1.0000E-05 loss: 	0.36183
 epoch [0008 / 0050] [0109/280] eta: 0 Days 0:27:56        lr: 	1.0000E-05 loss: 	0.33193
 epoch [0008 / 0050] [0209/280] eta: 0 Days 0:27:12        lr: 	1.0000E-05 loss: 	0.33086
9
starting val epoch 9
val [0009 / 0050] validation loss: 	0.38449
starting train epoch 9
 epoch [0009 / 0050] [0009/280] eta: 0 Days 0:27:36        lr: 	1.0000E-05 loss: 	0.34458
 epoch [0009 / 0050] [0109/280] eta: 0 Days 0:26:57        lr: 	1.0000E-05 loss: 	0.33022
 epoch [0009 / 0050] [0209/280] eta: 0 Days 0:26:20        lr: 	1.0000E-05 loss: 	0.32772
10
starting val epoch 10
val [0010 / 0050] validation loss: 	0.38440
starting train epoch 10
 epoch [0010 / 0050] [0009/280] eta: 0 Days 0:26:40        lr: 	1.0000E-05 loss: 	0.28971
 epoch [0010 / 0050] [0109/280] eta: 0 Days 0:26:2         lr: 	1.0000E-05 loss: 	0.31449
 epoch [0010 / 0050] [0209/280] eta: 0 Days 0:25:26        lr: 	1.0000E-05 loss: 	0.32166
11
starting val epoch 11
val [0011 / 0050] validation loss: 	0.38563
starting train epoch 11
 epoch [0011 / 0050] [0009/280] eta: 0 Days 0:25:42        lr: 	1.0000E-05 loss: 	0.30604
 epoch [0011 / 0050] [0109/280] eta: 0 Days 0:25:7         lr: 	1.0000E-05 loss: 	0.31871
 epoch [0011 / 0050] [0209/280] eta: 0 Days 0:24:33        lr: 	1.0000E-05 loss: 	0.32517
12
starting val epoch 12
val [0012 / 0050] validation loss: 	0.38715
starting train epoch 12
 epoch [0012 / 0050] [0009/280] eta: 0 Days 0:24:47        lr: 	1.0000E-05 loss: 	0.32180
 epoch [0012 / 0050] [0109/280] eta: 0 Days 0:24:18        lr: 	1.0000E-05 loss: 	0.30579
 epoch [0012 / 0050] [0209/280] eta: 0 Days 0:23:48        lr: 	1.0000E-05 loss: 	0.32607
13
starting val epoch 13
val [0013 / 0050] validation loss: 	0.38397
starting train epoch 13
 epoch [0013 / 0050] [0009/280] eta: 0 Days 0:24:2         lr: 	1.0000E-05 loss: 	0.42824
 epoch [0013 / 0050] [0109/280] eta: 0 Days 0:23:33        lr: 	1.0000E-05 loss: 	0.35679
 epoch [0013 / 0050] [0209/280] eta: 0 Days 0:23:4         lr: 	1.0000E-05 loss: 	0.32814
14
starting val epoch 14
val [0014 / 0050] validation loss: 	0.38357
starting train epoch 14
 epoch [0014 / 0050] [0009/280] eta: 0 Days 0:23:15        lr: 	1.0000E-05 loss: 	0.38907
 epoch [0014 / 0050] [0109/280] eta: 0 Days 0:22:48        lr: 	1.0000E-05 loss: 	0.32851
 epoch [0014 / 0050] [0209/280] eta: 0 Days 0:22:19        lr: 	1.0000E-05 loss: 	0.32111
15
starting val epoch 15
val [0015 / 0050] validation loss: 	0.38830
checkpoint
starting train epoch 15
 epoch [0015 / 0050] [0009/280] eta: 0 Days 0:22:33        lr: 	1.0000E-05 loss: 	0.31833
 epoch [0015 / 0050] [0109/280] eta: 0 Days 0:22:5         lr: 	1.0000E-05 loss: 	0.32733
 epoch [0015 / 0050] [0209/280] eta: 0 Days 0:21:38        lr: 	1.0000E-05 loss: 	0.32734
16
starting val epoch 16
val [0016 / 0050] validation loss: 	0.38971
checkpoint
starting train epoch 16
 epoch [0016 / 0050] [0009/280] eta: 0 Days 0:21:49        lr: 	1.0000E-05 loss: 	0.35055
 epoch [0016 / 0050] [0109/280] eta: 0 Days 0:21:23        lr: 	1.0000E-05 loss: 	0.30939
 epoch [0016 / 0050] [0209/280] eta: 0 Days 0:20:58        lr: 	1.0000E-05 loss: 	0.32778
17
starting val epoch 17
val [0017 / 0050] validation loss: 	0.38541
starting train epoch 17
 epoch [0017 / 0050] [0009/280] eta: 0 Days 0:21:3         lr: 	1.0000E-05 loss: 	0.33515
 epoch [0017 / 0050] [0109/280] eta: 0 Days 0:20:39        lr: 	1.0000E-05 loss: 	0.33330
 epoch [0017 / 0050] [0209/280] eta: 0 Days 0:20:16        lr: 	1.0000E-05 loss: 	0.32498
18
starting val epoch 18
val [0018 / 0050] validation loss: 	0.38707
starting train epoch 18
 epoch [0018 / 0050] [0009/280] eta: 0 Days 0:20:20        lr: 	1.0000E-05 loss: 	0.39238
 epoch [0018 / 0050] [0109/280] eta: 0 Days 0:19:58        lr: 	1.0000E-05 loss: 	0.33077
 epoch [0018 / 0050] [0209/280] eta: 0 Days 0:19:35        lr: 	1.0000E-05 loss: 	0.31582
19
starting val epoch 19
val [0019 / 0050] validation loss: 	0.38917
starting train epoch 19
 epoch [0019 / 0050] [0009/280] eta: 0 Days 0:19:39        lr: 	1.0000E-05 loss: 	0.32763
 epoch [0019 / 0050] [0109/280] eta: 0 Days 0:19:18        lr: 	1.0000E-05 loss: 	0.30452
 epoch [0019 / 0050] [0209/280] eta: 0 Days 0:18:56        lr: 	1.0000E-05 loss: 	0.32034
20
starting val epoch 20
val [0020 / 0050] validation loss: 	0.38142
starting train epoch 20
 epoch [0020 / 0050] [0009/280] eta: 0 Days 0:18:59        lr: 	1.0000E-05 loss: 	0.35446
 epoch [0020 / 0050] [0109/280] eta: 0 Days 0:18:37        lr: 	1.0000E-05 loss: 	0.31602
 epoch [0020 / 0050] [0209/280] eta: 0 Days 0:18:16        lr: 	1.0000E-05 loss: 	0.32090
21
starting val epoch 21
val [0021 / 0050] validation loss: 	0.38773
checkpoint
starting train epoch 21
 epoch [0021 / 0050] [0009/280] eta: 0 Days 0:18:20        lr: 	1.0000E-05 loss: 	0.34560
 epoch [0021 / 0050] [0109/280] eta: 0 Days 0:18:0         lr: 	1.0000E-05 loss: 	0.31318
 epoch [0021 / 0050] [0209/280] eta: 0 Days 0:17:39        lr: 	1.0000E-05 loss: 	0.31191
22
starting val epoch 22
val [0022 / 0050] validation loss: 	0.38694
starting train epoch 22
 epoch [0022 / 0050] [0009/280] eta: 0 Days 0:17:39        lr: 	1.0000E-05 loss: 	0.30817
 epoch [0022 / 0050] [0109/280] eta: 0 Days 0:17:19        lr: 	1.0000E-05 loss: 	0.31496
 epoch [0022 / 0050] [0209/280] eta: 0 Days 0:16:59        lr: 	1.0000E-05 loss: 	0.33341
23
starting val epoch 23
val [0023 / 0050] validation loss: 	0.40993
checkpoint
starting train epoch 23
 epoch [0023 / 0050] [0009/280] eta: 0 Days 0:17:4         lr: 	1.0000E-05 loss: 	0.35269
 epoch [0023 / 0050] [0109/280] eta: 0 Days 0:16:44        lr: 	1.0000E-05 loss: 	0.31917
 epoch [0023 / 0050] [0209/280] eta: 0 Days 0:16:25        lr: 	1.0000E-05 loss: 	0.32064
24
starting val epoch 24
val [0024 / 0050] validation loss: 	0.39270
checkpoint
starting train epoch 24
 epoch [0024 / 0050] [0009/280] eta: 0 Days 0:16:26        lr: 	1.0000E-05 loss: 	0.37657
 epoch [0024 / 0050] [0109/280] eta: 0 Days 0:16:7         lr: 	1.0000E-05 loss: 	0.31094
 epoch [0024 / 0050] [0209/280] eta: 0 Days 0:15:48        lr: 	1.0000E-05 loss: 	0.31881
25
starting val epoch 25
val [0025 / 0050] validation loss: 	0.39508
checkpoint
starting train epoch 25
 epoch [0025 / 0050] [0009/280] eta: 0 Days 0:15:48        lr: 	1.0000E-05 loss: 	0.30118
 epoch [0025 / 0050] [0109/280] eta: 0 Days 0:15:30        lr: 	1.0000E-05 loss: 	0.29893
 epoch [0025 / 0050] [0209/280] eta: 0 Days 0:15:11        lr: 	1.0000E-05 loss: 	0.31749
26
starting val epoch 26
val [0026 / 0050] validation loss: 	0.38599
starting train epoch 26
 epoch [0026 / 0050] [0009/280] eta: 0 Days 0:15:8         lr: 	1.0000E-05 loss: 	0.33424
 epoch [0026 / 0050] [0109/280] eta: 0 Days 0:14:50        lr: 	1.0000E-05 loss: 	0.30095
 epoch [0026 / 0050] [0209/280] eta: 0 Days 0:14:32        lr: 	1.0000E-05 loss: 	0.31967
27
starting val epoch 27
val [0027 / 0050] validation loss: 	0.38790
starting train epoch 27
 epoch [0027 / 0050] [0009/280] eta: 0 Days 0:14:28        lr: 	1.0000E-05 loss: 	0.30081
 epoch [0027 / 0050] [0109/280] eta: 0 Days 0:14:11        lr: 	1.0000E-05 loss: 	0.30949
 epoch [0027 / 0050] [0209/280] eta: 0 Days 0:13:53        lr: 	1.0000E-05 loss: 	0.31758
28
starting val epoch 28
val [0028 / 0050] validation loss: 	0.38659
starting train epoch 28
 epoch [0028 / 0050] [0009/280] eta: 0 Days 0:13:49        lr: 	1.0000E-05 loss: 	0.38243
 epoch [0028 / 0050] [0109/280] eta: 0 Days 0:13:32        lr: 	1.0000E-05 loss: 	0.31741
 epoch [0028 / 0050] [0209/280] eta: 0 Days 0:13:14        lr: 	1.0000E-05 loss: 	0.31158
29
starting val epoch 29
val [0029 / 0050] validation loss: 	0.38993
starting train epoch 29
 epoch [0029 / 0050] [0009/280] eta: 0 Days 0:13:9         lr: 	1.0000E-05 loss: 	0.36103
 epoch [0029 / 0050] [0109/280] eta: 0 Days 0:12:52        lr: 	1.0000E-05 loss: 	0.28612
 epoch [0029 / 0050] [0209/280] eta: 0 Days 0:12:35        lr: 	1.0000E-05 loss: 	0.30242
30
starting val epoch 30
val [0030 / 0050] validation loss: 	0.38919
starting train epoch 30
 epoch [0030 / 0050] [0009/280] eta: 0 Days 0:12:30        lr: 	1.0000E-05 loss: 	0.32301
 epoch [0030 / 0050] [0109/280] eta: 0 Days 0:12:13        lr: 	1.0000E-05 loss: 	0.32746
 epoch [0030 / 0050] [0209/280] eta: 0 Days 0:11:56        lr: 	1.0000E-05 loss: 	0.31148
31
starting val epoch 31
val [0031 / 0050] validation loss: 	0.39193
checkpoint
starting train epoch 31
 epoch [0031 / 0050] [0009/280] eta: 0 Days 0:11:53        lr: 	1.0000E-05 loss: 	0.39895
 epoch [0031 / 0050] [0109/280] eta: 0 Days 0:11:36        lr: 	1.0000E-05 loss: 	0.30724
 epoch [0031 / 0050] [0209/280] eta: 0 Days 0:11:19        lr: 	1.0000E-05 loss: 	0.30302
32
starting val epoch 32
val [0032 / 0050] validation loss: 	0.38840
starting train epoch 32
 epoch [0032 / 0050] [0009/280] eta: 0 Days 0:11:13        lr: 	1.0000E-05 loss: 	0.34955
 epoch [0032 / 0050] [0109/280] eta: 0 Days 0:10:57        lr: 	1.0000E-05 loss: 	0.31732
 epoch [0032 / 0050] [0209/280] eta: 0 Days 0:10:40        lr: 	1.0000E-05 loss: 	0.31632
33
starting val epoch 33
val [0033 / 0050] validation loss: 	0.38898
starting train epoch 33
 epoch [0033 / 0050] [0009/280] eta: 0 Days 0:10:34        lr: 	1.0000E-05 loss: 	0.29709
 epoch [0033 / 0050] [0109/280] eta: 0 Days 0:10:18        lr: 	1.0000E-05 loss: 	0.31653
 epoch [0033 / 0050] [0209/280] eta: 0 Days 0:10:2         lr: 	1.0000E-05 loss: 	0.30120
34
starting val epoch 34
val [0034 / 0050] validation loss: 	0.39025
checkpoint
starting train epoch 34
 epoch [0034 / 0050] [0009/280] eta: 0 Days 0:9:56         lr: 	1.0000E-05 loss: 	0.33340
 epoch [0034 / 0050] [0109/280] eta: 0 Days 0:9:41         lr: 	1.0000E-05 loss: 	0.28911
 epoch [0034 / 0050] [0209/280] eta: 0 Days 0:9:25         lr: 	1.0000E-05 loss: 	0.29479
35
starting val epoch 35
val [0035 / 0050] validation loss: 	0.37424
checkpoint
starting train epoch 35
 epoch [0035 / 0050] [0009/280] eta: 0 Days 0:9:19         lr: 	1.0000E-05 loss: 	0.34978
 epoch [0035 / 0050] [0109/280] eta: 0 Days 0:9:3          lr: 	1.0000E-05 loss: 	0.31365
 epoch [0035 / 0050] [0209/280] eta: 0 Days 0:8:47         lr: 	1.0000E-05 loss: 	0.29902
36
starting val epoch 36
val [0036 / 0050] validation loss: 	0.36228
checkpoint
starting train epoch 36
 epoch [0036 / 0050] [0009/280] eta: 0 Days 0:8:41         lr: 	1.0000E-05 loss: 	0.35666
 epoch [0036 / 0050] [0109/280] eta: 0 Days 0:8:25         lr: 	1.0000E-05 loss: 	0.28373
 epoch [0036 / 0050] [0209/280] eta: 0 Days 0:8:10         lr: 	1.0000E-05 loss: 	0.28788
37
starting val epoch 37
val [0037 / 0050] validation loss: 	0.34451
checkpoint
starting train epoch 37
 epoch [0037 / 0050] [0009/280] eta: 0 Days 0:8:3          lr: 	1.0000E-05 loss: 	0.32503
 epoch [0037 / 0050] [0109/280] eta: 0 Days 0:7:48         lr: 	1.0000E-05 loss: 	0.26060
 epoch [0037 / 0050] [0209/280] eta: 0 Days 0:7:33         lr: 	1.0000E-05 loss: 	0.27483
38
starting val epoch 38
val [0038 / 0050] validation loss: 	0.34314
checkpoint
starting train epoch 38
 epoch [0038 / 0050] [0009/280] eta: 0 Days 0:7:26         lr: 	1.0000E-05 loss: 	0.30015
 epoch [0038 / 0050] [0109/280] eta: 0 Days 0:7:11         lr: 	1.0000E-05 loss: 	0.28219
 epoch [0038 / 0050] [0209/280] eta: 0 Days 0:6:56         lr: 	1.0000E-05 loss: 	0.27922
39
starting val epoch 39
val [0039 / 0050] validation loss: 	0.34445
checkpoint
starting train epoch 39
 epoch [0039 / 0050] [0009/280] eta: 0 Days 0:6:48         lr: 	1.0000E-05 loss: 	0.22833
 epoch [0039 / 0050] [0109/280] eta: 0 Days 0:6:34         lr: 	1.0000E-05 loss: 	0.28372
 epoch [0039 / 0050] [0209/280] eta: 0 Days 0:6:19         lr: 	1.0000E-05 loss: 	0.27162
40
starting val epoch 40
val [0040 / 0050] validation loss: 	0.35343
starting train epoch 40
 epoch [0040 / 0050] [0009/280] eta: 0 Days 0:6:10         lr: 	1.0000E-05 loss: 	0.33309
 epoch [0040 / 0050] [0109/280] eta: 0 Days 0:5:56         lr: 	1.0000E-05 loss: 	0.27029
 epoch [0040 / 0050] [0209/280] eta: 0 Days 0:5:42         lr: 	1.0000E-05 loss: 	0.26876
41
starting val epoch 41
val [0041 / 0050] validation loss: 	0.34788
checkpoint
starting train epoch 41
 epoch [0041 / 0050] [0009/280] eta: 0 Days 0:5:33         lr: 	1.0000E-05 loss: 	0.32524
 epoch [0041 / 0050] [0109/280] eta: 0 Days 0:5:19         lr: 	1.0000E-05 loss: 	0.26545
 epoch [0041 / 0050] [0209/280] eta: 0 Days 0:5:5          lr: 	1.0000E-05 loss: 	0.27138
42
starting val epoch 42
val [0042 / 0050] validation loss: 	0.34605
checkpoint
starting train epoch 42
 epoch [0042 / 0050] [0009/280] eta: 0 Days 0:4:56         lr: 	1.0000E-05 loss: 	0.26972
 epoch [0042 / 0050] [0109/280] eta: 0 Days 0:4:42         lr: 	1.0000E-05 loss: 	0.26099
 epoch [0042 / 0050] [0209/280] eta: 0 Days 0:4:28         lr: 	1.0000E-05 loss: 	0.24723
43
starting val epoch 43
val [0043 / 0050] validation loss: 	0.34482
checkpoint
starting train epoch 43
 epoch [0043 / 0050] [0009/280] eta: 0 Days 0:4:19         lr: 	1.0000E-05 loss: 	0.28035
 epoch [0043 / 0050] [0109/280] eta: 0 Days 0:4:5          lr: 	1.0000E-05 loss: 	0.26008
 epoch [0043 / 0050] [0209/280] eta: 0 Days 0:3:51         lr: 	1.0000E-05 loss: 	0.25744
44
starting val epoch 44
val [0044 / 0050] validation loss: 	0.34347
checkpoint
starting train epoch 44
 epoch [0044 / 0050] [0009/280] eta: 0 Days 0:3:42         lr: 	1.0000E-05 loss: 	0.33360
 epoch [0044 / 0050] [0109/280] eta: 0 Days 0:3:28         lr: 	1.0000E-05 loss: 	0.24927
 epoch [0044 / 0050] [0209/280] eta: 0 Days 0:3:14         lr: 	1.0000E-05 loss: 	0.24757
45
starting val epoch 45
val [0045 / 0050] validation loss: 	0.34484
starting train epoch 45
 epoch [0045 / 0050] [0009/280] eta: 0 Days 0:3:4          lr: 	1.0000E-05 loss: 	0.37963
 epoch [0045 / 0050] [0109/280] eta: 0 Days 0:2:51         lr: 	1.0000E-05 loss: 	0.26873
 epoch [0045 / 0050] [0209/280] eta: 0 Days 0:2:37         lr: 	1.0000E-05 loss: 	0.25573
46
starting val epoch 46
val [0046 / 0050] validation loss: 	0.34245
checkpoint
starting train epoch 46
 epoch [0046 / 0050] [0009/280] eta: 0 Days 0:2:27         lr: 	1.0000E-05 loss: 	0.18868
 epoch [0046 / 0050] [0109/280] eta: 0 Days 0:2:13         lr: 	1.0000E-05 loss: 	0.22043
 epoch [0046 / 0050] [0209/280] eta: 0 Days 0:2:0          lr: 	1.0000E-05 loss: 	0.24415
47
starting val epoch 47
val [0047 / 0050] validation loss: 	0.34609
checkpoint
starting train epoch 47
 epoch [0047 / 0050] [0009/280] eta: 0 Days 0:1:50         lr: 	1.0000E-05 loss: 	0.33298
 epoch [0047 / 0050] [0109/280] eta: 0 Days 0:1:36         lr: 	1.0000E-05 loss: 	0.24267
 epoch [0047 / 0050] [0209/280] eta: 0 Days 0:1:23         lr: 	1.0000E-05 loss: 	0.24090
48
starting val epoch 48
val [0048 / 0050] validation loss: 	0.34628
checkpoint
starting train epoch 48
 epoch [0048 / 0050] [0009/280] eta: 0 Days 0:1:13         lr: 	1.0000E-05 loss: 	0.29358
 epoch [0048 / 0050] [0109/280] eta: 0 Days 0:0:59         lr: 	1.0000E-05 loss: 	0.24406
 epoch [0048 / 0050] [0209/280] eta: 0 Days 0:0:46         lr: 	1.0000E-05 loss: 	0.25352
49
starting val epoch 49
val [0049 / 0050] validation loss: 	0.34903
checkpoint
starting train epoch 49
 epoch [0049 / 0050] [0009/280] eta: 0 Days 0:0:35         lr: 	1.0000E-05 loss: 	0.19385
 epoch [0049 / 0050] [0109/280] eta: 0 Days 0:0:22         lr: 	1.0000E-05 loss: 	0.23820
 epoch [0049 / 0050] [0209/280] eta: 0 Days 0:0:9          lr: 	1.0000E-05 loss: 	0.24735
starting val epoch 0
val [0000 / 0050] validation loss: 	0.35846
Acute and unspecified renal failure                                                        & 0.835(0.866, 0.804) & 0.528 (0.602, 0.455)
fused_ehr test  0   best mean auc :0.835 mean auprc 0.528
                    CI AUROC (0.804, 0.866) CI AUPRC (0.455, 0.602)
                     AUROC accute 0.835 mixed 0.835 chronic 0.835
                     AUROC accute CI (0.804, 0.866) mixed (0.804 , 0.866) chronic (0.804, 0.866)
                     AUPRC accute  0.528 mixed 0.528 chronic 0.528
                     AUPRC accute CI  (0.455, 0.602) mixed (0.455,  0.602) chronic (0.455, 0.602)