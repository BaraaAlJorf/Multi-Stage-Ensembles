Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ehr loaded
cxr loaded
==> training
running for fusion_type early
0
starting val epoch 0
val [0000 / 0050] validation loss: 	0.77787
checkpoint
starting train epoch 0
 epoch [0000 / 0050] [0009/280] eta: 0 Days 13:23:15       lr: 	1.0000E-06 loss: 	0.82387
 epoch [0000 / 0050] [0109/280] eta: 0 Days 1:30:39        lr: 	1.0000E-06 loss: 	0.60431
 epoch [0000 / 0050] [0209/280] eta: 0 Days 0:56:17        lr: 	1.0000E-06 loss: 	0.52606
1
starting val epoch 1
val [0001 / 0050] validation loss: 	0.42536
checkpoint
starting train epoch 1
 epoch [0001 / 0050] [0009/280] eta: 0 Days 0:57:46        lr: 	1.0000E-06 loss: 	0.45407
 epoch [0001 / 0050] [0109/280] eta: 0 Days 0:47:33        lr: 	1.0000E-06 loss: 	0.42103
 epoch [0001 / 0050] [0209/280] eta: 0 Days 0:41:39        lr: 	1.0000E-06 loss: 	0.41328
2
starting val epoch 2
val [0002 / 0050] validation loss: 	0.41369
checkpoint
starting train epoch 2
 epoch [0002 / 0050] [0009/280] eta: 0 Days 0:44:5         lr: 	1.0000E-06 loss: 	0.47824
 epoch [0002 / 0050] [0109/280] eta: 0 Days 0:40:2         lr: 	1.0000E-06 loss: 	0.42106
 epoch [0002 / 0050] [0209/280] eta: 0 Days 0:37:1         lr: 	1.0000E-06 loss: 	0.39870
3
starting val epoch 3
val [0003 / 0050] validation loss: 	0.40587
checkpoint
starting train epoch 3
 epoch [0003 / 0050] [0009/280] eta: 0 Days 0:38:53        lr: 	1.0000E-06 loss: 	0.43242
 epoch [0003 / 0050] [0109/280] eta: 0 Days 0:36:33        lr: 	1.0000E-06 loss: 	0.40187
 epoch [0003 / 0050] [0209/280] eta: 0 Days 0:34:33        lr: 	1.0000E-06 loss: 	0.39789
4
starting val epoch 4
val [0004 / 0050] validation loss: 	0.39938
checkpoint
starting train epoch 4
 epoch [0004 / 0050] [0009/280] eta: 0 Days 0:36:6         lr: 	1.0000E-06 loss: 	0.43849
 epoch [0004 / 0050] [0109/280] eta: 0 Days 0:34:26        lr: 	1.0000E-06 loss: 	0.39980
 epoch [0004 / 0050] [0209/280] eta: 0 Days 0:32:54        lr: 	1.0000E-06 loss: 	0.38977
5
starting val epoch 5
val [0005 / 0050] validation loss: 	0.39448
checkpoint
starting train epoch 5
 epoch [0005 / 0050] [0009/280] eta: 0 Days 0:34:9         lr: 	1.0000E-06 loss: 	0.43765
 epoch [0005 / 0050] [0109/280] eta: 0 Days 0:32:51        lr: 	1.0000E-06 loss: 	0.39260
 epoch [0005 / 0050] [0209/280] eta: 0 Days 0:31:38        lr: 	1.0000E-06 loss: 	0.37736
6
starting val epoch 6
val [0006 / 0050] validation loss: 	0.39077
checkpoint
starting train epoch 6
 epoch [0006 / 0050] [0009/280] eta: 0 Days 0:32:35        lr: 	1.0000E-06 loss: 	0.47924
 epoch [0006 / 0050] [0109/280] eta: 0 Days 0:31:30        lr: 	1.0000E-06 loss: 	0.39748
 epoch [0006 / 0050] [0209/280] eta: 0 Days 0:30:37        lr: 	1.0000E-06 loss: 	0.38189
7
starting val epoch 7
val [0007 / 0050] validation loss: 	0.38892
checkpoint
starting train epoch 7
 epoch [0007 / 0050] [0009/280] eta: 0 Days 0:31:24        lr: 	1.0000E-06 loss: 	0.30422
 epoch [0007 / 0050] [0109/280] eta: 0 Days 0:30:30        lr: 	1.0000E-06 loss: 	0.35932
 epoch [0007 / 0050] [0209/280] eta: 0 Days 0:29:39        lr: 	1.0000E-06 loss: 	0.36087
8
starting val epoch 8
val [0008 / 0050] validation loss: 	0.38681
starting train epoch 8
 epoch [0008 / 0050] [0009/280] eta: 0 Days 0:30:3         lr: 	1.0000E-06 loss: 	0.40782
 epoch [0008 / 0050] [0109/280] eta: 0 Days 0:29:11        lr: 	1.0000E-06 loss: 	0.37343
 epoch [0008 / 0050] [0209/280] eta: 0 Days 0:28:23        lr: 	1.0000E-06 loss: 	0.37110
9
starting val epoch 9
val [0009 / 0050] validation loss: 	0.38656
checkpoint
starting train epoch 9
 epoch [0009 / 0050] [0009/280] eta: 0 Days 0:28:54        lr: 	1.0000E-06 loss: 	0.39741
 epoch [0009 / 0050] [0109/280] eta: 0 Days 0:28:8         lr: 	1.0000E-06 loss: 	0.36436
 epoch [0009 / 0050] [0209/280] eta: 0 Days 0:27:23        lr: 	1.0000E-06 loss: 	0.36373
10
starting val epoch 10
val [0010 / 0050] validation loss: 	0.38590
checkpoint
starting train epoch 10
 epoch [0010 / 0050] [0009/280] eta: 0 Days 0:27:50        lr: 	1.0000E-06 loss: 	0.34623
 epoch [0010 / 0050] [0109/280] eta: 0 Days 0:27:9         lr: 	1.0000E-06 loss: 	0.35214
 epoch [0010 / 0050] [0209/280] eta: 0 Days 0:26:29        lr: 	1.0000E-06 loss: 	0.36118
11
starting val epoch 11
val [0011 / 0050] validation loss: 	0.38567
checkpoint
starting train epoch 11
 epoch [0011 / 0050] [0009/280] eta: 0 Days 0:26:53        lr: 	1.0000E-06 loss: 	0.35528
 epoch [0011 / 0050] [0109/280] eta: 0 Days 0:26:14        lr: 	1.0000E-06 loss: 	0.35796
 epoch [0011 / 0050] [0209/280] eta: 0 Days 0:25:39        lr: 	1.0000E-06 loss: 	0.36516
12
starting val epoch 12
val [0012 / 0050] validation loss: 	0.38623
starting train epoch 12
 epoch [0012 / 0050] [0009/280] eta: 0 Days 0:25:51        lr: 	1.0000E-06 loss: 	0.38349
 epoch [0012 / 0050] [0109/280] eta: 0 Days 0:25:17        lr: 	1.0000E-06 loss: 	0.35174
 epoch [0012 / 0050] [0209/280] eta: 0 Days 0:24:44        lr: 	1.0000E-06 loss: 	0.36524
13
starting val epoch 13
val [0013 / 0050] validation loss: 	0.38652
starting train epoch 13
 epoch [0013 / 0050] [0009/280] eta: 0 Days 0:24:55        lr: 	1.0000E-06 loss: 	0.46732
 epoch [0013 / 0050] [0109/280] eta: 0 Days 0:24:23        lr: 	1.0000E-06 loss: 	0.39054
 epoch [0013 / 0050] [0209/280] eta: 0 Days 0:23:54        lr: 	1.0000E-06 loss: 	0.37180
14
starting val epoch 14
val [0014 / 0050] validation loss: 	0.38859
starting train epoch 14
 epoch [0014 / 0050] [0009/280] eta: 0 Days 0:24:3         lr: 	1.0000E-06 loss: 	0.43524
 epoch [0014 / 0050] [0109/280] eta: 0 Days 0:23:33        lr: 	1.0000E-06 loss: 	0.36905
 epoch [0014 / 0050] [0209/280] eta: 0 Days 0:23:3         lr: 	1.0000E-06 loss: 	0.36202
15
starting val epoch 15
val [0015 / 0050] validation loss: 	0.38790
starting train epoch 15
 epoch [0015 / 0050] [0009/280] eta: 0 Days 0:23:10        lr: 	1.0000E-06 loss: 	0.35648
 epoch [0015 / 0050] [0109/280] eta: 0 Days 0:22:43        lr: 	1.0000E-06 loss: 	0.36443
 epoch [0015 / 0050] [0209/280] eta: 0 Days 0:22:16        lr: 	1.0000E-06 loss: 	0.36223
16
starting val epoch 16
val [0016 / 0050] validation loss: 	0.38788
starting train epoch 16
 epoch [0016 / 0050] [0009/280] eta: 0 Days 0:22:22        lr: 	1.0000E-06 loss: 	0.37819
 epoch [0016 / 0050] [0109/280] eta: 0 Days 0:21:55        lr: 	1.0000E-06 loss: 	0.35185
 epoch [0016 / 0050] [0209/280] eta: 0 Days 0:21:29        lr: 	1.0000E-06 loss: 	0.37075
17
starting val epoch 17
val [0017 / 0050] validation loss: 	0.38785
starting train epoch 17
 epoch [0017 / 0050] [0009/280] eta: 0 Days 0:21:32        lr: 	1.0000E-06 loss: 	0.37341
 epoch [0017 / 0050] [0109/280] eta: 0 Days 0:21:8         lr: 	1.0000E-06 loss: 	0.38001
 epoch [0017 / 0050] [0209/280] eta: 0 Days 0:20:43        lr: 	1.0000E-06 loss: 	0.36556
18
starting val epoch 18
val [0018 / 0050] validation loss: 	0.38832
starting train epoch 18
 epoch [0018 / 0050] [0009/280] eta: 0 Days 0:20:47        lr: 	1.0000E-06 loss: 	0.38815
 epoch [0018 / 0050] [0109/280] eta: 0 Days 0:20:22        lr: 	1.0000E-06 loss: 	0.37113
 epoch [0018 / 0050] [0209/280] eta: 0 Days 0:19:58        lr: 	1.0000E-06 loss: 	0.35447
19
starting val epoch 19
val [0019 / 0050] validation loss: 	0.38855
starting train epoch 19
 epoch [0019 / 0050] [0009/280] eta: 0 Days 0:20:0         lr: 	1.0000E-06 loss: 	0.32501
 epoch [0019 / 0050] [0109/280] eta: 0 Days 0:19:36        lr: 	1.0000E-06 loss: 	0.35223
 epoch [0019 / 0050] [0209/280] eta: 0 Days 0:19:13        lr: 	1.0000E-06 loss: 	0.36316
20
starting val epoch 20
val [0020 / 0050] validation loss: 	0.38821
starting train epoch 20
 epoch [0020 / 0050] [0009/280] eta: 0 Days 0:19:14        lr: 	1.0000E-06 loss: 	0.39519
 epoch [0020 / 0050] [0109/280] eta: 0 Days 0:18:52        lr: 	1.0000E-06 loss: 	0.36099
 epoch [0020 / 0050] [0209/280] eta: 0 Days 0:18:30        lr: 	1.0000E-06 loss: 	0.35997
21
starting val epoch 21
val [0021 / 0050] validation loss: 	0.38858
starting train epoch 21
 epoch [0021 / 0050] [0009/280] eta: 0 Days 0:18:30        lr: 	1.0000E-06 loss: 	0.43026
 epoch [0021 / 0050] [0109/280] eta: 0 Days 0:18:9         lr: 	1.0000E-06 loss: 	0.36354
 epoch [0021 / 0050] [0209/280] eta: 0 Days 0:17:47        lr: 	1.0000E-06 loss: 	0.36036
22
starting val epoch 22
val [0022 / 0050] validation loss: 	0.38892
starting train epoch 22
 epoch [0022 / 0050] [0009/280] eta: 0 Days 0:17:47        lr: 	1.0000E-06 loss: 	0.36359
 epoch [0022 / 0050] [0109/280] eta: 0 Days 0:17:26        lr: 	1.0000E-06 loss: 	0.34749
 epoch [0022 / 0050] [0209/280] eta: 0 Days 0:17:5         lr: 	1.0000E-06 loss: 	0.37279
23
starting val epoch 23
val [0023 / 0050] validation loss: 	0.38877
starting train epoch 23
 epoch [0023 / 0050] [0009/280] eta: 0 Days 0:17:4         lr: 	1.0000E-06 loss: 	0.37625
 epoch [0023 / 0050] [0109/280] eta: 0 Days 0:16:44        lr: 	1.0000E-06 loss: 	0.35598
 epoch [0023 / 0050] [0209/280] eta: 0 Days 0:16:25        lr: 	1.0000E-06 loss: 	0.35735
24
starting val epoch 24
val [0024 / 0050] validation loss: 	0.38938
starting train epoch 24
 epoch [0024 / 0050] [0009/280] eta: 0 Days 0:16:23        lr: 	1.0000E-06 loss: 	0.45252
 epoch [0024 / 0050] [0109/280] eta: 0 Days 0:16:4         lr: 	1.0000E-06 loss: 	0.34876
 epoch [0024 / 0050] [0209/280] eta: 0 Days 0:15:45        lr: 	1.0000E-06 loss: 	0.36169
25
starting val epoch 25
val [0025 / 0050] validation loss: 	0.38981
starting train epoch 25
 epoch [0025 / 0050] [0009/280] eta: 0 Days 0:15:44        lr: 	1.0000E-06 loss: 	0.37064
 epoch [0025 / 0050] [0109/280] eta: 0 Days 0:15:26        lr: 	1.0000E-06 loss: 	0.35158
 epoch [0025 / 0050] [0209/280] eta: 0 Days 0:15:7         lr: 	1.0000E-06 loss: 	0.35935
26
starting val epoch 26
val [0026 / 0050] validation loss: 	0.38869
starting val epoch 0
val [0000 / 0050] validation loss: 	0.39351
Acute and unspecified renal failure                                                        & 0.703(0.742, 0.664) & 0.305 (0.371, 0.252)
fused_ehr test  0   best mean auc :0.703 mean auprc 0.305
                    CI AUROC (0.664, 0.742) CI AUPRC (0.252, 0.371)
                     AUROC accute 0.703 mixed 0.703 chronic 0.703
                     AUROC accute CI (0.664, 0.742) mixed (0.664 , 0.742) chronic (0.664, 0.742)
                     AUPRC accute  0.305 mixed 0.305 chronic 0.305
                     AUPRC accute CI  (0.252, 0.371) mixed (0.252,  0.371) chronic (0.252, 0.371)