Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ehr loaded
cxr loaded
rr loaded
==> training
running for fusion_type late
0
starting val epoch 0
val [0000 / 0050] validation loss: 	0.86368
checkpoint
starting train epoch 0
 epoch [0000 / 0050] [0009/267] eta: 0 Days 12:53:35       lr: 	1.0000E-06 loss: 	0.84647
 epoch [0000 / 0050] [0109/267] eta: 0 Days 2:30:56        lr: 	1.0000E-06 loss: 	0.53067
 epoch [0000 / 0050] [0209/267] eta: 0 Days 1:59:52        lr: 	1.0000E-06 loss: 	0.45332
1
starting val epoch 1
val [0001 / 0050] validation loss: 	0.40059
checkpoint
starting train epoch 1
 epoch [0001 / 0050] [0009/267] eta: 0 Days 2:4:36         lr: 	1.0000E-06 loss: 	0.44501
 epoch [0001 / 0050] [0109/267] eta: 0 Days 1:53:53        lr: 	1.0000E-06 loss: 	0.37022
 epoch [0001 / 0050] [0209/267] eta: 0 Days 1:47:15        lr: 	1.0000E-06 loss: 	0.36628
2
starting val epoch 2
val [0002 / 0050] validation loss: 	0.37505
checkpoint
starting train epoch 2
 epoch [0002 / 0050] [0009/267] eta: 0 Days 1:50:33        lr: 	1.0000E-06 loss: 	0.34771
 epoch [0002 / 0050] [0109/267] eta: 0 Days 1:45:31        lr: 	1.0000E-06 loss: 	0.32702
 epoch [0002 / 0050] [0209/267] eta: 0 Days 1:41:56        lr: 	1.0000E-06 loss: 	0.32723
3
starting val epoch 3
val [0003 / 0050] validation loss: 	0.35597
checkpoint
starting train epoch 3
 epoch [0003 / 0050] [0009/267] eta: 0 Days 1:44:23        lr: 	1.0000E-06 loss: 	0.28860
 epoch [0003 / 0050] [0109/267] eta: 0 Days 1:41:10        lr: 	1.0000E-06 loss: 	0.30945
 epoch [0003 / 0050] [0209/267] eta: 0 Days 1:38:26        lr: 	1.0000E-06 loss: 	0.30288
4
starting val epoch 4
val [0004 / 0050] validation loss: 	0.34853
checkpoint
starting train epoch 4
 epoch [0004 / 0050] [0009/267] eta: 0 Days 1:39:57        lr: 	1.0000E-06 loss: 	0.24819
 epoch [0004 / 0050] [0109/267] eta: 0 Days 1:37:42        lr: 	1.0000E-06 loss: 	0.28944
 epoch [0004 / 0050] [0209/267] eta: 0 Days 1:35:40        lr: 	1.0000E-06 loss: 	0.28966
5
starting val epoch 5
val [0005 / 0050] validation loss: 	0.33886
checkpoint
starting train epoch 5
 epoch [0005 / 0050] [0009/267] eta: 0 Days 1:36:58        lr: 	1.0000E-06 loss: 	0.27383
 epoch [0005 / 0050] [0109/267] eta: 0 Days 1:34:55        lr: 	1.0000E-06 loss: 	0.27672
 epoch [0005 / 0050] [0209/267] eta: 0 Days 1:33:10        lr: 	1.0000E-06 loss: 	0.27761
6
starting val epoch 6
val [0006 / 0050] validation loss: 	0.34491
checkpoint
starting train epoch 6
 epoch [0006 / 0050] [0009/267] eta: 0 Days 1:34:1         lr: 	1.0000E-06 loss: 	0.30483
 epoch [0006 / 0050] [0109/267] eta: 0 Days 1:32:19        lr: 	1.0000E-06 loss: 	0.25945
 epoch [0006 / 0050] [0209/267] eta: 0 Days 1:30:42        lr: 	1.0000E-06 loss: 	0.26982
7
starting val epoch 7
val [0007 / 0050] validation loss: 	0.36268
checkpoint
starting train epoch 7
 epoch [0007 / 0050] [0009/267] eta: 0 Days 1:31:32        lr: 	1.0000E-06 loss: 	0.25620
 epoch [0007 / 0050] [0109/267] eta: 0 Days 1:30:0         lr: 	1.0000E-06 loss: 	0.26629
 epoch [0007 / 0050] [0209/267] eta: 0 Days 1:28:38        lr: 	1.0000E-06 loss: 	0.25792
8
starting val epoch 8
val [0008 / 0050] validation loss: 	0.34090
checkpoint
starting train epoch 8
 epoch [0008 / 0050] [0009/267] eta: 0 Days 1:29:12        lr: 	1.0000E-06 loss: 	0.23759
 epoch [0008 / 0050] [0109/267] eta: 0 Days 1:27:49        lr: 	1.0000E-06 loss: 	0.24262
 epoch [0008 / 0050] [0209/267] eta: 0 Days 1:26:29        lr: 	1.0000E-06 loss: 	0.24836
9
starting val epoch 9
val [0009 / 0050] validation loss: 	0.34132
checkpoint
starting train epoch 9
 epoch [0009 / 0050] [0009/267] eta: 0 Days 1:26:53        lr: 	1.0000E-06 loss: 	0.24583
 epoch [0009 / 0050] [0109/267] eta: 0 Days 1:25:36        lr: 	1.0000E-06 loss: 	0.24298
 epoch [0009 / 0050] [0209/267] eta: 0 Days 1:24:17        lr: 	1.0000E-06 loss: 	0.23690
10
starting val epoch 10
val [0010 / 0050] validation loss: 	0.36932
checkpoint
starting train epoch 10
 epoch [0010 / 0050] [0009/267] eta: 0 Days 1:24:34        lr: 	1.0000E-06 loss: 	0.36765
 epoch [0010 / 0050] [0109/267] eta: 0 Days 1:23:17        lr: 	1.0000E-06 loss: 	0.22346
 epoch [0010 / 0050] [0209/267] eta: 0 Days 1:22:5         lr: 	1.0000E-06 loss: 	0.22475
11
starting val epoch 11
val [0011 / 0050] validation loss: 	0.37183
checkpoint
starting train epoch 11
 epoch [0011 / 0050] [0009/267] eta: 0 Days 1:22:20        lr: 	1.0000E-06 loss: 	0.24169
 epoch [0011 / 0050] [0109/267] eta: 0 Days 1:21:6         lr: 	1.0000E-06 loss: 	0.22472
 epoch [0011 / 0050] [0209/267] eta: 0 Days 1:19:56        lr: 	1.0000E-06 loss: 	0.21228
12
starting val epoch 12
val [0012 / 0050] validation loss: 	0.36239
starting train epoch 12
 epoch [0012 / 0050] [0009/267] eta: 0 Days 1:19:53        lr: 	1.0000E-06 loss: 	0.27931
 epoch [0012 / 0050] [0109/267] eta: 0 Days 1:18:45        lr: 	1.0000E-06 loss: 	0.19808
 epoch [0012 / 0050] [0209/267] eta: 0 Days 1:17:36        lr: 	1.0000E-06 loss: 	0.20062
13
starting val epoch 13
val [0013 / 0050] validation loss: 	0.40977
starting train epoch 13
 epoch [0013 / 0050] [0009/267] eta: 0 Days 1:17:31        lr: 	1.0000E-06 loss: 	0.11255
 epoch [0013 / 0050] [0109/267] eta: 0 Days 1:16:25        lr: 	1.0000E-06 loss: 	0.20000
 epoch [0013 / 0050] [0209/267] eta: 0 Days 1:15:20        lr: 	1.0000E-06 loss: 	0.19761
14
starting val epoch 14
val [0014 / 0050] validation loss: 	0.38436
starting train epoch 14
 epoch [0014 / 0050] [0009/267] eta: 0 Days 1:15:11        lr: 	1.0000E-06 loss: 	0.18806
 epoch [0014 / 0050] [0109/267] eta: 0 Days 1:14:7         lr: 	1.0000E-06 loss: 	0.20664
 epoch [0014 / 0050] [0209/267] eta: 0 Days 1:13:5         lr: 	1.0000E-06 loss: 	0.19278
15
starting val epoch 15
val [0015 / 0050] validation loss: 	0.41307
starting train epoch 15
 epoch [0015 / 0050] [0009/267] eta: 0 Days 1:12:59        lr: 	1.0000E-06 loss: 	0.14963
 epoch [0015 / 0050] [0109/267] eta: 0 Days 1:11:57        lr: 	1.0000E-06 loss: 	0.16835
 epoch [0015 / 0050] [0209/267] eta: 0 Days 1:10:55        lr: 	1.0000E-06 loss: 	0.17235
16
starting val epoch 16
val [0016 / 0050] validation loss: 	0.43188
starting train epoch 16
 epoch [0016 / 0050] [0009/267] eta: 0 Days 1:10:42        lr: 	1.0000E-06 loss: 	0.19522
 epoch [0016 / 0050] [0109/267] eta: 0 Days 1:9:42         lr: 	1.0000E-06 loss: 	0.16077
 epoch [0016 / 0050] [0209/267] eta: 0 Days 1:8:43         lr: 	1.0000E-06 loss: 	0.16495
17
starting val epoch 17
val [0017 / 0050] validation loss: 	0.40241
starting train epoch 17
 epoch [0017 / 0050] [0009/267] eta: 0 Days 1:8:29         lr: 	1.0000E-06 loss: 	0.15229
 epoch [0017 / 0050] [0109/267] eta: 0 Days 1:7:32         lr: 	1.0000E-06 loss: 	0.14640
 epoch [0017 / 0050] [0209/267] eta: 0 Days 1:6:34         lr: 	1.0000E-06 loss: 	0.15224
18
starting val epoch 18
val [0018 / 0050] validation loss: 	0.41665
starting train epoch 18
 epoch [0018 / 0050] [0009/267] eta: 0 Days 1:6:19         lr: 	1.0000E-06 loss: 	0.10853
 epoch [0018 / 0050] [0109/267] eta: 0 Days 1:5:24         lr: 	1.0000E-06 loss: 	0.14569
 epoch [0018 / 0050] [0209/267] eta: 0 Days 1:4:27         lr: 	1.0000E-06 loss: 	0.14332
19
starting val epoch 19
val [0019 / 0050] validation loss: 	0.45771
starting train epoch 19
 epoch [0019 / 0050] [0009/267] eta: 0 Days 1:4:11         lr: 	1.0000E-06 loss: 	0.14579
 epoch [0019 / 0050] [0109/267] eta: 0 Days 1:3:14         lr: 	1.0000E-06 loss: 	0.12964
 epoch [0019 / 0050] [0209/267] eta: 0 Days 1:2:18         lr: 	1.0000E-06 loss: 	0.13318
20
starting val epoch 20
val [0020 / 0050] validation loss: 	0.44397
starting train epoch 20
 epoch [0020 / 0050] [0009/267] eta: 0 Days 1:2:0          lr: 	1.0000E-06 loss: 	0.21945
 epoch [0020 / 0050] [0109/267] eta: 0 Days 1:1:5          lr: 	1.0000E-06 loss: 	0.13228
 epoch [0020 / 0050] [0209/267] eta: 0 Days 1:0:9          lr: 	1.0000E-06 loss: 	0.13023
21
starting val epoch 21
val [0021 / 0050] validation loss: 	0.44687
starting train epoch 21
 epoch [0021 / 0050] [0009/267] eta: 0 Days 0:59:51        lr: 	1.0000E-06 loss: 	0.16424
 epoch [0021 / 0050] [0109/267] eta: 0 Days 0:58:56        lr: 	1.0000E-06 loss: 	0.12022
 epoch [0021 / 0050] [0209/267] eta: 0 Days 0:58:3         lr: 	1.0000E-06 loss: 	0.12479
22
starting val epoch 22
val [0022 / 0050] validation loss: 	0.47639
starting train epoch 22
 epoch [0022 / 0050] [0009/267] eta: 0 Days 0:57:44        lr: 	1.0000E-06 loss: 	0.08913
 epoch [0022 / 0050] [0109/267] eta: 0 Days 0:56:50        lr: 	1.0000E-06 loss: 	0.09767
 epoch [0022 / 0050] [0209/267] eta: 0 Days 0:55:56        lr: 	1.0000E-06 loss: 	0.10280
23
starting val epoch 23
val [0023 / 0050] validation loss: 	0.49098
starting train epoch 23
 epoch [0023 / 0050] [0009/267] eta: 0 Days 0:55:38        lr: 	1.0000E-06 loss: 	0.10686
 epoch [0023 / 0050] [0109/267] eta: 0 Days 0:54:46        lr: 	1.0000E-06 loss: 	0.11025
 epoch [0023 / 0050] [0209/267] eta: 0 Days 0:53:54        lr: 	1.0000E-06 loss: 	0.11140
24
starting val epoch 24
val [0024 / 0050] validation loss: 	0.48423
starting train epoch 24
 epoch [0024 / 0050] [0009/267] eta: 0 Days 0:53:33        lr: 	1.0000E-06 loss: 	0.09598
 epoch [0024 / 0050] [0109/267] eta: 0 Days 0:52:41        lr: 	1.0000E-06 loss: 	0.09385
 epoch [0024 / 0050] [0209/267] eta: 0 Days 0:51:49        lr: 	1.0000E-06 loss: 	0.09299
25
starting val epoch 25
val [0025 / 0050] validation loss: 	0.50135
starting train epoch 25
 epoch [0025 / 0050] [0009/267] eta: 0 Days 0:51:27        lr: 	1.0000E-06 loss: 	0.09288
 epoch [0025 / 0050] [0109/267] eta: 0 Days 0:50:35        lr: 	1.0000E-06 loss: 	0.08388
 epoch [0025 / 0050] [0209/267] eta: 0 Days 0:49:43        lr: 	1.0000E-06 loss: 	0.09242
26
starting val epoch 26
val [0026 / 0050] validation loss: 	0.57477
starting val epoch 0
val [0000 / 0050] validation loss: 	0.34721
Acute and unspecified renal failure                                                        & 0.852(0.881, 0.824) & 0.517 (0.604, 0.441)
fused_ehr test  0   best mean auc :0.852 mean auprc 0.517
                    CI AUROC (0.824, 0.881) CI AUPRC (0.441, 0.604)
                     AUROC accute 0.852 mixed 0.852 chronic 0.852
                     AUROC accute CI (0.824, 0.881) mixed (0.824 , 0.881) chronic (0.824, 0.881)
                     AUPRC accute  0.517 mixed 0.517 chronic 0.517
                     AUPRC accute CI  (0.441, 0.604) mixed (0.441,  0.604) chronic (0.441, 0.604)