Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ehr loaded
cxr loaded
==> training
running for fusion_type joint
0
starting val epoch 0
val [0000 / 0050] validation loss: 	0.77787
checkpoint
starting train epoch 0
 epoch [0000 / 0050] [0009/280] eta: 0 Days 12:18:28       lr: 	1.0000E-07 loss: 	0.83467
 epoch [0000 / 0050] [0109/280] eta: 0 Days 1:49:28        lr: 	1.0000E-07 loss: 	0.70016
 epoch [0000 / 0050] [0209/280] eta: 0 Days 1:20:16        lr: 	1.0000E-07 loss: 	0.64799
1
starting val epoch 1
val [0001 / 0050] validation loss: 	0.54106
checkpoint
starting train epoch 1
 epoch [0001 / 0050] [0009/280] eta: 0 Days 1:20:52        lr: 	1.0000E-07 loss: 	0.56943
 epoch [0001 / 0050] [0109/280] eta: 0 Days 1:11:39        lr: 	1.0000E-07 loss: 	0.50060
 epoch [0001 / 0050] [0209/280] eta: 0 Days 1:5:52         lr: 	1.0000E-07 loss: 	0.48032
2
starting val epoch 2
val [0002 / 0050] validation loss: 	0.45394
checkpoint
starting train epoch 2
 epoch [0002 / 0050] [0009/280] eta: 0 Days 1:7:14         lr: 	1.0000E-07 loss: 	0.50951
 epoch [0002 / 0050] [0109/280] eta: 0 Days 1:3:19         lr: 	1.0000E-07 loss: 	0.44769
 epoch [0002 / 0050] [0209/280] eta: 0 Days 1:0:41         lr: 	1.0000E-07 loss: 	0.42639
3
starting val epoch 3
val [0003 / 0050] validation loss: 	0.42989
checkpoint
starting train epoch 3
 epoch [0003 / 0050] [0009/280] eta: 0 Days 1:2:0          lr: 	1.0000E-07 loss: 	0.45144
 epoch [0003 / 0050] [0109/280] eta: 0 Days 0:59:47        lr: 	1.0000E-07 loss: 	0.42411
 epoch [0003 / 0050] [0209/280] eta: 0 Days 0:57:54        lr: 	1.0000E-07 loss: 	0.42096
4
starting val epoch 4
val [0004 / 0050] validation loss: 	0.42234
checkpoint
starting train epoch 4
 epoch [0004 / 0050] [0009/280] eta: 0 Days 0:58:48        lr: 	1.0000E-07 loss: 	0.46741
 epoch [0004 / 0050] [0109/280] eta: 0 Days 0:57:3         lr: 	1.0000E-07 loss: 	0.42886
 epoch [0004 / 0050] [0209/280] eta: 0 Days 0:55:35        lr: 	1.0000E-07 loss: 	0.41943
5
starting val epoch 5
val [0005 / 0050] validation loss: 	0.41828
checkpoint
starting train epoch 5
 epoch [0005 / 0050] [0009/280] eta: 0 Days 0:56:20        lr: 	1.0000E-07 loss: 	0.47580
 epoch [0005 / 0050] [0109/280] eta: 0 Days 0:54:55        lr: 	1.0000E-07 loss: 	0.42457
 epoch [0005 / 0050] [0209/280] eta: 0 Days 0:53:38        lr: 	1.0000E-07 loss: 	0.40972
6
starting val epoch 6
val [0006 / 0050] validation loss: 	0.41517
checkpoint
starting train epoch 6
 epoch [0006 / 0050] [0009/280] eta: 0 Days 0:54:14        lr: 	1.0000E-07 loss: 	0.50637
 epoch [0006 / 0050] [0109/280] eta: 0 Days 0:53:8         lr: 	1.0000E-07 loss: 	0.42844
 epoch [0006 / 0050] [0209/280] eta: 0 Days 0:52:2         lr: 	1.0000E-07 loss: 	0.41278
7
starting val epoch 7
val [0007 / 0050] validation loss: 	0.41257
checkpoint
starting train epoch 7
 epoch [0007 / 0050] [0009/280] eta: 0 Days 0:52:25        lr: 	1.0000E-07 loss: 	0.33746
 epoch [0007 / 0050] [0109/280] eta: 0 Days 0:51:17        lr: 	1.0000E-07 loss: 	0.39290
 epoch [0007 / 0050] [0209/280] eta: 0 Days 0:50:17        lr: 	1.0000E-07 loss: 	0.39305
8
starting val epoch 8
val [0008 / 0050] validation loss: 	0.41016
checkpoint
starting train epoch 8
 epoch [0008 / 0050] [0009/280] eta: 0 Days 0:50:36        lr: 	1.0000E-07 loss: 	0.45305
 epoch [0008 / 0050] [0109/280] eta: 0 Days 0:49:38        lr: 	1.0000E-07 loss: 	0.41080
 epoch [0008 / 0050] [0209/280] eta: 0 Days 0:48:46        lr: 	1.0000E-07 loss: 	0.40382
9
starting val epoch 9
val [0009 / 0050] validation loss: 	0.40778
checkpoint
starting train epoch 9
 epoch [0009 / 0050] [0009/280] eta: 0 Days 0:49:0         lr: 	1.0000E-07 loss: 	0.44828
 epoch [0009 / 0050] [0109/280] eta: 0 Days 0:48:8         lr: 	1.0000E-07 loss: 	0.40261
 epoch [0009 / 0050] [0209/280] eta: 0 Days 0:47:20        lr: 	1.0000E-07 loss: 	0.39840
10
starting val epoch 10
val [0010 / 0050] validation loss: 	0.40539
checkpoint
starting train epoch 10
 epoch [0010 / 0050] [0009/280] eta: 0 Days 0:47:30        lr: 	1.0000E-07 loss: 	0.38087
 epoch [0010 / 0050] [0109/280] eta: 0 Days 0:46:43        lr: 	1.0000E-07 loss: 	0.38478
 epoch [0010 / 0050] [0209/280] eta: 0 Days 0:45:53        lr: 	1.0000E-07 loss: 	0.39130
11
starting val epoch 11
val [0011 / 0050] validation loss: 	0.40325
checkpoint
starting train epoch 11
 epoch [0011 / 0050] [0009/280] eta: 0 Days 0:46:0         lr: 	1.0000E-07 loss: 	0.37731
 epoch [0011 / 0050] [0109/280] eta: 0 Days 0:45:16        lr: 	1.0000E-07 loss: 	0.38976
 epoch [0011 / 0050] [0209/280] eta: 0 Days 0:44:33        lr: 	1.0000E-07 loss: 	0.39541
12
starting val epoch 12
val [0012 / 0050] validation loss: 	0.40111
checkpoint
starting train epoch 12
 epoch [0012 / 0050] [0009/280] eta: 0 Days 0:44:38        lr: 	1.0000E-07 loss: 	0.40521
 epoch [0012 / 0050] [0109/280] eta: 0 Days 0:43:55        lr: 	1.0000E-07 loss: 	0.38343
 epoch [0012 / 0050] [0209/280] eta: 0 Days 0:43:16        lr: 	1.0000E-07 loss: 	0.39424
13
starting val epoch 13
val [0013 / 0050] validation loss: 	0.39916
checkpoint
starting train epoch 13
 epoch [0013 / 0050] [0009/280] eta: 0 Days 0:43:19        lr: 	1.0000E-07 loss: 	0.49852
 epoch [0013 / 0050] [0109/280] eta: 0 Days 0:42:38        lr: 	1.0000E-07 loss: 	0.41770
 epoch [0013 / 0050] [0209/280] eta: 0 Days 0:41:59        lr: 	1.0000E-07 loss: 	0.39919
14
starting val epoch 14
val [0014 / 0050] validation loss: 	0.39778
checkpoint
starting train epoch 14
 epoch [0014 / 0050] [0009/280] eta: 0 Days 0:42:2         lr: 	1.0000E-07 loss: 	0.48359
 epoch [0014 / 0050] [0109/280] eta: 0 Days 0:41:27        lr: 	1.0000E-07 loss: 	0.39684
 epoch [0014 / 0050] [0209/280] eta: 0 Days 0:40:49        lr: 	1.0000E-07 loss: 	0.38764
15
starting val epoch 15
val [0015 / 0050] validation loss: 	0.39616
checkpoint
starting train epoch 15
 epoch [0015 / 0050] [0009/280] eta: 0 Days 0:40:47        lr: 	1.0000E-07 loss: 	0.40554
 epoch [0015 / 0050] [0109/280] eta: 0 Days 0:40:12        lr: 	1.0000E-07 loss: 	0.38861
 epoch [0015 / 0050] [0209/280] eta: 0 Days 0:39:34        lr: 	1.0000E-07 loss: 	0.38987
16
starting val epoch 16
val [0016 / 0050] validation loss: 	0.39432
checkpoint
starting train epoch 16
 epoch [0016 / 0050] [0009/280] eta: 0 Days 0:39:31        lr: 	1.0000E-07 loss: 	0.40691
 epoch [0016 / 0050] [0109/280] eta: 0 Days 0:38:55        lr: 	1.0000E-07 loss: 	0.37819
 epoch [0016 / 0050] [0209/280] eta: 0 Days 0:38:21        lr: 	1.0000E-07 loss: 	0.39310
17
starting val epoch 17
val [0017 / 0050] validation loss: 	0.39304
checkpoint
starting train epoch 17
 epoch [0017 / 0050] [0009/280] eta: 0 Days 0:38:25        lr: 	1.0000E-07 loss: 	0.39876
 epoch [0017 / 0050] [0109/280] eta: 0 Days 0:37:52        lr: 	1.0000E-07 loss: 	0.39876
 epoch [0017 / 0050] [0209/280] eta: 0 Days 0:37:18        lr: 	1.0000E-07 loss: 	0.38482
18
starting val epoch 18
val [0018 / 0050] validation loss: 	0.39196
checkpoint
starting train epoch 18
 epoch [0018 / 0050] [0009/280] eta: 0 Days 0:37:13        lr: 	1.0000E-07 loss: 	0.41098
 epoch [0018 / 0050] [0109/280] eta: 0 Days 0:36:39        lr: 	1.0000E-07 loss: 	0.39624
 epoch [0018 / 0050] [0209/280] eta: 0 Days 0:36:6         lr: 	1.0000E-07 loss: 	0.37942
19
starting val epoch 19
val [0019 / 0050] validation loss: 	0.39086
checkpoint
starting train epoch 19
 epoch [0019 / 0050] [0009/280] eta: 0 Days 0:35:59        lr: 	1.0000E-07 loss: 	0.34199
 epoch [0019 / 0050] [0109/280] eta: 0 Days 0:35:26        lr: 	1.0000E-07 loss: 	0.37570
 epoch [0019 / 0050] [0209/280] eta: 0 Days 0:34:55        lr: 	1.0000E-07 loss: 	0.38100
20
starting val epoch 20
val [0020 / 0050] validation loss: 	0.38980
checkpoint
starting train epoch 20
 epoch [0020 / 0050] [0009/280] eta: 0 Days 0:34:47        lr: 	1.0000E-07 loss: 	0.42328
 epoch [0020 / 0050] [0109/280] eta: 0 Days 0:34:14        lr: 	1.0000E-07 loss: 	0.37679
 epoch [0020 / 0050] [0209/280] eta: 0 Days 0:33:42        lr: 	1.0000E-07 loss: 	0.37595
21
starting val epoch 21
val [0021 / 0050] validation loss: 	0.38932
checkpoint
starting train epoch 21
 epoch [0021 / 0050] [0009/280] eta: 0 Days 0:33:33        lr: 	1.0000E-07 loss: 	0.47900
 epoch [0021 / 0050] [0109/280] eta: 0 Days 0:33:3         lr: 	1.0000E-07 loss: 	0.38510
 epoch [0021 / 0050] [0209/280] eta: 0 Days 0:32:31        lr: 	1.0000E-07 loss: 	0.37923
22
starting val epoch 22
val [0022 / 0050] validation loss: 	0.38879
checkpoint
starting train epoch 22
 epoch [0022 / 0050] [0009/280] eta: 0 Days 0:32:22        lr: 	1.0000E-07 loss: 	0.37337
 epoch [0022 / 0050] [0109/280] eta: 0 Days 0:31:51        lr: 	1.0000E-07 loss: 	0.36437
 epoch [0022 / 0050] [0209/280] eta: 0 Days 0:31:20        lr: 	1.0000E-07 loss: 	0.38599
23
starting val epoch 23
val [0023 / 0050] validation loss: 	0.38830
checkpoint
starting train epoch 23
 epoch [0023 / 0050] [0009/280] eta: 0 Days 0:31:11        lr: 	1.0000E-07 loss: 	0.40990
 epoch [0023 / 0050] [0109/280] eta: 0 Days 0:30:41        lr: 	1.0000E-07 loss: 	0.37814
 epoch [0023 / 0050] [0209/280] eta: 0 Days 0:30:12        lr: 	1.0000E-07 loss: 	0.37620
24
starting val epoch 24
val [0024 / 0050] validation loss: 	0.38774
checkpoint
starting train epoch 24
 epoch [0024 / 0050] [0009/280] eta: 0 Days 0:30:4         lr: 	1.0000E-07 loss: 	0.47835
 epoch [0024 / 0050] [0109/280] eta: 0 Days 0:29:35        lr: 	1.0000E-07 loss: 	0.36305
 epoch [0024 / 0050] [0209/280] eta: 0 Days 0:29:5         lr: 	1.0000E-07 loss: 	0.37518
25
starting val epoch 25
val [0025 / 0050] validation loss: 	0.38754
checkpoint
starting train epoch 25
 epoch [0025 / 0050] [0009/280] eta: 0 Days 0:28:54        lr: 	1.0000E-07 loss: 	0.38959
 epoch [0025 / 0050] [0109/280] eta: 0 Days 0:28:24        lr: 	1.0000E-07 loss: 	0.36152
 epoch [0025 / 0050] [0209/280] eta: 0 Days 0:27:57        lr: 	1.0000E-07 loss: 	0.37022
26
starting val epoch 26
val [0026 / 0050] validation loss: 	0.38646
starting train epoch 26
 epoch [0026 / 0050] [0009/280] eta: 0 Days 0:27:44        lr: 	1.0000E-07 loss: 	0.41180
 epoch [0026 / 0050] [0109/280] eta: 0 Days 0:27:16        lr: 	1.0000E-07 loss: 	0.36063
 epoch [0026 / 0050] [0209/280] eta: 0 Days 0:26:50        lr: 	1.0000E-07 loss: 	0.37602
27
starting val epoch 27
val [0027 / 0050] validation loss: 	0.38631
starting train epoch 27
 epoch [0027 / 0050] [0009/280] eta: 0 Days 0:26:36        lr: 	1.0000E-07 loss: 	0.37335
 epoch [0027 / 0050] [0109/280] eta: 0 Days 0:26:7         lr: 	1.0000E-07 loss: 	0.35976
 epoch [0027 / 0050] [0209/280] eta: 0 Days 0:25:38        lr: 	1.0000E-07 loss: 	0.36916
28
starting val epoch 28
val [0028 / 0050] validation loss: 	0.38678
checkpoint
starting train epoch 28
 epoch [0028 / 0050] [0009/280] eta: 0 Days 0:25:25        lr: 	1.0000E-07 loss: 	0.37019
 epoch [0028 / 0050] [0109/280] eta: 0 Days 0:24:56        lr: 	1.0000E-07 loss: 	0.36345
 epoch [0028 / 0050] [0209/280] eta: 0 Days 0:24:28        lr: 	1.0000E-07 loss: 	0.36546
29
starting val epoch 29
val [0029 / 0050] validation loss: 	0.38587
checkpoint
starting train epoch 29
 epoch [0029 / 0050] [0009/280] eta: 0 Days 0:24:14        lr: 	1.0000E-07 loss: 	0.37302
 epoch [0029 / 0050] [0109/280] eta: 0 Days 0:23:46        lr: 	1.0000E-07 loss: 	0.35104
 epoch [0029 / 0050] [0209/280] eta: 0 Days 0:23:19        lr: 	1.0000E-07 loss: 	0.36948
30
starting val epoch 30
val [0030 / 0050] validation loss: 	0.38533
starting train epoch 30
 epoch [0030 / 0050] [0009/280] eta: 0 Days 0:23:3         lr: 	1.0000E-07 loss: 	0.37862
 epoch [0030 / 0050] [0109/280] eta: 0 Days 0:22:35        lr: 	1.0000E-07 loss: 	0.38338
 epoch [0030 / 0050] [0209/280] eta: 0 Days 0:22:8         lr: 	1.0000E-07 loss: 	0.37163
31
starting val epoch 31
val [0031 / 0050] validation loss: 	0.38506
checkpoint
starting train epoch 31
 epoch [0031 / 0050] [0009/280] eta: 0 Days 0:21:54        lr: 	1.0000E-07 loss: 	0.48289
 epoch [0031 / 0050] [0109/280] eta: 0 Days 0:21:28        lr: 	1.0000E-07 loss: 	0.37420
 epoch [0031 / 0050] [0209/280] eta: 0 Days 0:21:0         lr: 	1.0000E-07 loss: 	0.36557
32
starting val epoch 32
val [0032 / 0050] validation loss: 	0.38551
checkpoint
starting train epoch 32
 epoch [0032 / 0050] [0009/280] eta: 0 Days 0:20:45        lr: 	1.0000E-07 loss: 	0.43119
 epoch [0032 / 0050] [0109/280] eta: 0 Days 0:20:18        lr: 	1.0000E-07 loss: 	0.38088
 epoch [0032 / 0050] [0209/280] eta: 0 Days 0:19:51        lr: 	1.0000E-07 loss: 	0.37517
33
starting val epoch 33
val [0033 / 0050] validation loss: 	0.38511
starting train epoch 33
 epoch [0033 / 0050] [0009/280] eta: 0 Days 0:19:35        lr: 	1.0000E-07 loss: 	0.36576
 epoch [0033 / 0050] [0109/280] eta: 0 Days 0:19:8         lr: 	1.0000E-07 loss: 	0.37442
 epoch [0033 / 0050] [0209/280] eta: 0 Days 0:18:41        lr: 	1.0000E-07 loss: 	0.35631
34
starting val epoch 34
val [0034 / 0050] validation loss: 	0.38445
starting train epoch 34
 epoch [0034 / 0050] [0009/280] eta: 0 Days 0:18:24        lr: 	1.0000E-07 loss: 	0.43739
 epoch [0034 / 0050] [0109/280] eta: 0 Days 0:17:59        lr: 	1.0000E-07 loss: 	0.34568
 epoch [0034 / 0050] [0209/280] eta: 0 Days 0:17:33        lr: 	1.0000E-07 loss: 	0.35388
35
starting val epoch 35
val [0035 / 0050] validation loss: 	0.38460
starting train epoch 35
 epoch [0035 / 0050] [0009/280] eta: 0 Days 0:17:16        lr: 	1.0000E-07 loss: 	0.45229
 epoch [0035 / 0050] [0109/280] eta: 0 Days 0:16:50        lr: 	1.0000E-07 loss: 	0.37195
 epoch [0035 / 0050] [0209/280] eta: 0 Days 0:16:24        lr: 	1.0000E-07 loss: 	0.36632
36
starting val epoch 36
val [0036 / 0050] validation loss: 	0.38454
checkpoint
starting train epoch 36
 epoch [0036 / 0050] [0009/280] eta: 0 Days 0:16:8         lr: 	1.0000E-07 loss: 	0.44769
 epoch [0036 / 0050] [0109/280] eta: 0 Days 0:15:41        lr: 	1.0000E-07 loss: 	0.36284
 epoch [0036 / 0050] [0209/280] eta: 0 Days 0:15:15        lr: 	1.0000E-07 loss: 	0.37122
37
starting val epoch 37
val [0037 / 0050] validation loss: 	0.38510
checkpoint
starting train epoch 37
 epoch [0037 / 0050] [0009/280] eta: 0 Days 0:14:58        lr: 	1.0000E-07 loss: 	0.38479
 epoch [0037 / 0050] [0109/280] eta: 0 Days 0:14:32        lr: 	1.0000E-07 loss: 	0.35883
 epoch [0037 / 0050] [0209/280] eta: 0 Days 0:14:6         lr: 	1.0000E-07 loss: 	0.36873
38
starting val epoch 38
val [0038 / 0050] validation loss: 	0.38478
starting train epoch 38
 epoch [0038 / 0050] [0009/280] eta: 0 Days 0:13:48        lr: 	1.0000E-07 loss: 	0.41058
 epoch [0038 / 0050] [0109/280] eta: 0 Days 0:13:22        lr: 	1.0000E-07 loss: 	0.37079
 epoch [0038 / 0050] [0209/280] eta: 0 Days 0:12:57        lr: 	1.0000E-07 loss: 	0.36608
39
starting val epoch 39
val [0039 / 0050] validation loss: 	0.38451
starting train epoch 39
 epoch [0039 / 0050] [0009/280] eta: 0 Days 0:12:39        lr: 	1.0000E-07 loss: 	0.36863
 epoch [0039 / 0050] [0109/280] eta: 0 Days 0:12:13        lr: 	1.0000E-07 loss: 	0.38733
 epoch [0039 / 0050] [0209/280] eta: 0 Days 0:11:47        lr: 	1.0000E-07 loss: 	0.36977
40
starting val epoch 40
val [0040 / 0050] validation loss: 	0.38514
checkpoint
starting train epoch 40
 epoch [0040 / 0050] [0009/280] eta: 0 Days 0:11:30        lr: 	1.0000E-07 loss: 	0.39695
 epoch [0040 / 0050] [0109/280] eta: 0 Days 0:11:4         lr: 	1.0000E-07 loss: 	0.36204
 epoch [0040 / 0050] [0209/280] eta: 0 Days 0:10:38        lr: 	1.0000E-07 loss: 	0.36983
41
starting val epoch 41
val [0041 / 0050] validation loss: 	0.38445
starting train epoch 41
 epoch [0041 / 0050] [0009/280] eta: 0 Days 0:10:20        lr: 	1.0000E-07 loss: 	0.44447
 epoch [0041 / 0050] [0109/280] eta: 0 Days 0:9:54         lr: 	1.0000E-07 loss: 	0.36708
 epoch [0041 / 0050] [0209/280] eta: 0 Days 0:9:28         lr: 	1.0000E-07 loss: 	0.36830
42
starting val epoch 42
val [0042 / 0050] validation loss: 	0.38472
checkpoint
starting train epoch 42
 epoch [0042 / 0050] [0009/280] eta: 0 Days 0:9:10         lr: 	1.0000E-07 loss: 	0.41848
 epoch [0042 / 0050] [0109/280] eta: 0 Days 0:8:45         lr: 	1.0000E-07 loss: 	0.36394
 epoch [0042 / 0050] [0209/280] eta: 0 Days 0:8:20         lr: 	1.0000E-07 loss: 	0.36663
43
starting val epoch 43
val [0043 / 0050] validation loss: 	0.38433
starting train epoch 43
 epoch [0043 / 0050] [0009/280] eta: 0 Days 0:8:1          lr: 	1.0000E-07 loss: 	0.38515
 epoch [0043 / 0050] [0109/280] eta: 0 Days 0:7:36         lr: 	1.0000E-07 loss: 	0.36218
 epoch [0043 / 0050] [0209/280] eta: 0 Days 0:7:10         lr: 	1.0000E-07 loss: 	0.36380
44
starting val epoch 44
val [0044 / 0050] validation loss: 	0.38422
checkpoint
starting train epoch 44
 epoch [0044 / 0050] [0009/280] eta: 0 Days 0:6:52         lr: 	1.0000E-07 loss: 	0.46187
 epoch [0044 / 0050] [0109/280] eta: 0 Days 0:6:27         lr: 	1.0000E-07 loss: 	0.34565
 epoch [0044 / 0050] [0209/280] eta: 0 Days 0:6:2          lr: 	1.0000E-07 loss: 	0.35817
45
starting val epoch 45
val [0045 / 0050] validation loss: 	0.38410
checkpoint
starting train epoch 45
 epoch [0045 / 0050] [0009/280] eta: 0 Days 0:5:43         lr: 	1.0000E-07 loss: 	0.47850
 epoch [0045 / 0050] [0109/280] eta: 0 Days 0:5:17         lr: 	1.0000E-07 loss: 	0.36520
 epoch [0045 / 0050] [0209/280] eta: 0 Days 0:4:52         lr: 	1.0000E-07 loss: 	0.35959
46
starting val epoch 46
val [0046 / 0050] validation loss: 	0.38503
checkpoint
starting train epoch 46
 epoch [0046 / 0050] [0009/280] eta: 0 Days 0:4:33         lr: 	1.0000E-07 loss: 	0.32500
 epoch [0046 / 0050] [0109/280] eta: 0 Days 0:4:8          lr: 	1.0000E-07 loss: 	0.34008
 epoch [0046 / 0050] [0209/280] eta: 0 Days 0:3:43         lr: 	1.0000E-07 loss: 	0.36167
47
starting val epoch 47
val [0047 / 0050] validation loss: 	0.38405
starting train epoch 47
 epoch [0047 / 0050] [0009/280] eta: 0 Days 0:3:24         lr: 	1.0000E-07 loss: 	0.45843
 epoch [0047 / 0050] [0109/280] eta: 0 Days 0:2:59         lr: 	1.0000E-07 loss: 	0.36012
 epoch [0047 / 0050] [0209/280] eta: 0 Days 0:2:34         lr: 	1.0000E-07 loss: 	0.35069
48
starting val epoch 48
val [0048 / 0050] validation loss: 	0.38385
checkpoint
starting train epoch 48
 epoch [0048 / 0050] [0009/280] eta: 0 Days 0:2:15         lr: 	1.0000E-07 loss: 	0.42466
 epoch [0048 / 0050] [0109/280] eta: 0 Days 0:1:50         lr: 	1.0000E-07 loss: 	0.35823
 epoch [0048 / 0050] [0209/280] eta: 0 Days 0:1:26         lr: 	1.0000E-07 loss: 	0.35628
49
starting val epoch 49
val [0049 / 0050] validation loss: 	0.38359
checkpoint
starting train epoch 49
 epoch [0049 / 0050] [0009/280] eta: 0 Days 0:1:6          lr: 	1.0000E-07 loss: 	0.32429
 epoch [0049 / 0050] [0109/280] eta: 0 Days 0:0:41         lr: 	1.0000E-07 loss: 	0.34277
 epoch [0049 / 0050] [0209/280] eta: 0 Days 0:0:17         lr: 	1.0000E-07 loss: 	0.36267
starting val epoch 0
val [0000 / 0050] validation loss: 	0.39528
Acute and unspecified renal failure                                                        & 0.713(0.751, 0.672) & 0.307 (0.374, 0.254)
fused_ehr test  0   best mean auc :0.713 mean auprc 0.307
                    CI AUROC (0.672, 0.751) CI AUPRC (0.254, 0.374)
                     AUROC accute 0.713 mixed 0.713 chronic 0.713
                     AUROC accute CI (0.672, 0.751) mixed (0.672 , 0.751) chronic (0.672, 0.751)
                     AUPRC accute  0.307 mixed 0.307 chronic 0.307
                     AUPRC accute CI  (0.254, 0.374) mixed (0.254,  0.374) chronic (0.254, 0.374)