Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ehr loaded
cxr loaded
==> training
running for fusion_type late
0
starting val epoch 0
val [0000 / 0050] validation loss: 	1.07356
checkpoint
starting train epoch 0
 epoch [0000 / 0050] [0009/280] eta: 0 Days 14:30:12       lr: 	1.0000E-03 loss: 	1.73149
 epoch [0000 / 0050] [0109/280] eta: 0 Days 2:5:28         lr: 	1.0000E-03 loss: 	0.62555
 epoch [0000 / 0050] [0209/280] eta: 0 Days 1:29:51        lr: 	1.0000E-03 loss: 	0.54296
1
starting val epoch 1
val [0001 / 0050] validation loss: 	0.39805
checkpoint
starting train epoch 1
 epoch [0001 / 0050] [0009/280] eta: 0 Days 1:30:21        lr: 	1.0000E-03 loss: 	0.45802
 epoch [0001 / 0050] [0109/280] eta: 0 Days 1:19:22        lr: 	1.0000E-03 loss: 	0.41148
 epoch [0001 / 0050] [0209/280] eta: 0 Days 1:12:42        lr: 	1.0000E-03 loss: 	0.39513
2
starting val epoch 2
val [0002 / 0050] validation loss: 	0.39636
starting train epoch 2
 epoch [0002 / 0050] [0009/280] eta: 0 Days 1:13:43        lr: 	1.0000E-03 loss: 	0.39885
 epoch [0002 / 0050] [0109/280] eta: 0 Days 1:9:27         lr: 	1.0000E-03 loss: 	0.37326
 epoch [0002 / 0050] [0209/280] eta: 0 Days 1:6:5          lr: 	1.0000E-03 loss: 	0.37945
3
starting val epoch 3
val [0003 / 0050] validation loss: 	0.40497
starting train epoch 3
 epoch [0003 / 0050] [0009/280] eta: 0 Days 1:7:8          lr: 	1.0000E-03 loss: 	0.37468
 epoch [0003 / 0050] [0109/280] eta: 0 Days 1:4:51         lr: 	1.0000E-03 loss: 	0.37859
 epoch [0003 / 0050] [0209/280] eta: 0 Days 1:2:30         lr: 	1.0000E-03 loss: 	0.38780
4
starting val epoch 4
val [0004 / 0050] validation loss: 	0.42276
checkpoint
starting train epoch 4
 epoch [0004 / 0050] [0009/280] eta: 0 Days 1:3:50         lr: 	1.0000E-03 loss: 	0.40554
 epoch [0004 / 0050] [0109/280] eta: 0 Days 1:2:9          lr: 	1.0000E-03 loss: 	0.38178
 epoch [0004 / 0050] [0209/280] eta: 0 Days 1:0:40         lr: 	1.0000E-03 loss: 	0.39099
5
starting val epoch 5
val [0005 / 0050] validation loss: 	0.41351
starting train epoch 5
 epoch [0005 / 0050] [0009/280] eta: 0 Days 1:1:24         lr: 	1.0000E-03 loss: 	0.50011
 epoch [0005 / 0050] [0109/280] eta: 0 Days 0:59:48        lr: 	1.0000E-03 loss: 	0.37011
 epoch [0005 / 0050] [0209/280] eta: 0 Days 0:58:24        lr: 	1.0000E-03 loss: 	0.36977
6
starting val epoch 6
val [0006 / 0050] validation loss: 	0.38433
starting train epoch 6
 epoch [0006 / 0050] [0009/280] eta: 0 Days 0:58:56        lr: 	1.0000E-03 loss: 	0.40619
 epoch [0006 / 0050] [0109/280] eta: 0 Days 0:57:41        lr: 	1.0000E-03 loss: 	0.39672
 epoch [0006 / 0050] [0209/280] eta: 0 Days 0:56:24        lr: 	1.0000E-03 loss: 	0.39130
7
starting val epoch 7
val [0007 / 0050] validation loss: 	0.38158
starting train epoch 7
 epoch [0007 / 0050] [0009/280] eta: 0 Days 0:56:44        lr: 	1.0000E-03 loss: 	0.46699
 epoch [0007 / 0050] [0109/280] eta: 0 Days 0:55:36        lr: 	1.0000E-03 loss: 	0.38381
 epoch [0007 / 0050] [0209/280] eta: 0 Days 0:54:41        lr: 	1.0000E-03 loss: 	0.38192
8
starting val epoch 8
val [0008 / 0050] validation loss: 	0.46414
starting train epoch 8
 epoch [0008 / 0050] [0009/280] eta: 0 Days 0:55:2         lr: 	1.0000E-03 loss: 	0.50009
 epoch [0008 / 0050] [0109/280] eta: 0 Days 0:54:0         lr: 	1.0000E-03 loss: 	0.37920
 epoch [0008 / 0050] [0209/280] eta: 0 Days 0:53:10        lr: 	1.0000E-03 loss: 	0.38581
9
starting val epoch 9
val [0009 / 0050] validation loss: 	0.38696
checkpoint
starting train epoch 9
 epoch [0009 / 0050] [0009/280] eta: 0 Days 0:53:42        lr: 	1.0000E-03 loss: 	0.51541
 epoch [0009 / 0050] [0109/280] eta: 0 Days 0:52:43        lr: 	1.0000E-03 loss: 	0.37000
 epoch [0009 / 0050] [0209/280] eta: 0 Days 0:51:47        lr: 	1.0000E-03 loss: 	0.37146
10
starting val epoch 10
val [0010 / 0050] validation loss: 	0.41386
starting train epoch 10
 epoch [0010 / 0050] [0009/280] eta: 0 Days 0:51:57        lr: 	1.0000E-03 loss: 	0.55805
 epoch [0010 / 0050] [0109/280] eta: 0 Days 0:51:4         lr: 	1.0000E-03 loss: 	0.41005
 epoch [0010 / 0050] [0209/280] eta: 0 Days 0:50:21        lr: 	1.0000E-03 loss: 	0.40727
11
