Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
ehr loaded
cxr loaded
==> training
running for fusion_type early
0
starting val epoch 0
val [0000 / 0050] validation loss: 	0.77787
checkpoint
starting train epoch 0
 epoch [0000 / 0050] [0009/280] eta: 0 Days 13:57:23       lr: 	1.0000E-04 loss: 	0.60413
 epoch [0000 / 0050] [0109/280] eta: 0 Days 1:34:11        lr: 	1.0000E-04 loss: 	0.39418
 epoch [0000 / 0050] [0209/280] eta: 0 Days 0:58:9         lr: 	1.0000E-04 loss: 	0.37037
1
starting val epoch 1
val [0001 / 0050] validation loss: 	0.35720
checkpoint
starting train epoch 1
 epoch [0001 / 0050] [0009/280] eta: 0 Days 0:58:34        lr: 	1.0000E-04 loss: 	0.33037
 epoch [0001 / 0050] [0109/280] eta: 0 Days 0:48:5         lr: 	1.0000E-04 loss: 	0.30814
 epoch [0001 / 0050] [0209/280] eta: 0 Days 0:41:29        lr: 	1.0000E-04 loss: 	0.30493
2
starting val epoch 2
val [0002 / 0050] validation loss: 	0.32775
checkpoint
starting train epoch 2
 epoch [0002 / 0050] [0009/280] eta: 0 Days 0:43:41        lr: 	1.0000E-04 loss: 	0.27301
 epoch [0002 / 0050] [0109/280] eta: 0 Days 0:39:37        lr: 	1.0000E-04 loss: 	0.28334
 epoch [0002 / 0050] [0209/280] eta: 0 Days 0:36:17        lr: 	1.0000E-04 loss: 	0.27562
3
starting val epoch 3
val [0003 / 0050] validation loss: 	0.36337
checkpoint
starting train epoch 3
 epoch [0003 / 0050] [0009/280] eta: 0 Days 0:38:7         lr: 	1.0000E-04 loss: 	0.29312
 epoch [0003 / 0050] [0109/280] eta: 0 Days 0:35:32        lr: 	1.0000E-04 loss: 	0.27142
 epoch [0003 / 0050] [0209/280] eta: 0 Days 0:33:27        lr: 	1.0000E-04 loss: 	0.27516
4
starting val epoch 4
val [0004 / 0050] validation loss: 	0.33684
starting train epoch 4
 epoch [0004 / 0050] [0009/280] eta: 0 Days 0:34:26        lr: 	1.0000E-04 loss: 	0.31430
 epoch [0004 / 0050] [0109/280] eta: 0 Days 0:32:41        lr: 	1.0000E-04 loss: 	0.28761
 epoch [0004 / 0050] [0209/280] eta: 0 Days 0:31:7         lr: 	1.0000E-04 loss: 	0.27516
5
starting val epoch 5
val [0005 / 0050] validation loss: 	0.33772
checkpoint
starting train epoch 5
 epoch [0005 / 0050] [0009/280] eta: 0 Days 0:32:19        lr: 	1.0000E-04 loss: 	0.34159
 epoch [0005 / 0050] [0109/280] eta: 0 Days 0:31:8         lr: 	1.0000E-04 loss: 	0.28262
 epoch [0005 / 0050] [0209/280] eta: 0 Days 0:30:2         lr: 	1.0000E-04 loss: 	0.26828
6
starting val epoch 6
val [0006 / 0050] validation loss: 	0.34088
starting train epoch 6
 epoch [0006 / 0050] [0009/280] eta: 0 Days 0:30:41        lr: 	1.0000E-04 loss: 	0.36275
 epoch [0006 / 0050] [0109/280] eta: 0 Days 0:29:37        lr: 	1.0000E-04 loss: 	0.28344
 epoch [0006 / 0050] [0209/280] eta: 0 Days 0:28:39        lr: 	1.0000E-04 loss: 	0.27373
7
starting val epoch 7
val [0007 / 0050] validation loss: 	0.34268
starting train epoch 7
 epoch [0007 / 0050] [0009/280] eta: 0 Days 0:29:12        lr: 	1.0000E-04 loss: 	0.24167
 epoch [0007 / 0050] [0109/280] eta: 0 Days 0:28:19        lr: 	1.0000E-04 loss: 	0.25715
 epoch [0007 / 0050] [0209/280] eta: 0 Days 0:27:27        lr: 	1.0000E-04 loss: 	0.25331
8
starting val epoch 8
val [0008 / 0050] validation loss: 	0.33308
starting train epoch 8
 epoch [0008 / 0050] [0009/280] eta: 0 Days 0:27:55        lr: 	1.0000E-04 loss: 	0.27929
 epoch [0008 / 0050] [0109/280] eta: 0 Days 0:27:8         lr: 	1.0000E-04 loss: 	0.27165
 epoch [0008 / 0050] [0209/280] eta: 0 Days 0:26:22        lr: 	1.0000E-04 loss: 	0.26714
9
starting val epoch 9
val [0009 / 0050] validation loss: 	0.33302
starting train epoch 9
 epoch [0009 / 0050] [0009/280] eta: 0 Days 0:26:45        lr: 	1.0000E-04 loss: 	0.21621
 epoch [0009 / 0050] [0109/280] eta: 0 Days 0:26:6         lr: 	1.0000E-04 loss: 	0.26420
 epoch [0009 / 0050] [0209/280] eta: 0 Days 0:25:26        lr: 	1.0000E-04 loss: 	0.26316
10
starting val epoch 10
val [0010 / 0050] validation loss: 	0.34880
starting train epoch 10
 epoch [0010 / 0050] [0009/280] eta: 0 Days 0:25:46        lr: 	1.0000E-04 loss: 	0.21495
 epoch [0010 / 0050] [0109/280] eta: 0 Days 0:25:10        lr: 	1.0000E-04 loss: 	0.25839
 epoch [0010 / 0050] [0209/280] eta: 0 Days 0:24:33        lr: 	1.0000E-04 loss: 	0.25746
11
starting val epoch 11
val [0011 / 0050] validation loss: 	0.34751
starting train epoch 11
 epoch [0011 / 0050] [0009/280] eta: 0 Days 0:24:50        lr: 	1.0000E-04 loss: 	0.23673
 epoch [0011 / 0050] [0109/280] eta: 0 Days 0:24:16        lr: 	1.0000E-04 loss: 	0.24715
 epoch [0011 / 0050] [0209/280] eta: 0 Days 0:23:43        lr: 	1.0000E-04 loss: 	0.25172
12
starting val epoch 12
val [0012 / 0050] validation loss: 	0.40092
starting train epoch 12
 epoch [0012 / 0050] [0009/280] eta: 0 Days 0:23:58        lr: 	1.0000E-04 loss: 	0.25667
 epoch [0012 / 0050] [0109/280] eta: 0 Days 0:23:28        lr: 	1.0000E-04 loss: 	0.23553
 epoch [0012 / 0050] [0209/280] eta: 0 Days 0:22:59        lr: 	1.0000E-04 loss: 	0.25572
13
starting val epoch 13
val [0013 / 0050] validation loss: 	0.33371
starting train epoch 13
 epoch [0013 / 0050] [0009/280] eta: 0 Days 0:23:12        lr: 	1.0000E-04 loss: 	0.33298
 epoch [0013 / 0050] [0109/280] eta: 0 Days 0:22:45        lr: 	1.0000E-04 loss: 	0.28840
 epoch [0013 / 0050] [0209/280] eta: 0 Days 0:22:17        lr: 	1.0000E-04 loss: 	0.25654
14
starting val epoch 14
val [0014 / 0050] validation loss: 	0.32951
starting train epoch 14
 epoch [0014 / 0050] [0009/280] eta: 0 Days 0:22:28        lr: 	1.0000E-04 loss: 	0.37369
 epoch [0014 / 0050] [0109/280] eta: 0 Days 0:22:3         lr: 	1.0000E-04 loss: 	0.27802
 epoch [0014 / 0050] [0209/280] eta: 0 Days 0:21:38        lr: 	1.0000E-04 loss: 	0.26785
15
starting val epoch 15
val [0015 / 0050] validation loss: 	0.35711
starting train epoch 15
 epoch [0015 / 0050] [0009/280] eta: 0 Days 0:21:47        lr: 	1.0000E-04 loss: 	0.24241
 epoch [0015 / 0050] [0109/280] eta: 0 Days 0:21:22        lr: 	1.0000E-04 loss: 	0.25364
 epoch [0015 / 0050] [0209/280] eta: 0 Days 0:20:57        lr: 	1.0000E-04 loss: 	0.25954
16
starting val epoch 16
val [0016 / 0050] validation loss: 	0.34955
starting train epoch 16
 epoch [0016 / 0050] [0009/280] eta: 0 Days 0:21:4         lr: 	1.0000E-04 loss: 	0.25823
 epoch [0016 / 0050] [0109/280] eta: 0 Days 0:20:41        lr: 	1.0000E-04 loss: 	0.24195
 epoch [0016 / 0050] [0209/280] eta: 0 Days 0:20:18        lr: 	1.0000E-04 loss: 	0.25893
17
starting val epoch 17
val [0017 / 0050] validation loss: 	0.33626
starting train epoch 17
 epoch [0017 / 0050] [0009/280] eta: 0 Days 0:20:25        lr: 	1.0000E-04 loss: 	0.20552
 epoch [0017 / 0050] [0109/280] eta: 0 Days 0:20:2         lr: 	1.0000E-04 loss: 	0.26136
 epoch [0017 / 0050] [0209/280] eta: 0 Days 0:19:42        lr: 	1.0000E-04 loss: 	0.25228
18
starting val epoch 18
val [0018 / 0050] validation loss: 	0.35385
starting train epoch 18
 epoch [0018 / 0050] [0009/280] eta: 0 Days 0:19:47        lr: 	1.0000E-04 loss: 	0.30183
 epoch [0018 / 0050] [0109/280] eta: 0 Days 0:19:26        lr: 	1.0000E-04 loss: 	0.26772
 epoch [0018 / 0050] [0209/280] eta: 0 Days 0:19:5         lr: 	1.0000E-04 loss: 	0.26088
19
starting val epoch 19
val [0019 / 0050] validation loss: 	0.33437
starting train epoch 19
 epoch [0019 / 0050] [0009/280] eta: 0 Days 0:19:8         lr: 	1.0000E-04 loss: 	0.28488
 epoch [0019 / 0050] [0109/280] eta: 0 Days 0:18:47        lr: 	1.0000E-04 loss: 	0.22577
 epoch [0019 / 0050] [0209/280] eta: 0 Days 0:18:27        lr: 	1.0000E-04 loss: 	0.24160
20
starting val epoch 20
val [0020 / 0050] validation loss: 	0.34268
starting val epoch 0
val [0000 / 0050] validation loss: 	0.35146
Acute and unspecified renal failure                                                        & 0.838(0.868, 0.806) & 0.530 (0.612, 0.462)
fused_ehr test  0   best mean auc :0.838 mean auprc 0.530
                    CI AUROC (0.806, 0.868) CI AUPRC (0.462, 0.612)
                     AUROC accute 0.838 mixed 0.838 chronic 0.838
                     AUROC accute CI (0.806, 0.868) mixed (0.806 , 0.868) chronic (0.806, 0.868)
                     AUPRC accute  0.530 mixed 0.530 chronic 0.530
                     AUPRC accute CI  (0.462, 0.612) mixed (0.462,  0.612) chronic (0.462, 0.612)